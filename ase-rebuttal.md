We thank reviewers for their insightful comments. Below, we address a few common points before responding to the specific questions from each reviewer. We will add all explanations to the paper.


# Description of Client Programs
In this work, we proposed automated refactoring techniques that work without context. We target at all method call expressions in any code repository that involves deprecated methods, rather than a specific code repository. Upon successful verification, any such call expression can be trustedly replaced with the generated replacement. 

To this end, both our symbolic and neural engines accept a generic method call expression (for example, `date.getHours()`), and return (if successful) a sequence of Java statements involving the original program variables (e.g. `date`). Applying the generated transformation to a code repository is thus a series of straight forward replacements.

# Artifact Availability
The link to our anonymous repository containing the reproduction bundle was submitted in the HotCRP site (in the section Information about Artifact Availability). The [link](https://anonymous.4open.science/r/refactoring-synthesis-3078/README.md) is still effective.

# Novelty / Significance of Findings
To the best of our knowledge, we are the first to apply code hints in deprecation comments for automatic refactoring. We will add this to earlier sections in our paper and claim as contribution. 

Our research questions are centered around the benefits of deprecation comments. Whether those comments, while designed to be helpful to developers, are helpful to a symbolic engine or an LLM is an open question. Our work gives a positive answer to this question and we believe the implications are significant. Firstly, our findings encourage future symbolic engine to incorporate the component library constructed from the comments; secondly, an LLM-based refactoring engine needs to take deprecation comments into consideration during prompt construction or even fine-tuning. Both are exicting future works. We will add related discussion in the paper.


# Reviewer A
## Q1 && Q2 (Lacking description of client code)
Please see our discussions in __Description of Client Programs__ above. 

## Equivalence Checking
We intentionally leave aside methods with no semantically equivalent replacements. We will clarify this in the paper.

Our equivalence checking is unsound as discussed in Section 5.2. But this is due to the natural of fuzzing, which, however, is proven to be reliable in testing real world software.

## Long Execution Time
We aim to build a tool that works without context (see __Description of Client Programs__). A transformation generated by our engine works for method call expressions in any position. As a result, we believe the long execution time of verifying a particular transformation is tolerable.



# Reviewer B
## Comment 4 (Motivation for our approaches)
We formulate the automatic deprecation refactoring problem as a program synthesis problem. In this context, the specification is the original call expression, and the related deprecation comments in natural language. Component-based synthesis methods, particularly those inspired by Counterexample-Guided Inductive Synthesis (CEGIS) have achieved great success (...citations). We believe that those approaches are particularly relevant to our problem of quantifying the "usefulness" of deprecation comments, since code hints in those comments serve as a good starting point for constructing a component library.

On the other hand, LLMs have shown great advances in code generation and general software engineering tasks. Investigating the impact of including deprecation comments in the prompt on the quality of code generation is intriguing. We believe our exploration could provide insights into how these comments enhance or influence the performance of LLMs in generating appropriate code transformations.


## Comment 11 (Lacking detailed explanation for our experimental setup)
### Model selection and hyper parameters
We selected the Claude model family because these models are among the best performing models for code generation [1]. We set the model temperature to 0.2 following prior works on LLMs-based code generation [2]. We will provide more detailed descriptions of our model selection and hyperparameter choices in the final paper.

### Experiments regarding removing code hints
In our experiments, we did not remove code hints from the benchmarks. However, not all benchmarks were documented or contained usable code fragments.  As a result, we split our dataset into two groups: one containing benchmarks with code hints and the other without, as presented in Table 1.


# Reviewer C
## Q1 (Novelty of our approach)
Please see our discussions in __Novelty / Significance of Findings__ above. 

## Q2 (Reproducibility of our prompt)
While the paper reports on results from a single run of the experiments, we originally ran the experiments with Claude 2, Claude 3 multiple times and did not observe significant variance in the number of successful cases.

We logged all prompts and responses from LLMs, and we will enable our artifact to "replay" these prompts and responses. This will enable other researchers to reproduce our results exactly, and without cost.

## Q3 && Q4 (Were transformations applied to client code/Lacking description of client code)
Please see our discussions in __Description of Client Programs__ above. 



[1] Chen, Mark et al. “Evaluating Large Language Models Trained on Code.” ArXiv abs/2107.03374 (2021): n. pag.
[2] Deligiannis, Pantazis & Lal, Akash & Mehrotra, Nikita & Rastogi, Aseem. (2023). Fixing Rust Compilation Errors using LLMs. 

