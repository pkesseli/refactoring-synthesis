%-----------------------------------------------------------------------------
%               Template for OOPSLA
%               based on:
%               Template for sigplanconf LaTeX Class
%
% Name:         sigplanconf-template.tex
%
% Purpose:      A template for sigplanconf.cls, which is a LaTeX 2e class
%               file for SIGPLAN conference proceedings.
%
% Guide:        Refer to "Author's Guide to the ACM SIGPLAN Class,"
%               sigplanconf-guide.pdf
%
% Author:       Paul C. Anagnostopoulos
%               Windfall Software
%               978 371-2316
%               paul@windfall.com
%
% Created:      15 February 2005
%
%-----------------------------------------------------------------------------
%\documentclass[10pt,conference]{IEEEtran}
%\IEEEoverridecommandlockouts
%\usepackage[scaled]{helvet} % see www.ctan.org/get/macros/latex/required/psnfss/psnfss2e.pdf

%% \BibTeX command to typeset BibTeX logo in the docs
\documentclass[sigconf,review,anonymous]{acmart}
%\documentclass[sigconf,review, anonymous]{acmart}
%\acmConference[ISSTA 2024]{ACM SIGSOFT International Symposium on Software Testing and Analysis}{16-20 September, 2024}{Vienna, Austria}

\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

\makeatletter
\newcommand{\removelatexerror}{\let\@latex@error\@gobble}
\makeatother


\usepackage{microtype}
\usepackage{url}                  % format URLs
\usepackage{listings}          % format code
\lstset{
  mathescape, 
  language={Java},
  numbers=left,
  stepnumber=1,
  basicstyle=\footnotesize,
  xleftmargin=5.0ex
}
\usepackage{enumitem}      % adjust spacing in enums
%\usepackage[colorlinks=true,allcolors=blue,breaklinks,draft=false]{hyperref}   % hyperlinks, including DOIs and URLs in bibliography
% known bug: http://tex.stackexchange.com/questions/1522/pdfendlink-ended-up-in-different-nesting-level-than-pdfstartlink

\usepackage[htt]{hyphenat}
\usepackage{graphicx}
\usepackage{float,subfig}
\usepackage{xspace,framed}
\usepackage{colortbl}
\usepackage{calc}
\usepackage[ruled,vlined]{algorithm2e}

\usepackage{amsthm}
%\newtheorem{definition}{Definition}
%\newtheorem{example}{Example}

%\usepackage{algorithm}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{tikz}
\usepackage[justification=centering]{caption}
\usepackage{stmaryrd}
\usetikzlibrary{positioning, arrows, automata, shapes}
\usepackage{hhline}
\usepackage{pifont}
%\usepackage{cite}
\usepackage{pdflscape} % Experiments table is landscape
\usepackage{longtable}
\usepackage{afterpage}
\usepackage{wasysym}
\usepackage{tcolorbox}
\definecolor{prompt_bg}{RGB}{252,255,221}
\definecolor{prompt_title}{RGB}{0,51,102}
\newcommand{\templatevar}[1]{\$\{\textcolor{red}{#1}\}}

\usepackage[justification=centering]{caption}
\input{macros}

%\setlength{\belowcaptionskip}{-10pt}
%\setlength{\textfloatsep}{1em}
%\setlength{\dbltextfloatsep}{1em}
%\setlength{\abovecaptionskip}{0.1em}
% \floatsep: space left between floats (12.0pt plus 2.0pt minus 2.0pt).
% \textfloatsep: space between last top float or first bottom float and the text (20.0pt plus 2.0pt minus 4.0pt).
% \intextsep : space left on top and bottom of an in-text float (12.0pt plus 2.0pt minus 2.0pt).
% \dbltextfloatsep is \textfloatsep for 2 column output (20.0pt plus 2.0pt minus 4.0pt).
% \dblfloatsep is \floatsep for 2 column output (12.0pt plus 2.0pt minus 2.0pt).
% \abovecaptionskip: space above caption (10.0pt).
% \belowcaptionskip: space below caption (0.0pt).

\allowdisplaybreaks

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% uncomment for extended version 
\newcommand*{\extended}{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% The following oopsla2016 options are available:
%
% 1stsubmission   For the initial submission
% 2ndsubmission   For the 2nd submission
% final           For camera-ready

\begin{document}

%\copyrightdata{978-1-nnnn-nnnn-n/yy/mm} 
%\doi{nnnnnnn.nnnnnnn}

% Uncomment one of the following two, if you are not going for the 
% traditional copyright transfer agreement.

%\exclusivelicense                % ACM gets exclusive license to publish, 
                                  % you retain copyright

%\permissiontopublish             % ACM gets nonexclusive license to publish
                                  % (paid open-access papers, 
                                  % short abstracts)

\title{Quantifying the benefits of code hints in refactoring deprecated Java APIs }
%\title{Symbolic vs. Neural Refactoring of Deprecated APIs} % guided by types and code hints}
%\subtitle{Subtitle Text, if any}
% double-blind submission
% single-blind only for Technical Research: http://icse2017.gatech.edu/technical-research-cfp
 % \authorinfo{Cristina David}
 %            {University of Oxford}
 %            {cristina.david@cs.ox.ac.uk}
 % \authorinfo{Pascal Kesseli}
 %            {University of Oxford}
 %            {pascal.kesseli@cs.ox.ac.uk}
 % \authorinfo{Daniel Kroening}
 %            {University of Oxford}
 %            {kroening@cs.ox.ac.uk}

%% \author{Cristina David}
%% \email{cristina.david@bristol.ac.uk}
%% \orcid{0000-0002-9106-934X}
%% \affiliation{
%%   \institution{University of Bristol}  
%%   \city{Bristol}  
%%   \country{UK}
%%   \postcode{BS8 1TH}
%% }
%% \author{Pascal Kesseli}
%% \affiliation{
%%   \institution{Diffblue}  
%%   \country{UK}
%% }
%% \author{Daniel Kroening}
%% \affiliation{
%%   \institution{Amazon}  
%%   \country{UK}
%% }


%% \author{
%% \mbox{Cristina David}\inst{1} \and
%% Pascal Kesseli\inst{2} \and
%% Daniel Kroening\inst{2}}

%% \institute{University of Bristol, UK 
%% \and
%%   University of Oxford, UK}

%% \author{Cristina David}
%% \affiliation{University of Oxford}
%% \email{cristina.david@cs.ox.ac.uk}

%% \author{Pascal Kesseli}
%% \affiliation{University of Oxford}
%% \email{pascal.kesseli@cs.ox.ac.uk}

%% \author{Daniel Kroening}
%% \affiliation{University of Oxford}
%% \email{kroening@cs.ox.ac.uk}


\begin{abstract}
  As a project evolves, certain fields, methods or classes are
  marked as deprecated in a bid to discourage their future
  use. When done manually, refactoring legacy code in order to eliminate uses of
  deprecated APIs is an error-prone and time-consuming process. In
  this paper, we built a symbolic and a neural approach
  for the automatic refactoring of deprecated APIs.
  The former is based on type-directed and component-based
  program synthesis, whereas the latter uses LLMs.
We implemented our refactoring engines and used them to refactor the deprecated methods from the Oracle
JDK 15 Deprecated API documentation~\cite{OracleJdk15DeprecatedAPI}.
Given the recent success of LLMs for code generation tasks, our goal was investigating whether, for specialised problems
such as the automatic refactoring of deprecated APIs, symbolic methods are still useful, or whether they were completely overtaken by LLM-based ones.
When comparing the abilities of the two engines, our results show 
that the symbolic and neural engines complement each other,
with the symbolic engine still winning for a certain category
of benchmarks, at much lower computational cost.
  
  %propose an automated program refactoring technique
  %that eliminates such uses of deprecated APIs by
  %performing semantic reasoning and search in the space of possible
  %refactorings using automated program synthesis. In particular, the
  %program synthesis is guided by Javadoc code hints and type information,
  %and makes use of fuzz testing. %% We
  %% encode both the synthesis of refactoring candidates and their
  %% verification as program safety problems that we then solve with
  %% coverage-guided property-based testing.
  %
  %% Refactorings are structured changes to existing software that leave its
  %% externally observable behaviour unchanged.  Their intent is to improve
  %% readability, performance or other non-behavioural properties. 
  %% State-of-the-art automatic refactoring tools are {\em syntax}-driven and,
  %% therefore, overly conservative.  In this paper we explore {\em
  %% semantics}-driven refactoring, which enables much more sophisticated
  %% refactoring schemata.  As~an exemplar of this broader idea, we present
  %% an automatic refactoring tool that replaces uses of deprecated legacy APIs/libraries
  %% with the latest ones.
  %% Our refactoring procedure performs
  %% semantic reasoning and search in the space of possible refactorings using
  %% automated program synthesis.  Our experimental results support the
  %% conjecture that semantics-driven refactorings are more precise and are
  %% able to rewrite more complex code scenarios when compared to syntax-driven
  %% refactorings.
%
\end{abstract}

\maketitle

%\keywords{program refactoring, program synthesis, program verification}

%\category{CR-number}{subcategory}{third-level}

% general terms are not compulsory anymore, 
% you may leave them out
%\terms
%term1, term2

%% \keywords
%% Maintenance and reuse



\section{Introduction}\label{sec:intro}


%There are also practical problems

%Implementation -> Practical problem (express them as questions):

%How to isolate .....


%Types: Type-directed synthesis has been particulary successful in functional programming. We want to investigate how successful is in imperative programming.

As a project evolves, there are certain fields, methods or classes
that the developers are discouraged from using in the future as
they've been superseded and may cease to exist.
However, removing them directly would break the backward compatibility
of the project's API.  Instead, such elements can be tagged with the
\texttt{@Deprecated} annotation.  %% In general, when deprecating a
%% field/method/class, the \texttt{@deprecated} Javadoc tag is used in the
%% comment section to inform the developer the reason of deprecation and
%% what can be used in place.


The transformation of existing code such that it doesn't use
deprecated APIs is not always straightforward, as illustrated in
Fig.~\ref{ex:deprecated-method-other}. In the example, we make use of
the \texttt{getHours} method of the \texttt{Date} class, which is
deprecated.  In this situation, in order to replace the use of the
deprecated method, we must first obtain a \texttt{Calendar} object.
However, we can't use the \texttt{Calendar} constructor as it is
protected, and we must instead call \texttt{getInstance}.
Furthermore, in order to be able to use this \texttt{Calendar} object
for our purpose, we must first set its time using the existing
\texttt{date}.  We do this by calling \texttt{setTime} with
\texttt{date} as argument.  Finally, we can retrieve the hour by
calling \texttt{calendar.get(Calendar.HOUR\_OF\_DAY)}.

\begin{figure}[ht]
\begin{lstlisting}[mathescape=true,showstringspaces=false]
void main(String[] args) {
  // Deprecated:
  int hour = date.getHours();
  
  // Should have been:
  final Calendar calendar = Calendar.getInstance();
  calendar.setTime(date);
  int hour = calendar.get(Calendar.HOUR_OF_DAY);
}
\end{lstlisting}
\caption{Deprecated method example.}
\label{ex:deprecated-method-other}
\end{figure}

Another example, which is easy for humans but difficult for automatic techniques, is the call to the static \lstinline[breaklines=true]{java.net.URLDecoder.decode(s)} method, which should be refactored to \lstinline[breaklines=true]{java.net.URLDecoder.decode(s, "UTF-8")}.
However, this requires guessing the ``UTF-8'' constant denoting a platform-specific
string encoding scheme, which is very difficult for automatic code generation techniques, especially those using symbolic reasoning~\cite{DBLP:conf/cav/AbateDKKP18}.
%While it is trivial for IDEs such as Eclipse or IntelliJ IDEA to
%recognise the use of deprecated APIs and warn the programmer about it,
%refactoring the code to a preferred API is significantly more
%complex. Indeed neither Eclipse 4.27.0 nor IntelliJ IDEA 2023.1.1 make
%any attempt to suggest an alternative to the user.

%Program synthesis is generally concerned with code generation via symbolic reasoning. 
%Although powerful, program synthesis is very hard due to the vast search space, i.e.~the space of possible programs that must be considered. Thus, its success is generally reliant on how efficiently it is guided towards a solution by discarding uninteresting programs as early as possible.

Besides general challenges related to automatic code generation, there are other language-specific ones. For instance, the code to be refactored might be using abstract classes and abstract methods, and it may not be obvious how to subclass from the code to be refactored (e.g.~\lstinline[breaklines=true]{engineGetParameter} in \lstinline[breaklines=true]{java.security.SignatureSpi}); it might call methods that, while not abstract, need to be overridden by subclasses (e.g., method \lstinline[breaklines=true]{layout} in class \lstinline[breaklines=true]{java.awt.Component} has an empty body); or it might be calling native methods whose implementation is written in another programming language such as C/C++ (e.g., \lstinline[breaklines=true]{weakCompareAndSet} in \lstinline[breaklines=true]{java.util.concurrent.atomic.AtomicReference}). In order to even understand the behaviour of the code to be refactored, one needs to know how to subclass the abstract classes, override methods, and to understand the behaviour of code written in other languages.

In this paper, we are interested in the automatic generation of refactorings for deprecated APIs (while we focus on the refactoring of deprecated methods, the same
techniques can be applied to deprecated fields and classes). 
%To answer this question, we design both a symbolic and a neural approach based on Large Language Models (LLMs). More than simply generating refactorings for deprecated APIs, given the recent focus on LLM based code generation, we are interested in investigating whether, for specialised problems such as the current one, symbolic approaches can still be of use at much lower computational costs compared to neural ones. An answer to this question is useful especially given that the efficacy of LLMs hinges heavily on their parameter count. Notably, as an answer to this problem, architectures such as
%Mixture-of-Experts (MoE), which adopts a conditional computation paradigm by only selecting parts of an ensemble, referred to as experts, and activating them depending on the data at hand. In the context of such an architecture, one needs to consider what should play as an expert for the current deprecated refactoring problem.
In particular, we are interested in investigating the benefits of Javadoc code hints when automating such a refactoring.

%Let us first  consider the information available when trying to find a refactoring:

%{\bf (1)} The original code and the refactoring must have {\em the same behaviour}, i.e., 
%the generated refactoring needs to be semantically equivalent to the original.
%One implication is that the orginal code and the refactoring must have {\em the same types}. In particular, this consists of the types of the objects to be consumed (i.e., the inputs), as well as the types of the objects that need to be produced (i.e., the outputs). In the example in Figure~\ref{ex:deprecated-method-other}, the \texttt{date} object can be seen as an input with type \texttt{Date}, whereas \texttt{hour} of type \texttt{int} is considered an output.

%{\bf (2)} The Javadoc comments may contain {\em code hints}.

In general, when deprecating a field/method/class, the \texttt{@Deprecated} Javadoc tag is used in the
comment section to inform the developer of the reason for deprecation and, sometimes, what can be used in its place. We call such a suggestion a {\em code hint}.
These hints don't provide the whole refactoring, but can be used to guide the search process.
%
For illustration, let's look at our running example in Figure~\ref{ex:deprecated-method-other}. The source code for the \texttt{getHours} method in class \texttt{Date} is accompanied by the comment in Figure~\ref{ex:code-hints}, where the \texttt{@code} tag suggests replacing the deprecated \texttt{getHours}
with \texttt{Calendar.get(Calendar.HOUR\_OF\_DAY)}.
%It's not straightforward to use this code to generate the refactoring, with some of the challenges being as follows: 
Using this code hint is not straightforward.
Although it may seem as if
method \texttt{get} is static allowing an immediate call,
it is actually an instance method, requiring us to have an object of
class \texttt{Calendar}. However, no such object is available in the
original code meaning that it must be created by the refactored code.
Consequently, we must find the necessary
instructions that consume existing objects and create a \texttt{Calendar} object.
Besides generating the required objects, we also need to set their
fields. For instance,
in Figure~\ref{ex:deprecated-method-other}, we must call
\texttt{calendar.setTime(date)} to set the calendar's date
based on the existing \texttt{date} object.


\begin{figure}
\begin{lstlisting}[mathescape=true,showstringspaces=false]
/*
 * @return  the hour represented by this date.
 * @see     java.util.Calendar
 * @deprecated As of JDK version 1.1,
 * replaced by {@code Calendar.get
 *              (Calendar.HOUR_OF_DAY)}.
 */  
\end{lstlisting}
\caption{Code hints for the running example.}
\label{ex:code-hints}
\end{figure}


%While it might seem that this hint ma solve the entire problem, 

%\todo{possible other challenges: parsing, abstract classes. remove challenge 3.}

%{\bf Challenge 1.}

In order to investigate the benefits of code hints when automating the deprecated refactoring,
we build two code generation engines, namely a symbolic and a neural one.
%In an attempt to make them follow the same general structure, we base them both on the CounterExample Guided Inductive Synthesis (CEGIS)~\cite{DBLP:conf/pldi/Solar-LezamaJB08} architecture, which 
%iteratively improves a candidate refactoring until it obeys a given specification. In our case, the specification expresses the fact that the original and
%the refactored code must behave indistinguishably.

For the symbolic engine, we make use of component-based synthesis~\cite{DBLP:conf/icse/JhaGST10,DBLP:conf/pldi/GulwaniJTV11,DBLP:conf/popl/FengM0DR17}
and type-directed synthesis~\cite{DBLP:conf/sfp/Katayama05,DBLP:conf/pldi/OseraZ15,DBLP:journals/pacmpl/YamaguchiMDW21}. Essentially, we use types and code hints to populate
a component library (i.e., a library of instructions), such that these components are then weaved together to generate the desired program.
Given the recent success of Large Language Models (LLMs) for code generation~\cite{chen2022codet,DBLP:journals/corr/abs-2108-07732,DBLP:journals/corr/abs-2302-05527,DBLP:journals/corr/abs-2202-13169,llmsforcodecompletion,codegenclasslevel,ni2023lever,zhang2023repocoder,lostintranslation,Ding2024cocomic,yang2024sweagent}, the neural approach 
generates candidate refactorings by querying an LLM -- we used Claude 2.1 and Claude 3~\cite{claude}.

%In recent years, Large Language Models (LLMs) have gained a lot of popularity as they have been successfully applied to a wide range of tasks including code generation~\cite{chen2022codet,DBLP:journals/corr/abs-2108-07732,DBLP:journals/corr/abs-2302-05527,DBLP:journals/corr/abs-2202-13169,llmsforcodecompletion,codegenclasslevel,ni2023lever,zhang2023repocoder,lostintranslation,Ding2024cocomic,yang2024sweagent}. While they may seem like the panacea for all code generation problems, they come at a high computational cost as their  the efficacy hinges heavily on the parameter count. The other option when considering code generation are symbolic methods, which make use of hard coded rules~\cite{DBLP:conf/icse/JhaGST10,DBLP:conf/pldi/GulwaniJTV11,DBLP:conf/popl/Gulwani11,DBLP:conf/pldi/OseraZ15,DBLP:journals/pacmpl/YamaguchiMDW21}. Thus, the question we are trying to answer in the current paper is: for specialised problems, such as the automatic refactoring of deprecated APIs, are symbolic methods still useful, or were they completely overtaken by LLM-based ones? Answering this question is particularly important given the rise of architectures such as  Mixture-of-Experts~\cite{DBLP:conf/kdd/MaZYCHC18,DBLP:conf/iclr/Zuo00KHZGZ22,DBLP:conf/ppopp/HeZAWLSL22}, which adopt a conditional computation paradigm by only selecting parts of an ensemble of experts, based on the input data. In the context of such an architecture, one needs to consider the best option for each expert, e.g. the one responsible for the current deprecated refactoring problem.

Our experiments show that code hints are critical for the performance of both the symbolic and the neural engines: when code hints are given, both engines perform well, making the automation of deprecated APIs refactorings feasible. Without code hints, the refactoring becomes much harder for both engines, such that the majority of benchmarks without code hints are failing. Thus, our conclusion is that, in order to facilitate
the automation of the refactoring of deprecated APIs, code hints should be added to all deprecated methods that are expected to have a replacement.

As a side observation, when looking at the comparison of the symbolic and the neural approaches, code hints help the symbolic approach to efficiently prune the solution space, resulting in a better performance than the neural one at a lower monetary cost. We believe this is important to note, especially as LLMs are often seen as a panacea for all tasks, including code generation. This work shows that symbolic methods can still be effective in specialised settings, where efficient pruning of the solution space is possible. 



%Our experiments show that, if the Javadoc contains code hints, thus allowing us to build an effective instruction library (as shown in Section~\ref{sec:components-seeding}), then the symbolic
%approach is preferable to the neural one.
%In the absence of such code hints, when we can only use types to guide the code generation,
%both approaches perform worse, with the neural one
%slightly better. This makes sense as a general strategy: whenever there is enough information about
%the solution to effectively prune the solution space (in our case, when code hints are present), symbolic code generation can work effectively, at low computational cost.
%Otherwise, LLMs may have a better chance.
%Thus, one can envision a combined strategy, where symbolic and neural reasoning work together to
%provide the best results for a lower budget than a neural approach alone would require.


%While for the neural approach, the code hints become part of the prompt input to the LLM, in the symbolic approach 
%we use them as the building blocks
%of the new refactoring. 
%For this purpose, we take inspiration from component-based program synthesis,
%which weaves together components from a library
%(typically methods from an API) in order to generate the desired program \cite{DBLP:conf/icse/JhaGST10,DBLP:conf/pldi/GulwaniJTV11,DBLP:conf/popl/FengM0DR17}.


 %Traditionally, types are commonly used to guide program synthesis. The idea of type-directed synthesis has been used for generation of functional programs, where the absence of side-effects
 %enables types to provide a comprehensive description of a program's behaviour~\cite{DBLP:conf/sfp/Katayama05,DBLP:conf/pldi/OseraZ15,DBLP:journals/pacmpl/YamaguchiMDW21}. Thus, our work also serves as an investigation of the usefulness of type information in generating imperative code.


%For both approaches, we make use of a CounterExample Guided Inductive Synthesis (CEGIS)~\cite{DBLP:conf/pldi/Solar-LezamaJB08} architecture, where we
%iteratively attempt to improve a candidate refactoring (generated using a component library as built in Section~\ref{sec:components-seeding}) until it obeys a given specification.
%As it will be explained later in the section, for the neural engine, the CEGIS architecture is not followed exactly by the synthesis phase.


%\todo{unsure whether this fits, if at all.}
%For code generation, in the literature, there is a focus on the functional properties of the generated code, which generally refers to correctness. In the case of program refactoring,
%there are also qualitative expectations as the refactoring is meant to improve the code quality (while also maintaining functional correctness).





%In this paper, we investigate two of the most common types pf program synthesis: type-directed synthesis and component-based synthesis. In particular, we want to see how a symbolic and a neural approach comapre against each other for for of these cases.

%* Type-directed synthesis has been widely used for functional programs, but less so for procedural programming. The exception is using very sophisticates type systems such as graded linear type systems, or ... In this work, our focus is on Java so we rely on a traditional type system. On the side of the neural approach, we are not aware of any type-based investigation. 
%* For component ... For the LLM-based approach, this translates into providing the daprecates message and allowing the LLM to make use of the corresponding code hints.


%While symbolic approaches have been investigated for a long time, recently, there is a new direction for program synthesis powered by the evolution of LLMs.
%In this paper, we will contrast the two approaches:
%*  a symbolic, type and components based approach
%* a neural approach based on LLMs


%RQs:
%* Can LLMs reason about type?
%* Can LLMs follow component based synthesis? 




%The questions that we are trying to answer are:
%\begin{itemize}
%\item[RQ1] How do the two approaches take advantage of type information?
%\item[RQ2] How do the two approaches take advantage of the deprecation message?
%\item[RQ3] How do the two approaches deal with qualitative requirements?
%\end{itemize}  


%* For the symbolic approach, we are interested in how type-directed synthesis works for Java. This has been effective mostly in functional programming, where the types provide a comprehensive description of the code. How does this extend to Java?
%For the symbolic approach, we will answer the first question by using type-directed synthesis, and the second by using component-based synthesis.


%* For the neural approach, we will construct the prompt in two different ways:
%** firstly, we only provide the type signature.
%** then, we also provide the deprecated message containing the code hints.

%* What happens when we add components?
%* How does the LLM work with components?

%Usually for LLMs, we expect a natural language description of the task and a few i/o examples. In this work, we investigate their abilities in a different setting, which is borrowed from symbolic synthesis: we provide types and components.

%We are the first to investigate ....


%For both approaches, we have an iterative approach where in each iteration:
%* we use a synthesiser (symbolic or neural) to get a solution
%* we verify it for semantic equivalence to the deprecated function
%* if equivalent, we return to the user
%* if not true, than we provide the counterexample back to the synthesis engine. 

%For LLM, I want the following:
%* results just with types
%* results with code hints.

%For the LLM results, I want:
%* unsound solution
%* correct solution that obeys the code hint
%* correct solution that doesn't obey the code hint
%* incorrect solution (because it has loops)

%Q: Should we provide the code of the deprecated function in the prompt?





%% There is also
%% very limited support offered by the research community, mainly
%% focusing on replacing calls to deprecated methods by their
%% bodies~\cite{DBLP:conf/paste/Perkins05}. 



%\emph{Current solution.}  In this paper, we investigate an approach for automatically refactoring uses of deprecated APIs based on program synthesis.
%To verify that the refactoring maintains the behaviour of the original code, we design a semantic equivalence check.

%Additionally, we are interested in comparing our symbolic approach against a neural one using Large Language Models (LLMs), which we augment with our semantic equivalence checking procedure.
%In particular, given the recent advances that LLMs made in code generation,
%we want to investigate whether they completely subsume symbolic methods, or whether, for specialised tasks such as the current refactoring,
%symbolic methods are still useful at lower computational costs.
%We picked GPT-4 Turbo.
%There are several studies of ChatGPT's abilities to solve software engineering problems such as bug fixing~\cite{sobania2023analysis,chatgptbugs} and 
%prompt pattern design for refactoring, requirements elicitation, and software design ~\cite{white2023chatgpt}.
%While, in this paper, we focus on the refactoring of deprecated methods, the same
%technique can be applied to deprecated fields and classes.


%The first one is a symbolic approach based on Counter-Example Guided Inductive Synthesis (CEGIS), whereas the second is neural approach based on Large 
%
%propose an
%automated refactoring technique for replacing uses of deprecated APIs that offers guarantees about the preservation of semantics.
%Additionally, we investigate its abilities to those of LLMs from the perspective of 

%% 1. {\em Program synthesis based refactoring:} 
%% We employ Counter-Example Guided Inductive Synthesis (CEGIS) to generate program refactorings. 
%% %
%% Although powerful, program synthesis is very hard due to the vast search space, i.e.~the space of possible programs that must be considered. Thus, its success is generally reliant on how efficiently it is guided towards a solution by discarding uninteresting programs as early as possible. Most commonly, the information used to guide the search is represented by input/output examples~\cite{DBLP:conf/pldi/FeserCD15}, type signatures~\cite{DBLP:conf/pldi/OseraZ15}, or full functional specifications of the expected code~\cite{DBLP:conf/ijcai/MannaW79}.
%% %
%% The vast search space is particularly a challenge in the current
%% setting given that, in the absence of any additional guidance, the
%% entire \texttt{java.lang} could be considered when synthesising the
%% refactoring.  In order to prune the search space, we make use of type information and Javadoc code hints.

%% 2. {\em LLMs based refactoring:} 
%% Recently, Large Language Models (LLMs) have made a lot of progress in code generation tasks. 
%% These models gain remarkable capabilities on a host of language tasks when prompted with just a problem description.
%% Thus, we are interesting in investigating how the refactorings generated by such models compare to those generated by program synthesis,
%% especially with respect to guaranteeing the preservation of semantics. In this work, we use
%% GPT-4 Turbo, which is a LLM trained using Reinforcement Learning from Human Feedback.
%% %
%% For the prompt, we use the same information provided to the program synthesiser, namely the signature of the deprecated method (which includes the types) and the Javadoc code hints.

%However, the responsibility of guaranteeing that the generated code obeys the specification is left to
%the programmer.

%% We are interested in answering the following research questions:
%% \begin{itemize}
%% \item[RQ1] Can GPT-4 Turbo replace symbolic code generation techniques such as our program synthesis and fuzzing based refactoring?
%% \item[RQ2] How often does Chat GPT generate refactorings that preserve the semantics of the original code?
%% \item[RQ3] Do code hints help the generation of refactorings compared to types?
%% \end{itemize}

%% We believe RQ1 is particularly important given the high computational cost of LLMs. Our results highlight the fact that there is room for
%% symbolic code generation methods to outperform general-purpose LLMs at a much lower cost.

%\paragraph{Existing approaches to automated program refactoring}
%One of the existing directions is to 
%replace calls
%to deprecated methods by their
%bodies~\cite{DBLP:conf/paste/Perkins05}.
%However, this
%strategy doesn't usually follow the intention of the language designers, who
%use deprecation to signal that there is a safer and generally better
%way of achieving a given goal.
%Other direction is specifically targeted at the Android API, and requires existing examples of how the updates were handled by other programmers~\cite{DBLP:conf/issta/FazziniXO19,DBLP:conf/iwpc/HaryonoTKSML0J20,DBLP:journals/ese/HaryonoTLJLKSM22}.
%Conversely, we do not require such example, and instead use type information and the Javadoc deprecation comments.


%Another closely related class of works focus on API adaptation after library updates~\cite{DBLP:conf/icse/WuGAK10,DBLP:conf/kbse/Huang0PW021}. These works use techniques such as call dependency and text similarity analyses to automatically identify change rules linking different library releases. Essentially, a change rule describes a match between methods existing in the old release but which have been removed or deprecated in the new one, and replacement methods in the new release. For instance, for our running example in Figure~\ref{ex:deprecated-method-other}, a change rule would inform the programmer that  method \texttt{getHours} from class \texttt{Date} must be replaced by method \texttt{get} from class \texttt{Calendar}. Notably, {\em change rules are different from refactorings} as they don't provide the actual replacement code. Instead, the programmer is still required to perform a lot of the work
%manually: for instance, she has to figure out lines 6--8 for the refactoring in Figure~\ref{ex:deprecated-method-other}. This is a time-consuming and error-prone process, especially as the
%responsibility of guaranteeing that the refactored code maintains the original behaviour
%is left to the programmer.

%Indeed, when referring back to the example in
%Figure~\ref{ex:deprecated-method-other}, it's not immediately obvious
%that the original and the refactored code behave in the same manner.
%One needs a good understanding of the code's semantics in order to
%reach such a conclusion. As we will see later in the paper, guaranteeing semantic equivalence is particularly difficult in the presence of aliasing, when there may be multiple references to the same memory location.
%On the other hand, refactorings that rely solely on a
%program's syntax and do not consider its semantics end up being overly
%conservative, mostly addressing structural changes to the program.
%The observation that refactoring generally benefits from semantic
%analyses of a program is not new, e.g. previous works made use of
%program invariants~\cite{Kataoka:2001:ASP:846228.848644}, or
%types~\cite{Steimann2011,Steimann2012Pilgrim,Steimann2011KollePilgrim}.



\paragraph{Contributions:}

\begin{itemize}

\item We proposed a symbolic and a neural refactoring technique for deprecated APIs. The former makes use of type-directed and component-based synthesis, whereas the latter is LLM based. In order to check the correctness of the refactorings, we designed an equivalence check, which takes into consideration both the state of the stack and the heap.
  %The technique incorporates semantic information about the code and it is guided by Javadoc code hints and type information.

%\item We encode both the synthesis of refactoring candidates and their verification as program safety problems that we then solve with fuzz testing~\cite{DBLP:conf/issta/PadhyeLS19}.

\item We implemented our techniques and used them to refactor deprecated methods from the Oracle
JDK 15 Deprecated API documentation~\cite{OracleJdk15DeprecatedAPI}.

\item We investigated the benefits of code hints for both the symbolic and neural approach. %In particular, we want to find out whether there is still space for specialised techniques
  %such as ours in a GPT-4 Turbo era.
  %\todo{come back to this after investigating experimental results}
  Our results show that code hints are critical for the performance of both the symbolic and the neural engines: adding more code hints to Javadoc
can substantially help automate the refactoring of deprecated APIs.
  %when code
%hints are given, both engines perform well, making the automa-
%tion of deprecated APIs refactorings feasible
 % Our results highlight the fact that the symbolic and neural methods can be used in a complementary fashion, which reduces the computational cost.
%symbolic code generation (which require much less compute) to outperform general-purpose LLMs in specialised settings, at a lower computational cost.


\end{itemize}  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%The fourth challenge is addressed
%by designing an equivalence relation, which we then check with fuzz testing.
%While we can't provide full correctness guarantees,
%given a large number of test inputs, we consider this a close approximation.
%More details about the semantic equivalence check are given in Section~\ref{sec:equiv}.
%Additionally, in Section~\ref{sec:encoding}, we provide technical details about the synthesis and fuzzing based refactoring procedure, 
%and in Section~\ref{sec:implementation} we discuss some implementation details.


\section{Our approach} \label{sec:overview}
%\todo{Make this more general, to cover both neural and symbolic.}

%\section{Program synthesis based refactoring} \label{sec:encoding}

%% Once the component library is built, for the actual synthesis,
%% we use a counterexample-guided iterative process that enables us
%% to keep refining a candidate refactoring until we are happy with it.
%% The refinement is based on the results provided by 
%% fuzz testing.
%% We provide more details about %
%% the synthesis process in Section~\ref{sec:encoding}.


%% At a high level, the new code is obtained by composing methods
%% that are accessible from that respective location. 
%% Consequently, we take inspiration from component-based program synthesis,
%% which weaves together components from a library
%% (typically methods from an API) in order to generate the desired program \cite{DBLP:conf/icse/JhaGST10,DBLP:conf/pldi/GulwaniJTV11,DBLP:conf/popl/FengM0DR17}.

%% For component-based synthesis, real-world
%% APIs can quickly lead to very large component libraries. %, making synthesis infeasible.
%% For illustration, in our setting, indiscriminately adding all members
%% found in \texttt{java.lang} would make synthesis infeasible. % (see experimental results in Section~\ref{sec:experimental-results}).
%% %
%% Previous works have used type information to make components-based synthesis
%% feasible in the presence of large component
%% libraries~\cite{DBLP:conf/popl/FengM0DR17}.  However, as shown by our
%% experimental results in
%% Section~\ref{sec:experimental-results}, the use of types in our setting is still insufficient.


%\section{Synthesising the refactoring}\label{sec:encoding}



%We make the symbolic and the neural approach as similar as possible. In particular, they are both iterative, such that, in each iteration, a candidate refactoring is being generated, which is then verified for semantic equivalence with the original code. If the verification succeeds, then the solution is returned to the user. Otherwise, the counterexample is used  to refine the current solution.

%While for the neural approach, the code hints become part of the prompt input to the LLM, in the symbolic approach 
%we use them as the building blocks
%of the new refactoring. 
%For this purpose, we take inspiration from component-based program synthesis,
%which weaves together components from a library
%(typically methods from an API) in order to generate the desired program \cite{DBLP:conf/icse/JhaGST10,DBLP:conf/pldi/GulwaniJTV11,DBLP:conf/popl/FengM0DR17}.


 %Traditionally, types are commonly used to guide program synthesis. The idea of type-directed synthesis has been used for generation of functional programs, where the absence of side-effects
 %enables types to provide a comprehensive description of a program's behaviour~\cite{DBLP:conf/sfp/Katayama05,DBLP:conf/pldi/OseraZ15,DBLP:journals/pacmpl/YamaguchiMDW21}. Thus, our work also serves as %an investigation of the usefulness of type information in generating imperative code.

%As it will be explained later in the section, for the neural engine, the CEGIS architecture is not followed exactly by the synthesis phase.
In this section, we describe the two code generation engines.
For both approaches, we make use of a CounterExample Guided Inductive Synthesis (CEGIS)~\cite{DBLP:conf/pldi/Solar-LezamaJB08} architecture, where we
iteratively attempt to improve a candidate refactoring
%(generated using a component library as built in Section~\ref{sec:components-seeding})
until it behaves indistiguishably
from the orignal code, i.e., the original and
the refactored blocks of code are observationally equivalent.
%
In the rest of the paper, we will use the predicate
$equivalent(P_1(\vec{i}), P_2(\vec{i}))$ to denote that code $P_1$ is
observationally equivalent to $P_2$ for inputs $\vec{i}$, meaning that the two programs
can't be distinguished by their behaviour on inputs $\vec{i}$.  In general,
we refer to the original code as $P_1$ and the refactored code as
$P_2$. We will fully define what $equivalent$ actually means in Section~\ref{sec:equiv}.

In each iteration of the synthesis process, there are two phases, a synthesis phase and a verification phase. The synthesis phase generates a {\em candidate refactoring} that is equivalent to
the original code on a finite set of inputs. Then, the verification phase tries to find a new {\em counterexample input} that distinguishes between the current candidate refactoring and the original code. If it manages, this input is added to the set of finite inputs used by the next synthesis phase, and if it fails, then the current candidate is indeed a solution refactoring.

%We next show how we encode the problem corresponding to each of the two phases. This is done so that fuzz testing can be used as the safety checker.

\subsection{Synthesis phase}\label{sec:synthesis}
In this phase, we have 
a finite set of input examples $\{\vec{i_1} \cdots \vec{i_n}\}$ and we attempt to find a candidate refactoring
that is observationally equivalent to the original code for the given inputs.

\paragraph{The symbolic engine}
We construct the following \texttt{Synthesise} method, which
takes the refactored code $P_2$ as input.

\begin{lstlisting}[mathescape=true,showstringspaces=false]
Synthesise ($P_2$) {
  if (equivalent($P_1$($\vec{i_1}$), $P_2$($\vec{i_1}$)) && ...
      && equivalent($P_1$($\vec{i_n}$), $P_2$($\vec{i_n}$)))
    assert(false);
}
\end{lstlisting}

This method consists of only one conditional saying that if $P_1$ and $P_2$ are equivalent on all given inputs $\vec{i_1}, \cdots, \vec{i_n}$, then \texttt{assert(false)} is reached.
Given that this assertion always fails if reached, it means that the method is unsafe when $P_1$ and $P_2$ are equivalent on the given inputs.
Thus, we can reduce the synthesis problem to the problem of checking the safety of \texttt{Synthesise}. If a safety checker manages to
find an input $P_2$ for which the assertion fails (meaning that \texttt{Synthesise} is unsafe), then this $P_2$ must be equivalent to $P_1$ on the given inputs. This $P_2$ is the {\em refactoring candidate}
that the synthesis phase is supposed to generate.

We use an existing fuzz testing platform for Java, JQF~\cite{DBLP:conf/issta/PadhyeLS19},
as the safety checker. JQF is designed to handle structured inputs, where inputs of type \texttt{T}
are generated by a backing \texttt{Generator<T>}. JQF provides a library of
generators for basic types such as primitive values. We implement custom
JQF generators based on a library of instructions built as explained in Section~\ref{sec:components-seeding},
enabling JQF to construct $P_2$ by weaving together instructions from the library.

If the safety checker fails to find a $P_2$ for which the assertion fails, then the overall synthesis technique fails to generate a solution refactoring.
There could be two reasons for this situation, which we cannot differentiate between.
Firstly, there may indeed be no $P_2$ that can be constructed with the available components in the component library such that
it is equivalent to $P_1$ on the given inputs. Secondly, this could be caused by JQF's unsoundness.
Given that fuzzers rely on testing, they may fail to find inputs that trigger unsafe behaviours even when such inputs exist.
Consequently, we cannot guarantee that we will always find a refactoring whenever one exists. We will provide more details about our decision to use fuzzing as the safety checker in 
Section~\ref{sec:implementation}.

%For the neural approach, we query the LLM to obtain a candidate refactoring. Similarly to the symbolic approach, we use counterexamples found in prior verification
%iterations. However, as opposed to the symbolic approach, the LLM doesn't give any guarantees that the counterexamples were taken into consideration.

\paragraph{Neural engine}
For each deprecated method, we query the corresponding LLM by constructing a prompt as shown in \autoref{fig:prompt}. The method
definition and the JavaDoc comment (if present) are provided in the context. Furthermore, we
provide a code snippet of the deprecated method usage to be refactored
(e.g. \lstinline{this.minimumSize();}). Finally, we attach a list of formatting constraints.

%As explained in Section~\ref{sec:overview},
The code returned by the LLM is verified against the original code. 
If the verification fails, we attach the set of counterexamples generated by the fuzzing
engine to subsequent prompts. We apply object serialisation to obtain structured-views of the input and output states.
In addition to this, we followed the prompting guidelines provided by Anthropic Claude\footnote{https://docs.anthropic.com/en/docs/prompt-engineering} (e.g. using XML tags).
Do note that, as opposed to the symbolic approach, the LLM doesn't give any guarantees that the counterexamples were actually taken into consideration.


\begin{figure}
  \centering
  \begin{tcolorbox}[
    colback=prompt_bg,
    colframe=prompt_title,
    title=Prompt Template,
    subtitle style={boxrule=0.4pt, colback=yellow!50!blue!25!white, colupper=black}
  ]
  \scriptsize

  \tcbsubtitle{Initial Prompt}
  \textbf{User:}\\
  \textit{\# Context}\\
  \texttt{The method \templatevar{METHOD\_NAME} of the class \templatevar{CLASS\_NAME} is deprecated. Below is the method definition:}\\
  \texttt{\templatevar{METHOD\_DEFINITION}}\\
  % \texttt{The method \templatevar{METHOD\_NAME} of the class \templatevar{CLASS\_NAME} is deprecated. Below is the method signature:}\\
  % \texttt{\templatevar{METHOD\_SIGNATURE}}\\

  \texttt{Here are its Javadoc comments that may contain a @deprecated tag explaining why the item has been deprecated and suggesting what to use instead.}\\
  \texttt{\templatevar{JAVADOC\_COMMENT}}\\

  \texttt{However, I used this method call in my code base, the code snippet is given below:}\\
  \texttt{\templatevar{CODE\_SNIPPET}}\\

  \textit{\# Instruction}\\
  \texttt{Help me refactor this code snippet so that it doesn't use the deprecated method. Do not simply inline the method body, use APIs suggested by the Javadoc comments if there are any.}\\

  \textit{\# Constraints}\\
  \texttt{Take the following constraints into consideration:}\\
  \texttt{\templatevar{FORMATTING\_CONSTRAINTS}}\\

  \tcbsubtitle{Subsequent Prompt}
  \texttt{\textcolor{gray}{Additionally, here is a set of input/output examples that you should respect,}}\\
  \texttt{\templatevar{EXAMPLES}}\\

  \textbf{Assistant:}\\
  \end{tcolorbox}
  \caption{LLMs Prompt Template.}
  \label{fig:prompt}
\end{figure}



%Also, we found that the neural engine works better when we don't provide the previous faulty refactoring in the prompt,
%as that seems to make the LLM less likely to fix initial mistakes.

%% In other words, answering the initial question posed by the
%% synthesis phase is now reduced to checking the safety of the \texttt{Synthesise} method.
%% If the method is unsafe meaning that the assertion is reached and failed (if reached, the assertion \texttt{assert(false)} always fails)

%% Again, we reduce the problem to be solved in the synthesis phase to
%% the problem of checking the safety of the \texttt{Synthesise} method,
%% which we do by fuzzing.
%% The fuzzer will find an input \var{Candidate} that fails the assertion,
%% meaning that it returns the same output as \texttt{Orig}
%% on the finite set of inputs \var{x_1, \cdots, x_n}.


%% iterative process that enables us to keep refining a candidate refactoring until we are happy with it. This process consists of two phases, described next:
%%   \begin{itemize}
%%   \item[{\bf Phase 1:}] Given a fixed set of inputs $In$, find a refactoring that works for $In$. For this purpose, we encode   
%%     the synthesis problem as a verification problem that can be solved by fuzzing. We discuss this problem encoding in
%%     Section~\ref{sec:encoding}. The found refactoring is called a candidate.
%%   \item[{\bf Phase 2:}] Given the candidate synthesised at the previous step, verify whether it is indeed a correct refactoring.
%%     Again, we answer this question by encoding it as a software verification problem to be solved by fuzzing (see Section~\ref{sec:encoding}).
%%     If an input for which the candidate isn't observationally equivalent to the original code is found, then
%%     we return to the previous step and add this input to the set $In$. Otherwise we have found a correct refactoring and we are done.
%%   \end{itemize}


\subsection{Verification phase}
This is exactly the same for the two approaches.
We are provided with a candidate
refactoring $P_2$ and we must check whether there exists any input
$\vec{i}$ for which the original code and the candidate
refactoring are not observationally equivalent.  To do this, we build
the following \texttt{Verify} method, which given some input
$\vec{i}$, asserts that the two programs are equivalent for
$\vec{i}$.

\begin{lstlisting}[mathescape=true,showstringspaces=false]
Verify($\vec{i}$) { assert(equivalent($P_1(\vec{i})$, $P_2(\vec{i})$)); }
\end{lstlisting}

In other words, answering the question posed by the verification
phase is reduced to checking the safety of this method: if
\texttt{Verify} is safe (i.e., the assertion is not violated) for any
input~$\vec{i}$, then there is no input that can distinguish
between the original code and the candidate refactoring (this
is indeed a sound refactoring). However, if \texttt{Verify} is not safe,
then we want to be able to obtain a {\em counter\-example input}
$\vec{i_{cex}}$ for which the assertion fails.
%
This counterexample will be provided back to the synthesis phase and
used to refine the current candidate refactoring. Again, we check the
safety of \texttt{Verify} with JQF.
%which means that we may fail to find a distinguishing input even if one
%exists.

In this section, we hid the complexity of the equivalence check inside
the $equivalent$ predicate. This is far from trivial as it requires handling
of notions such as aliasing, loaded classes, static fields etc.. This will be
described in detail next.

\subsection{Checking program equivalence}\label{sec:equiv}

A core part of the synthesis procedure is the $equivalent(P_1(\vec{i}), P_2(\vec{i}))$ predicate,
which checks that the original code $P_1$ and a candidate refactoring $P_2$
are equivalent for a given input $\vec{i}$. %(where the inputs are obtained by fuzzing, as explained in the previous section).
%This is used for both the symbolic and the neural approach.
In this section, we provide details on how we check this equivalence.

The state of a Java program is modelled by the current program stack
(consisting of method-specific values and references to objects in the
heap) as well as its heap (consisting of instance variables and static
field values). Static field values are stored with their respective
classes, which in turn are loaded by class loaders.  Our equivalence
check must take all these into consideration. One of the main challenges is aliasing, which we will discuss later in the section.

We start by introducing some notation:
\begin{itemize}
\item \var{loadedClasses(P)} returns the set of classes loaded by the class loader
  in which \var{P} is executed. Note that Java allows to load the same class
in different class loaders, which creates independent copies of its
static fields.
\item \var{liveVars(P)} provides the set of variables that are live
at the end of $P$. %% Thus, \var{liveVars(P_1)} gives us the
%% variables that have been updated in the original code \var{P_1} and are
%% live at the end of \var{P_1}.
\item \var{staticFields(C)} returns
  the set of static fields of a class \var{C}.
\item $exc(P, \vec{i})$ returns the exception thrown by $P$'s execution on input $\vec{i}$.  
\end{itemize}  

\begin{example}\label{ex:defs}
  For our running example in Figure~\ref{ex:deprecated-method-other},
  \var{P_1} and \var{P_2} are represented by the following lines of code. Note that
  both $P_1$ and $P_2$ take variable $date$ as input.

\begin{lstlisting}[mathescape=true,showstringspaces=false]
  // $P_1$:
  int hour = date.getHours();

  // $P_2$:
  final Calendar calendar = Calendar.getInstance();
  calendar.setTime(date);
  int hour = calendar.get(Calendar.HOUR_OF_DAY);
\end{lstlisting}
%
Then, we have:
\[
\begin{aligned}[t]
  \var{liveVars(P_1)} &= \{date, hour\}\\
  \var{liveVars(P_2)} &= \{calendar, date, hour\}\\  
  \var{loadedClasses(P_1)} &= \{Date\} \\
  \var{loadedClasses(P_2)} &= \{Calendar, Date\} \\  
  \var{staticFields(Date)} &= \emptyset\\
  \var{staticFields(Calendar)} &= \{DATE, YEAR, \cdots\}\\
  \var{exc(P_1, \_)}=\var{exc(P_2, \_)} &=\emptyset
\end{aligned}
\]
\end{example}

%Note $exc$ returns $\emptyset$ regardless of the value of $date$.

In order to extract the (last) object assigned to a variable $v$ by the execution of $P$ on a specific input $\vec{i}$,
we will use the notation $E[P(\vec{i})](v)$. Essentially, if we consider the trace generated by executing $P(\vec{i})$,
then $E[P(\vec{i})]$ maps each variable defined in $P$ to the last object assigned to it by this trace.
Next, we define the notion of equivalence with respect to a concrete input $\vec{i}$ (this definition is incomplete and we will build on it in the rest of the section).
We overload equality to work over sets (for live variables and loaded classes).

\begin{definition}[Program equivalence with respect to a concrete input $\vec{i}$ {\bf[partial]}]\label{def:prog-equiv}
  Given two code blocks $P_1$ and $P_2$ and concrete input~$\vec{i}$,
  we say that \var{P_1} and \var{P_2} are equivalent
  with respect to $\vec{i}$, written as $equivalent(P_1(\vec{i}), P_2(\vec{i}))$
  if and only if the following conditions hold:
%
%% \[
%% \begin{aligned}
%%   & (1)~ exc(P_1, \vec{i})=\{e_1\} \wedge exc(P_2, \vec{i})=\{e_2\} \wedge equals(e_1,e_2) ~\vee\\
%%   & \qquad exc(P_1, \vec{i})=exc(P_2, \vec{i}) =\emptyset \\ 
%%       & (2)~ \var{liveVar}(P_1) = \var{liveVar}(P_2) \wedge \forall v\in \var{liveVar}(P_1). \\
%%       & \qquad equals(E[P_1(\vec{i})](v), E[P_2(\vec{i})](v))\\
%%   & (3)~  \var{loadedClasses}(P_1) = \var{loadedClasses}(P_2) ~\wedge\\
%%   & \qquad \forall C \in \var{loadedClasses}(P_1). \forall f {\in} \var{staticFields}(C).\\
%%   & \qquad equals(E[P_1(\vec{i})](f), E[P_2(\vec{i})](f))
%%     \end{aligned}
%%     \]


\[
\begin{aligned}
  & (1)~ exc(P_1, \vec{i})=\{e_1\} \wedge exc(P_2, \vec{i})=\{e_2\} \wedge equals(e_1,e_2) ~\vee\\
  & \qquad exc(P_1, \vec{i})=exc(P_2, \vec{i}) =\emptyset \\ 
      & (2)~ \var{liveVar}(P_1) = \var{liveVar}(P_2) \wedge \forall v\in \var{liveVar}(P_1). \\
      & \qquad equals(E[P_1(\vec{i})](v), E[P_2(\vec{i})](v))\\
  & (3)~  \var{loadedClasses}(P_1) = \var{loadedClasses}(P_2) ~\wedge\\
  & \qquad \forall C \in \var{loadedClasses}(P_1). \forall f {\in} \var{staticFields}(C).\\
  & \qquad equals(E[P_1(\vec{i})](f), E[P_2(\vec{i})](f))
    \end{aligned}
    \]

 
  \end{definition}


%awt.component show
%awt.component hide

The above definition says that in order for \var{P_1} and
\var{P_2} to be equivalent with respect to input $\vec{i}$,
(1) either they both throw an exception and the two exceptions have the same type, or none does,
(2) the set of variables live at the end of \var{P_1} is the same as the set of variables live at the end of  \var{P_2}, and must be
assigned equal objects by the executions of \var{P_1} and
\var{P_2} on $\vec{i}$, respectively, and
(3) the classes loaded by \var{P_1} must be the same as the classes loaded by \var{P_2}, and
all the static fields in the classes loaded by both the class loaders of \var{P_1} and \var{P_2} must be assigned equal objects by the
two executions, respectively.  
%\todo{this may be mentioned in several places}
In our implementation, if either the deprecated or the refactored code didn't load a class, we load it for it,
and check that the initial state of that class is the same for both blocks of code.

%\todo{check the description about equals}
Equality refers to value equality, which we
check by recursively following attribute chains until we reach
primitive types.
We generally do not consider existing \texttt{equals} methods
unless (i)~the method was not written by the user (i.e.,  JCL classes),
(ii)~the type inherits from \texttt{java.lang.Object}, (iii)~the class
implements \texttt{java.lang.Comparable} and (iv)~the type declares an
\texttt{equals} implementation.  Examples of classes that satisfy these
strict requirements are \texttt{java.lang.Integer} or
\texttt{java.util.Date}.  All other implementations of \texttt{equals} are
considered unreliable and ignored.

%Alternatively, if there is an explicit \texttt{equals} implemented for a given type, then we use it.
%% By equal objects we mean objects where the values of the
%% corresponding attributes are equal.


%% Note that, as mentioned in its name, Definition~\ref{def:prog-equiv} only partially
%% defines the equivalence of \var{P_1} and \var{P_2} on input \var{\vec{x}}. For now
%% we will ignore this and get back to it later in the section.


%In our experiments, given that we only refactor deprecated methods, the only live variable at the end of the original invocation is \var{this}.

\begin{example}\label{ex:equiv}
  When applying Definition~\ref{def:prog-equiv} to \var{P_1} and \var{P_2} given in Example~\ref{ex:defs},
  the following conditions must hold (given that class \var{Date} has no static fields, the third condition is trivial):
\[
\begin{aligned}
      & (1)~ \var{exc(P_1)}=\var{exc(P_2)} =\emptyset\\
      & (2)~ equals(E[P_1(\vec{i})](hour), E[P_2(\vec{i})](hour)) ~ \wedge\\ %date\in \{date,calendar\} ~ \wedge \\
      & \qquad equals(E[P_1(\vec{i})](date), E[P_2(\vec{i})](date))\\
& (3)~  \var{Date} \in \{\var{Date}, \var{Calendar}\}
    %%   throws an exception and   $equiv(exception(R_1), exception(R_1))$. 
  %% \item \var{R_1} doesn't throw an exception and $equiv(R_1,R_2)$ and
   %% $\forall f_i \in getStatic(class(R_1)) \wedge g_i \in getStatic(class(R_2)). equiv(f_i,g_i)$.
    \end{aligned}
    \]

 
    %% Then, in order for the two portions of code to be
    %% equivalent with respect to an input \var{x}, the objects assigned to variable \var{date} by the execution of  \var{P_1(x)} and \var{P_2(x)}
    %% must be equivalent. We will discuss checking equivalence of two objects in two distinct executions next.
    
\end{example}

%%\todo{define $equivalent(E[P_1(x)](v), E[P_2(x)](v))$}.

{\bf The challenge of aliasing.}
%
When expressing the equivalence relation between \var{P_1} and \var{P_2}, we
intentionally missed one important aspect, namely aliasing.  To understand
the problem let's look a the following example, which makes use of
java.awt.Container, where a generic Abstract Window Toolkit (AWT) container
object is a component that can contain other AWT components.  Method
\texttt{preferredSize} used by the original code below returns the preferred
size of the calling container. Method
\texttt{preferredSize} is deprecated, and, instead, the refactored version
uses \texttt{getPreferredSize}.

%, which denotes a slightly
%modified version of the running example.

\begin{example}\label{ex:aliasing}
~\begin{lstlisting}[mathescape=true,showstringspaces=false]
  // $P_3$:
  Dimension dim1 = container.preferredSize();
  Dimension dim2 = container.preferredSize();    

  // $P_4$:
  Dimension dim1 = container.getpreferredSize();  
  Dimension dim2 = dim1;
\end{lstlisting}
%\end{example}


%\todo{Get another example from Pascal, where the return is an object.}

%% \begin{example}\label{ex:aliasing}
%%   \begin{lstlisting}[mathescape=true,showstringspaces=false]
    
%%   // assume date refers to 2022-12-01 13:30:00    

%%   // $P_3$:
%%   int hour1 = date.getHours();
%%   int hour2 = date.getHours();    

%%   // $P_4$:
%%   final Calendar calendar = Calendar.getInstance();
%%   calendar.setTime(date);
%%   int hour1 = calendar.get(Calendar.HOUR_OF_DAY);
%%   int hour2 = hour1;
%% \end{lstlisting}

%% For the code above, we will refer to the first two lines denoting the
%% original deprecated code as \var{P_3}, and the rest, denoting the
%% refactored code, as \var{P_4}.

We note that, the original code above defines two variables \var{dim1} and
\var{dim2}, each assigned an object of type \texttt{Dimension} returned by calling
\var{container.preferredSize()}.
Conversely, in the refactored code, \var{dim1} and \var{dim2} are aliases,
i.e., they point to the same
object of type \texttt{Dimension}.

The original and the refactored code are equivalent according to Definition~\ref{def:prog-equiv}.
Let's next assume that the following code
is used after both the original and the refactored code, respectively.

\begin{lstlisting}[mathescape=true,showstringspaces=false]
  dim1.setSize(1,2);
  dim2.setSize(2,3);  
\end{lstlisting}

%Our intention is to execute both the original and the refactored code, respectively, plus the additional two lines.
If the original and the refactored code were indeed equivalent,
%with respect to input \var{d},
then we would expect \var{dim1} and \var{dim2} to have the same value at the end of both blocks of code. 
%executions, respectively.
However, this is not the case.
In the original code, \var{dim1} will have width 1 and height 2, whereas in the refactored code,
\var{dim1} will have width 2 and height 3. This is due to the fact that,
in the refactored code, \var{dim1} and \var{dim2} are aliases. %% Then, when the object referenced by
%% \var{hour1} was changed to refer to the date denoting ``30/10/2020'', this also affected \var{date2}.
Thus, when \var{dim2} has its size set to (2,3), this also affects \texttt{dim1}.
%\var{P_3} and \var{P_4} are not equivalent for input \var{d}.
\end{example}  

Intuitively, any aliases between live variables at the end of the original code should also
be present at the end of the refactored code. For this purpose, we use the notation
\var{aliasEquivClass(V)} which returns the set of all equivalence classes induced over the
set of variables \var{V} by the aliasing relation.  
Notably, the aliasing equivalence relation must also hold over the
static fields of the classes loaded by both programs.


\begin{example}
For \var{P_1} and \var{P_2}, there are no aliases. 
%% \[
%% \begin{aligned}[t]
%%   &\var{aliasEquivClass(liveVar(P_1)} {\cup} 
%%     \var{staticFields(loadedClasses(P_1))} {=} \emptyset\\
%%   &\var{aliasEquivClass(liveVar(P_2)} {\cup}
%%     \var{staticFields(loadedClasses(P_2))} {=} \emptyset 
%% \end{aligned}
%% \]
%
However, for \var{P_3} and \var{P_4} we have:
\[
\begin{aligned}[t]
  &\var{aliasEquivClass(liveVar(P_3)}~ \cup\\
  &\var{staticFields(loadedClasses(P_3))} = \emptyset\\
  &\var{aliasEquivClass(liveVar(P_4)} ~\cup\\
  &\var{staticFields(loadedClasses(P_4))} = \{\{dim1, dim2\}\} 
\end{aligned}
\]
In \var{P_4}, the aliasing relation induces one equivalence class, namely \var{\{dim1, dim2\}}.

\end{example}
  
Next, we complete Definition~\ref{def:prog-equiv} by capturing the aliasing aspect.

\begin{definition}[Addition to Definition~\ref{def:prog-equiv}]\label{def:prog-equiv-add}
   In addition to Definition~\ref{def:prog-equiv},
  two programs \var{P_1} and \var{P_2} are equivalent
  with respect to $\vec{i}$ iff:
\[
    \begin{aligned}
      & (4)~ \forall v_1,v_2 \in \mathit{liveVar}(P_1) \cup \mathit{staticFields}(loadedClasses(P_1)). \\
      & \qquad aliases(P_1, v_1, v_2) \Longleftrightarrow  aliases(P_2, v_1,v_2)      
    %% & (5)~\forall f_1,f_2 \in staticFields(loadedClasses(P_1)).\\
    %%   & \qquad aliases(P_1, f_1, f_2) \Rightarrow aliases(P_2, f_1,f_2)
    %%   throws an exception and   $equiv(exception(R_1), exception(R_1))$. 
  %% \item \var{R_1} doesn't throw an exception and $equiv(R_1,R_2)$ and
   %% $\forall f_i \in getStatic(class(R_1)) \wedge g_i \in getStatic(class(R_2)). equiv(f_i,g_i)$.
    \end{aligned}
    \]
   
  \end{definition}

where, given a program $P$, variables $v_1$ and $v_2$ are aliases, i.e., $aliases(P, v_1, v_2)$ holds
if and only if they are in the same equivalence class induced by the aliasing relation,\\
  i.e., $aliasEquivClass(\{v_1,v_2\}) = \{\{v_1,v_2\}\}$.

  We abuse the notation to use \var{staticFields} over a set of classes, rather than just one class.
  The objective is to return the union of all static fields defined in all the classes in the set of classes taken as argument.


%% At a high level, the new code is obtained by composing methods
%% that are accessible from that respective location. 
%% Consequently, we take inspiration from component-based program synthesis,
%% which weaves together components from a library
%% (typically methods from an API) in order to generate the desired program \cite{DBLP:conf/icse/JhaGST10,DBLP:conf/pldi/GulwaniJTV11,DBLP:conf/popl/FengM0DR17}.



%%---
%% Most commonly, the information used to guide the search is represented by input/output examples~\cite{DBLP:conf/pldi/FeserCD15}, type signatures~\cite{DBLP:conf/pldi/OseraZ15}, or full functional specifications of the expected code~\cite{DBLP:conf/ijcai/MannaW79}.
%% %
%% The vast search space is particularly a challenge in the current
%% setting given that, in the absence of any additional guidance, the
%% entire \texttt{java.lang} could be considered when synthesising the
%% refactoring.  In order to prune the search space, we start by
%% investigating the information available to our refactoring procedure.

  
%As mentioned in Section~\ref{sec:intro}, for both approaches, we want to use  
%the code hints provided in the Javadoc. In particular, we use these code hints to build a library of components that we can then use to generate the refactoring.


\section{Seeding of the component library} \label{sec:components-seeding}
%% We designed two versions of the library, differentiated by the seeding guidance:
%% \begin{itemize}
%% \item For the {\bf CodeHints-library} the seeding is guided
%%   by both Javadoc code hints and types.
%% \item For the {\bf Types-library} the seeding is only guided by types.
%% \end{itemize}  

In this section, we describe how we populate the instruction library (also referred to as the component library)
used by the symbolic approach starting from code hints. We'll refer to this library as the Codehints-library.
This step is critical as, in order for the symbolic engine to succeed, the library must be as small as possible
while containing all the instructions necessary
for constructing the solution.

For illustration, let's go back to the running example in Figure~\ref{ex:deprecated-method-other} (with the corresponding Javadoc comment in Figure~\ref{ex:code-hints}).
%
As mentioned in Section~\ref{sec:intro}, when building the corresponding component library for this example, 
%
%{\bf Challenge 1.}  Although it may seem as if
%method \texttt{get} is static allowing an immediate call,
%it is actually an instance method, requiring us to have an object of
%class \texttt{Calendar}. However, no such object is available in the
%original code meaning that it must be created by the refactored code.
%Consequently,
we must find the necessary
components that consume existing objects and create a \texttt{Calendar} object, so that we can call method \texttt{get} as suggested by the code hint.
Besides adding components that allow us to generate the required objects, we also need components for setting their
fields. In our example, we must call
\texttt{calendar.setTime(date)} to set the calendar's date
based on the existing \texttt{date} object.

Adding too many components to the library will
make the code generation task infeasible. In particular, we should
be able to differentiate between components that we can use
(we have or we are able to generate
all the necessary arguments and the current object for calling them), and those
that we can't because we can't obtain some of the arguments
and/or the current object. Adding the latter components to the library will
significantly slow down the code generation process by
adding infeasible programs to the search space.
We address these challenges by dynamically building the component library for each refactoring
such that it only contains components specific to that particular use case.


%{\bf Challenge 3.} Code hints are
%not always expressed as well-formed Java. In some sense, the task of
%parsing code hints lies in between parsing Java code and interpreting natural language.

%{\bf Challenge 4.} As previously mentioned, the generated refactoring needs to be {\em semantically equivalent to the original}, meaning that it must preserve its behaviour.
%Coming up with an equivalence check is very difficult, especially in the presence of side-effects, as we must take into consideration both the state of the stack and the heap (including the static fields).

%This process is described in detail in Section~\ref{sec:components-seeding}.
%The third challenge is further discussed under ``Phase 1: Initialise with code hints'' in Section~\ref{sec:components-seeding}.

%% In Section~\ref{sec:components-seeding}, we provide in-depth details about the
%% seeding of the component library and how to tackle the above challenges. 
%% At a high level, the process is guided by types and code hints.

%% \section{{\bf Seeding of the component library}}\label{sec:components-seeding}
%% As mentioned in Section~\ref{sec:overview}, a critical aspect of our technique is the construction
%% of the component library. 
%% If the component library is too small, meaning that it doesn't contain all
%% the required components, then synthesis will fail to find a refactoring.
%% Similarly, if it contains too many components, then synthesis quickly becomes infeasible
%% due to the large search space.



%% \subsection{CodeHints-library} \label{sec:core-library}

%In our running example in Figure~\ref{ex:deprecated-method-other}, we can view \texttt{date} of type \texttt{Date} as the input to the code to be refactored, and the integer \texttt{hour} as the output. Generally speaking, the refactoring technique will weave together components from the component library that consume the inputs and generate the required outputs.

Throughout the seeding process, we keep track of the following sets:
$\mathit{consumable\_ objs}$ (inputs to the code to be refactored, which need to be consumed by the refactoring),  
$\mathit{available\_types}$ (types for which we either have consumable objects or the corresponding generators to create them) and 
$\mathit{target\_types}$ (types for which we must be able to generate objects). To start with, the $\mathit{target\_types}$ set
contains the types of the original code's outputs. %, as well as the types of the code hints' arguments (required in order to make use of the code hints).
%(objects directly generated by the code hints or objects created by the generators added to make the code hints realisable)

For the running example, we start with:
%considering the hints are given in Figure~\ref{ex:code-hints} after the
%\texttt{@code} tag., at the beginning of the seeding algorithm, we have:
%For the running example, after initialising the component library with the code hints, we have:
\[
\begin{aligned}
  \mathit{consumable\_objs} &=  \{\texttt{date}\}\\  
  \mathit{available\_types} &=  \{\texttt{Date}\}\\
  \mathit{target\_types} &= \{\texttt{int}\}
\end{aligned}
\]

%% the library seeding algorithm must add components that are able to consume the
%% inputs to the deprecated code and produce the objects required to make use
%% of the Javadoc hints.
%

%From a type-guided perspective,
The objective of the seeding algorithm is to
add components to the library that make use of the $\mathit{available\_types}$ to generate objects
of $\mathit{target\_types}$. At the same time, we want to consume the objects from the
$\mathit{consumable\_objs}$ set, i.e., the inputs of the original code.

The seeding algorithm for the CodeHints-library is provided in Figure~\ref{alg:seeding-core} and consists of three phases, which we discuss next.

\paragraph{{\bf Phase 1: Initialise with code hints}}
During the first phase, the initialisation, we add all the constants and instructions from the code hints to the library.
%
%% Intuitively, the Javadoc code hints should help with that, so we start by adding the
%% suggested instructions to the component library.
For our running example, the hints in Figure~\ref{ex:code-hints} %% after the
%% \texttt{@code} tag.
%% These hints
instruct us to add method \texttt{``int get(int field)''} from class
\texttt{Calendar} and constant \texttt{``Calendar.HOUR\_OF\_DAY''} to our library.
%For the core library, we actually fix the \texttt{``Calendar.HOUR\_OF\_DAY''} constant
%in place, thus adding \texttt{``int get(Calendar.HOUR\_OF\_DAY)''}.
As a side note, finding the right constants is a well known
challenge for program synthesis~\cite{DBLP:conf/cav/AbateDKKP18}, and thus the subsequent
synthesis process will always attempt to use the constants provided in the code hints before generating new ones.

Intuitively, we need to make sure that the Javadoc suggestions
are realisable as captured by Definition~\ref{def:realizable}, where 
%We formally refer to such a component as {\em realisable} as captured by Definition~\ref{def:realizable}, where
%$source\_types$ to refer to types for which we have the means to generate objects, and
$required\_types(method)$ refers to the types of the objects required to call $method$ (i.e., the types corresponding to its arguments and current object).
%for $instr$ to be realisable.
In our running example, $required\_types(\texttt{get}) = \{\texttt{int, Calendar}\}$
given that, in order to call \texttt{get}, we must provide an argument of type \texttt{int} and a current object of type \texttt{Calendar}.
Method \texttt{get} is not realisable as the library doesn't contain any generator for \texttt{Calendar}.
Consequently, \texttt{Calendar} is added to $target\_types$, resulting in:
$$target\_types = \{\texttt{int, Calendar}\}$$

\begin{definition}[Realisable method]\label{def:realizable}
Method \var{i} is {\em realisable} iff $\forall t \in required\_types(i). t \in available\_types \vee t~is~a~primitive~type$.
\end{definition}


%\subsection{Parsing the @code hints} \label{sec:parse}

%As explained in Section~\ref{sec:components-seeding}, when seeding the component library, our algorithm must
%interpret Javadoc hints. 
One challenge in this phase is interpreting the
\texttt{@code} blocks inside \texttt{@deprecated} sections, which 
we attempt to parse as Java expressions. In particular, we parse each hint as if it were invoked in the context of
the method to refactor, such that imports or the implicit \texttt{this} argument
are considered during parsing.

%\todo{needs some rewriting?}
%% The issue is that code hints are not always expressed as well-formed Java.
%% In some sense, the task of parsing code hints lies in between parsing Java code and interpreting natural language. 
%
In order to address the challenge 3 that code hints are not always expressed as well-formed Java,
we customised the
GitHub Java parser
to accept undeclared identifiers and type names
as arguments. For instance, the Javadoc hint for deprecating \texttt{boolean inside(int X, int Y)}
%suggests using 
in Figure~\ref{ex:javadoc-hint} suggests using
\texttt{contains(int, int)}, which would normally cause a parsing error as the \texttt{int} type
appears in the place of argument names. In our setting, we accept this hint as valid.
There are still situations where our parser is too strict and fails to accept
some of the code hints. Additionally, there are scenarios where, while the Javadoc
does contain a useful code hint, it is not tagged accordingly with
the \texttt{@code} tag. %% These situations are reflected in the experimental results
%% in Section~\ref{sec:experimental-results}
%% as some of the benchmarks for which we failed to identify a code hint.
%We will improve our technique for collecting code hints as future work.

%% These benchmarks
%% are contained in Table~\ref{tab:configuration-results}, but omitted in Table~\ref{tab:configuration-results-ch} (some of the missing benchmarks from Table~\ref{tab:configuration-results-ch} do genuinely miss a code hint). 
 
\begin{figure}
\begin{lstlisting}[mathescape=true,showstringspaces=false]
/**
* Checks whether or not this {@code Rectangle}
* contains the point at the specified location
* {@code (X,Y)}.
*
* @param  X the specified X coordinate
* @param  Y the specified Y coordinate
* @return    {@code true} if the point
*            {@code (X,Y)} is inside this
*            {@code Rectangle};
*            {@code false} otherwise.
* @deprecated As of JDK version 1.1,
* replaced by {@code contains(int, int)}.
*/
@Deprecated
public boolean inside(int X, int Y) {
  // ...
}
\end{lstlisting}
\caption{Javadoc hint example.}
\label{ex:javadoc-hint}
\end{figure}




\paragraph{{\bf Phase 2: Add generators for $target\_types$}}

By generators we refer to constructors and any other methods returning objects of that particular type.
In the algorithm in Figure~\ref{alg:seeding-core},
once we added a generator for a new type, we must add that type to $available\_types$.
Additionally, if the new generator consumes any objects from $consumable\_objs$,
we must remove them from the set.

For our running example, we must seed our component library with generators for \texttt{Calendar}.
%
We first scan all the public constructors of class \texttt{Calendar}
and all the public methods from class \texttt{Calendar} that return an object of type \texttt{Calendar}. We find the following four options:

\begin{enumerate}
  \item \texttt{static Calendar	getInstance()} -- creates a \texttt{Calendar} object using the default time zone and locale.
  \item \texttt{static Calendar getInstance(Locale aLocale)} -- creates a \texttt{Calendar} object using the default time zone and specified locale.
  \item \texttt{static Calendar	getInstance(TimeZone zone)} -- creates a \texttt{Calendar} object using the specified time zone and default locale.
  \item \texttt{static Calendar	getInstance(TimeZone zone, Locale aLocale)} -- creates a \texttt{Calendar} object with the specified time zone and locale.
\end{enumerate}

Out of the four methods,
only the first one is realisable. Conversely, in order to call the second method, we would need to generate an object of type \texttt{Locale},
for which we don't have a corresponding component in the library.
The last two methods are in a similar situation as the second. Thus, we only add the first method to the CodeHints-library.

\paragraph{{\bf Phase 3: Add transformers for $target\_types$}}
The third and last phase adds  transformers for the target types.
By transformers we refer to methods that modify the value of an instance variable.
Given our objective to generate objects for the $target\_types$ while consuming objects from $consumable\_objs$,
we prioritise transformers that consume such objects.
For instance, for our running example there are 16 public transformers for the \texttt{Calendar} class.
However, instead of adding all of them to the CodeHints-library, we prioritise those that
consume the \texttt{date} object. There is only one such transformer \texttt{void setTime(Date date)}.
Once we added any new transformers to the library, we remove the consumed objects from \texttt{consumable\_obj}.

Notably, all the components that
we add to the library must be accessible from the current location.

\begin{figure}
\removelatexerror% Nullify \@latex@error
\begin{algorithm}[ht]
\SetAlgoLined
\KwOut{CodeHints-library}
%<<<<<<< HEAD
// {\bf Phase 1: Initialise}\\
Add constants and instructions from the code hints to the library\;
\For{each unrealisable instruction $i$ in library}{
  $target\_types$ = $target\_types \cup required\_types(i)$\;
}
$\newline$
// {\bf Phase 2: Add generators for $target\_types$}\\
\For{each $t \in target\_types$ for which there is no generator}{
  Add realisable generators for $t$ to library\;
  $available\_types = available\_types \cup \{t\}$\;
  Remove consumed objs from $consumable\_objs$\;
}
$\newline$
// {\bf Phase 3: Add transformers for $target\_types$}\\
\While{$consumable\_obj\neq\emptyset$}{
Add realisable transformers to the library for types in $target\_types$ that consume objects from $consumable\_objs$\;
Remove consumed objs from $consumable\_objs$\;
}
\end{algorithm}
 \caption{Seeding algorithm for the CodeHints-library}
\label{alg:seeding-core}
\end{figure}

It may be the case that there is no realisable generator for a target type, or no transformers for the $target\_types$ that can consume all the
$consumable\_objs$. When we finish exploring all the available methods, we exit the corresponding loops in phases 2 and 3.
As a consequence, the synthesiser may fail to generate a valid refactoring from the CodeHints-library.
%% For such a situation, we extend the component library as explained in Section~\ref{sec:extend-library}.
A possible future direction is allowing unrealisable generators and transformers to be added to the library and iterating phases 2 and 3 a given number of times (potentially until reaching a fixed point).
%% iterate phase 2 and 3 until fixed point to 

\subsection{Types-library} \label{sec:extend-library}

For cases when code hints are not provided, in addition to the CodeHints-library, we also provide a component library that is populated based only on the type signature
of the deprecated method. Next, we describe how this Types-library is being built.

For the CodeHints-library, the Javadoc code hints guidance results in the code hints being
used to collect the $target\_types$ set during the initialisation phase.
If we don't have access to code hints and we are seeding solely
based on types, we start with the $target\_types$ set containing
%the class where the original code resides, as well as
the types %of the inputs and outputs
of the outputs of the original code. %(essentially using the signature of the original code).
Phases 2 and 3 of the seeding algorithm are the same as those in Figure~\ref{alg:seeding-core},
where we attempt to add generators and transformers for the $target\_types$ to the library.

%For our running example, we would have $target\_types = \{\}$

%Given that both the CodeHints-library and the Types-library consider the output of the
%original code as a target type, we expect some overlap between the two libraries.
Compared to the CodeHints-library, the seeding process for the Types-library may end up missing critical types provided by the Javadoc code hints. For instance, in our running example, the \texttt{Calendar} class is only mentioned by the
code hints and would not be included in the seeding of the Types-library.
One possibility for adding
\texttt{Calendar} to the target types without considering
the Javadoc hints, would be to add all the classes from the
\texttt{java.util} package, which contains \texttt{Date}. However, doing so would result
in a very large component library, most likely outside the capabilities of existing
synthesis techniques.

  \section{Design and implementation choices} \label{sec:implementation}

  %In the previous sections we presented our technique from a formal point of view.
  %Conversely, in this section we discuss details related to our implementation choices.  
  
%  In this section we discuss Java specific challenges 
 % and our solution, as well as some other design and implementation choices we made.

\subsection{Abstract classes and interfaces}\label{sec:abstract}

The programs that need to be checked for equivalence may refer to
abstract classes and interfaces. These are by default instantiated
using the mocking framework Mockito.

Alternatively, we also curate a
list of explicit constructors
of subclasses to be used in favour of Mockito mocks for certain types, and users
have the option to extend this list with a custom configuration.
%This is particularly useful when the default behaviour of mocked objects under-approximates
%the behaviour of the explicit constructors of subclasses.
%In such situations, the use of mocked objects can lead to
%spurious refactorings (this will be further discussed in Section~\ref{}).
%
%This problem generally appears whenever methods are overriden in subclasses.
%An example of this situation is the refactoring of the
%method \texttt{layout} in the abstract class \texttt{java.awt.Component}. The
%method is not abstract, but has an empty body and is meant to be overriden by
%subclasses. Using this empty body as a specification will only guarantee a sound
%refactoring for subclasses that do not override it. Our approach is still
%guaranteed to find the preferred refactoring of using \texttt{doLayout} instead,
%since we take the Javadoc hint into account. However, a more complete approach
%in general would be to
%fuzz against
%all available subclasses of
%\texttt{Component} on the classpath.
This is particularly useful when attempting to avoid constructors that are not amenable to fuzzing, such as
collection constructors that take an integer capacity argument, where an
unlucky fuzzed input might lead to an out of memory error.
%For instance, for \lstinline{ArrayList}, we use the constructors that don't mention an initial capacity.


%We e.g. hard-coded that we should not generate an ArrayList using the constructor that takes a single int, since that is the capacity. But ArrayList has other constructors that we use.


%\subsection{Using fuzz testing for synthesis} \label{sec:fuzzing}

%In order to generate programs, we make use of JQF~\cite{DBLP:conf/issta/PadhyeLS19}, a  %coverage-guided property-based
%fuzz testing platform for Java.
%
%As explained in Section~\ref{sec:encoding}, we use JQF for two tasks. Firstly, in the synthesis phase, 
%JQF is generating the candidate refactoring.
%Secondly, in the verification phase, JQF generates
%counterexample inputs that can differentiate between the original and refactored codes.
%The first task in particular is a difficult setting for fuzzing because programs are
%highly structured, whereas fuzzers often work with unstructured inputs
%that can be generated via byte-level mutations such as bit flips.

%JQF is designed to handle structured inputs, where inputs of type \texttt{T}
%are generated by a backing \texttt{Generator<T>}. JQF provides a library of
%generators for basic types such as primitive values. We implement custom
%JQF generators for the synthesis and verification phase of the CEGIS loop. %% In

%% We use JQF in Zest~\cite{DBLP:conf/issta/PadhyeLSPT19}
%% guidance mode, which incorporates feedback from the test program in the form of
%% semantic validity of test inputs and the code coverage achieved during test
%% execution.
\subsection{Observable state}\label{sec:observable}

Our equivalence check uses the Java Reflection API to observe the side effects
of programs, such as assigning new values to fields. As a consequence, we cannot
observe effects that are not visible through this API, such as I/O operations or
varibles maintained in native code only. I/O operations could be recorded using
additional bytecode instrumentation in future work, but for the scope of our current
implementation, if programs only differ in such for our current engine invisible
side effects, our engine is prone to producing a refactoring that it's not fully equivalent to the original.



  
\subsection{Instrumentation and isolation}

We use reflection to invoke the original method to refactor with fuzzed inputs,
as well as our synthesised refactoring candidates. This allows us to dynamically
invoke new candidates without the need for compilation.
%The state of a Java
%program is modelled by the current program stack as well as its heap consisting
%of instance variables and static field values. Static field values are stored
%with their respective classes, which in turn are loaded by class loaders.
Regarding static fields, Java
allows to load the same class in different class loaders, which creates
independent copies of its static fields.

%In the verification phase,
In order to fully isolate the program state during the execution of the original and refactored code from each other, we load all involved classes in separate
class loaders. These class loaders are disposed immediately after the current set
of fuzzed inputs are executed, and new class loaders are created for the next
inputs. %(which will be used in the next iteration of the CEGIS loop).
Classes that do not maintain a static state are loaded in a shared
parent class loader to improve performance.

The OpenJDK Java virtual machine implementation does not allow us to load
classes contained in the \texttt{java.lang} package in such an isolated class
loader. For such classes we cannot apply refactorings that depend on static
fields, as they cannot be reset and will retain the state of previous
executions. Instead of observing the effect of one isolated refactoring
candidate, we would observe their accumulated effect, which would be incorrect.
%leading to both
%incompleteness and unsoundness depending on the context.
For the scope of our
experiments this did not pose a problem, since most of the affected classes in
the \texttt{java.lang} package do not maintain a static state, and the ones who do
were irrelevant to our refactorings.

%Everything that is part of our comparison is in the isolated class loader.


%% For instance user defined types will usually be located in the isolated
%% class loader, enabling the comparison.
%% However, what do we do about wrapper object aliasing?
%% For anything outside the class loader, we use reference equality.
%% PAK: I think this should be in a separate section. Being loaded in the isolated
%% class loader or not doesn't prohibit comparison per se.

%% We also never use reference equality,
%% but rather rely on native `equals' implementations if they satisfy certain criteria.
%% This should probably also be explained in its own subsection.

%% One potential issue when checking equivalence of two blocks of code
%% is the treatment of loaded classes. In particular, one program might
%% load more classes than the other, but still have the exact same effect
%% as the other one. This can happen for instance if one program calls a
%% helper method from a class that needs to be loaded, whereas the other
%% program inlines the same method, thus avoiding loading the class.  Our
%% assumption is that loading and initialization don't perform any
%% changes outside of the observable state already checked by the
%% existing equivalence check.  While this assumption is generally safe
%% to make, there might be some cases when this is not the case, e.g. the
%% class initializer might print something on the screen. For now, we
%% don't handle these cases.
%% \todo{So we don't need to load the exact same classes? What if static fields are changed? -
%% PAK: We check that all static fields in classes that were loaded by either program are equivalent.
%% That means that if one program didn't load a class, we load it for it, and check that the
%% initial state of that class is the same as the other program's. 



%% The goal of our instrumentation is to execute a program candidate in
%% the same context as the original, deprecated method. The original
%% method can affect the program state by virtue of its return value,
%% field assignments in the case of instance methods, and static field
%% assignments. In order to account for these effects, our
%% instrumentation passes a reference to the original method's object
%% instance, if present, as an argument to the program candidate.
%% An example of this is presented in
%% Fig.~\ref{ex:side-effects-instrumentation}.

%% \begin{figure}
%% \begin{lstlisting}[mathescape=true,showstringspaces=false]
%% // Redundant snippet:
%% Date today;
%% Date tomorrow;
  
%% void init(int year, int month, int day) {
%%   // Original:
%%   this.setDates(year, month, day);

%%   // Instrumented:
%%   Program.of(0xabcd).execute(year, month, day, this);
%% }
  
%% @Deprecated
%% private void setDates(int year, int month, int day) {
%%   final Calendar calendar = Calendar.getInstance();
%%   calendar.set(year, month, day);
%%   this.today = calendar.getTime();
%%   calendar.add(Calendar.DAY_OF_MONTH, 1);
%%   this.tomorrow = calendar.getTime();
%% }
%% \end{lstlisting}
%% \caption{Deprecated side-effects instrumentation.}
%% \label{ex:side-effects-instrumentation}
%% \end{figure}

%% Fig.~\ref{ex:date-instrumentation} illustrates an example where no
%% such relevant instance object exists and thus only the original
%% arguments are passed as arguments into the program candidate.

%% \begin{figure}
%% \begin{lstlisting}[mathescape=true,showstringspaces=false]
%% // Redundant snippet:
%% static void main(String[] args) {
%%   // Original:
%%   Date date = new Date(120, 12, 30);

%%   // Instrumented:
%%   Date date = Program.of(0xabcd).execute(120, 12, 30);
%% }
%% \end{lstlisting}
%% \caption{Deprecated `Date' instrumentation.}
%% \label{ex:date-instrumentation}
%% \end{figure}

  
 





\subsection{Checking aliasing}

While in Section~\ref{sec:equiv}, we discussed aliasing preservation in terms of
preserving the equivalence classes induced by the aliasing relation,
in our implementation we enforce a simpler but stricter  than necessary check.
In particular, all the objects
%(memory locations)
that are being referenced during the code's execution
are assigned increasing symbolic identifiers.
Then, we enforce aliasing preservation (condition (4) in Definition~\ref{def:prog-equiv-add}),
by checking that, for all variables defined by both the original and the refactored code,
the objects they reference have the same symbolic id.

For illustration, in the original code in Example~\ref{ex:aliasing}, the object of type \texttt{Dimension} referenced by variable \var{dim1}
is assigned symbolic value \var{s_1}, whereas the object referenced by \var{dim2} is assigned symbolic value \var{s_2}.
In a similar manner, the object referenced by both \var{dim1} and \var{dim2} in the refactored code is assigned value \var{s_1}.
Then, the aliasing check fails as, in the original code, the object referenced by \var{dim2} has symbolic value \var{s_2},
whereas, in the refactored code, the object referenced by \var{dim2} has symbolic value \var{s_1}.

While this is a  stronger than needed requirement, it was sufficient for our experiments.
In particular, we didn't find any benchmark where a sound refactoring
was rejected due to it.
If it did prove too restrictive in practice, we could
relax it by explicitly tracking the equivalence classes induced by the aliasing
relation.

%\todo{Refer to the new example obtained from Pascal.}

%% heap equivalence, we enforce a strict notion of aliasing
%% equivalence.
%% While exploring the heaps and comparing
%% them for value equivalence, every pointer encountered is assigned
%% a strictly increasing symbolic id $sid(v)$.
%% Two heaps $H_A$, $H_B$ are only considered equivalent if
%% $\mathop \forall \limits_{i=1}^{n} sid(v^A_i)=sid(v^B_i)$. This is a strong
%% under-approximation and may be subject to improvement in future work, but was
%% sufficient for the scope of our experiments.

%% Then, we can check whether the set of aliases is preserved in the
%% refactored program. For instance, if \texttt{obj1} and \texttt{obj2}
%% are aliases in the original program, then they must be aliases also
%% in the second program.
%\todo{check exactly what this is used for.}






\subsection{Sources of unsoundness and incompleteness}\label{sec:incompleteness}
As aforementioned, due to the use of fuzz
testing in the synthesis phase, our technique may fail to find a
refactoring even when one exists (i.e., it is {\em incomplete}),
whereas using fuzz testing in the verification phase may result in
finding a refactoring that is not equivalent to the original on all
inputs (i.e., our technique is {\em unsond}).

While we could attempt to swap fuzz testing with a formal verification technique,
program verification is undecidable in general.
Even for restricted, decidable fragments, such
techniques tend to be expensive as they generally require some form of
symbolic execution and calls to off-the-shelf solvers.  Moreover,
programming languages evolve faster than formal verification techniques and
therefore new language features might not be supported by the front ends of
such techniques. An attempt at using formal methods for proving
observational equivalence during the refactoring process was made in~\cite{DBLP:journals/corr/abs-1712-07388} and exposed the aforementioned limitations.
%
More sources of incompleteness and unsoundness are discussed in the experimental evaluation in Section~\ref{sec:experimental-results},
under the ``Missed refactorings'' and ``Unsound refactorings'' paragraphs, respectively.


%% {\bf More sources of unsoundness.} Besides the use of fuzz testing, we identified three other potential sources of unsoundness,
%% which we discuss next.

%% Firstly, unsoundness can be caused by I/O,
%% which is not part of the JVM state. We currently don't check
%% that the original and refactored code perform the same I/O. While we exclude methods that only perform I/O, we attempt to refactor those that perform I/O alongside other core logic.

%% Secondly, our algorithm does currently not explore the entire classpath for
%% potential overrides of the method to refactor. This may lead to unsound results
%% when different subclasses of the original class are used polymorphically in the
%% refactored context.

%% Finally, as described in Sec.~\ref{sec:equiv}, we rely in some cases on
%% \texttt{equals} implementations for equivalence comparison.  While we
%% strictly filter which \texttt{equals} implementations we consider, if any of
%% them do not accurately identify an equivalent program state, our refactoring
%% may be unsound.

%\todo{Other sources of incompleteness and unsoundness?}

%% such a technique would require symbolic execution, which is no-
%% toriously difficult for real-world programs due to features such as
%% loops, library functions, complex data structures. The most success-
%% ful such verification techniques are either bounded (i.e. check that
%% an erroneous state cannot be reached within a bounded number of
%% loop unrollings) or require user input in the form of loop invariants.
%% While the former class of techniques provides limited soundness
%% guarantees, the latter requires intricate help from the user. In fact,
%% we are not aware of any sound and automatic symbolic verification
%% method that can handle the kind of code we are interested in.

%% Discuss tradeoff: symbolic execution is expensive; programming
%% languages evolve faster than verification techniques and therefore
%% many new features are not supported by their front ends (and/or
%% backends depending on how complex the new feature is); even
%% after translating the code into a logical formula, calls to solvers are
%% expensive.


%% \subsection{Equivalence reduction}
%% JQF never generates the same exact program twice. However it may generate many
%% syntactically different, but semantically equivalent ones.
%% Program synthesis techniques make use of equivalence reduction in order to
%% reduce the number of equivalent programs that get explored.
%% For example, \cite{DBLP:conf/cav/AlbarghouthiGK13} 
%% prune the search space using observational equivalence with respect to a set of input/output examples, i.e., two programs are considered to be in the same equivalence class if, for all given inputs in the set of input/output examples, they produce the same outputs. Alternatively, \cite{DBLP:conf/vmcai/SmithA19} generate only programs in a specific normal form, where term rewriting is used to transform a program into its normal form. In~\cite{DBLP:journals/corr/KoukoutosKK16}, Koukoutos et al. make use of attribute grammars to only produce certain types of expressions in their normal form, thus skipping other expressions
%% that are syntactically different, yet semantically equivalent.
%% We plan on investigating some of these directions as part of future work.


\section{Experimental evaluation}\label{sec:experimental-results}

We implemented the refactoring generation techniques in two tools called \tool and \llm, corresponding to the symbolic and the neural approach, respectively (available together with all the experiments at \url{https://anonymous.4open.science/r/resynth-artifact-43D6}).

\subsection{Experimental setup}

Our benchmark suite contains 236 out of 392 deprecated methods in the Oracle
JDK 15 Deprecated API documentation~\cite{OracleJdk15DeprecatedAPI}.
We included all the deprecated methods except those that were deprecated without a replacement (e.g. \lstinline[breaklines=true]{java.rmi.registry.RegistryHandler.registryImpl(int)}), 
%methods that do not have a semantically equivalent refactoring
%(see Section~\ref{sec:semantically-different-refactorings}),
methods inaccessible by non-JCL classes (i.e., all the finalize methods, which are called by the garbage collector, not user code, meaning that we can't modify their calls), 
methods that only perform I/O (as our equivalence check doesn't include I/O side effects),
and native methods.
%methods with I/O side-effects and native methods, which are currently
%unsupported by our implementation (see Sec.~\ref{sec:incompleteness}).

For \llm, we use Claude 2.1 and Claude 3 (both accessed via Amazon Bedrock), denoted as \llma and \llmb, respectively. We set the model temperature to $0.2$.
%
For all engines, we bound the search by at most 500 inputs and 5 minutes per verification phase, and at
most 2 minutes per synthesis phase. 
%For each refactoring found by GPT-4 Turbo, we use fuzzing to check program equivalence (according to the
%description in Section~\ref{sec:equiv}), and bound the verification time to 5 minutes.

Experiments were performed on an Ubuntu
22.04 x64 operating system running in a laptop with 16\,GB RAM and
11th Gen Intel Core i7-11850H at 2.50\,GHz.  The JVM used was Oracle JDK 15.02.

%\subsection{Engines setup}

%\paragraph{Symbolic engine}
%In our experiments, we run \tool and \llm as follows.
For \tool, if code hints are present, we start with the CodeHints-library and fall back to the Types-library if the synthesis phase (as described in Section~\ref{sec:synthesis}) times out for the CodeHints-library; if no code hints exist then we use the Types-library.

%\paragraph{Neural engine}





%due to JVM crashes or timeouts of the equivalence check,
%while unsound refactorings are those for which the equivalence check returned false.




%three versions of \tool: (1) \tool with the Types-library; (2) \tool with the CodeHints-library;
%(3) a combined best configuration for \tool, where we start with the CodeHints-library and fall back to the Types-library if the synthesis phase (as described in Section~\ref{sec:encoding}) times out.
%\subsection{Research questions}
%We are interested in answering the following research questions:
%\begin{itemize}
%\item (RQ1) How do \tool, \llma and \llmb compare to each other on all the benchmarks?

%\item (RQ2) How do \tool, \llma and \llmb compare to each other on the benchmarks that provide code hints?  

%\item (RQ2) How does \tool compare against GPT-4 Turbo for the current refactoring task?

%\item (RQ3) Do code hints help the generation of refactorings?
%\end{itemize}
  
%and two configurations for GPT-4 Turbo (one where the prompt contains the code hints, and one where it doesn't).
%The configurations differ in terms of the component library
%they consider as well as the JQF fuzzing guidance applied.
%% We use JQF in Zest~\cite{DBLP:conf/issta/PadhyeLSPT19}
%% guidance mode, which incorporates feedback from the test program in the form of
%% semantic validity of test inputs and the code coverage achieved during test
%% execution.
%% %
%% %
%% The coverage feedback used by Zest means that JQF uses lightweight program
%% instrumentation to trace the code coverage reached by each generated
%% input that is fed to the program under test.  
%% Instead of generating inputs from scratch, coverage-guided fuzzers
%% evolve a set of saved inputs. If a new input leads to an increase
%% in code coverage, it is saved for subsequent evolution.
%% Configurations marked as {\bf Rand} use JQF's random fuzzing
%% guidance, whereas {\bf Zest} uses JQF's coverage-guided Zest guidance.
%
%For each configuration, %we evaluate how many sound as well as unsound
%refactorings are produced, how many refactoring opportunities were missed
%and the average time to successfully produce a refactoring.



\subsection{Results}\label{sec:experimental-results}

%the configuration marked as {\bf Types}
%is based on the Types-library, whereas the one marked as ({\bf CodeHints}) uses the CodeHints-library.

\begin{table}[h]
\begin{tabular} {|l|r|r|r|r|r|}
\hline
Config         & \checkmark & \xmark & \lightning & \% & $\diameter$ runtime (s) \\ \hline
%No code hints         &  &  &  &  & \\ 
%\tool w/ types      &         79 &    116 &         41 & 33 & 216.1 \\
%\tool w/ code hints &        103 &    131 &          2 & 44 & 218.9 \\
\tool       &        7 &    82 &         16 & 6 & 213.7 \\  
\llma                     &        12 &     91 &         2  & 11 & 205.23 \\
\llmb                     &        10 &     93 &         2  & 9 & 204.25 \\ 
Best Virtual Engine    &        14  &         79 &      12 &  13 & 198.12 \\
\hline\hline
\tool  (CH)     &        104 &     19 &   8         & 79 & 217.7 \\
\llma  (CH)  &             92 &     39  &  0         & 70 & 208.09 \\
\llmb   (CH) &             95 &     35  &  1         & 73 & 205.27 \\
Best Virtual Engine (CH) &           109 &     14  &  8         & 83 & 211.89 \\
\hline\hline
\end{tabular} 
\caption{Experimental results for all benchmarks.}
\label{tab:configuration-results}   
\end{table}

Table~\ref{tab:configuration-results} provides an overview of the number of
sound refactorings (\checkmark), missed refactorings (\xmark), unsound
refactorings (\lightning), the percentage of sound refactorings (\%)
produced per configuration, as well as the average runtime per refactoring.
For the symbolic engine, missed refactorings are those that failed our equivalence check, whereas for the neural one,
they may denote either that the generated refactoring didn't compile or that it failed the equivalence check.
Unsound refactorings are
those that passed the equivalence check, but were manually detected by us as not being equivalent to the original code.
We describe the reasons why this can happen later in this section.
The average runtime includes both the time taken to generate a refactoring and to verify it.
%the equivalence check
%(without the equivalence check, the average time the LLM takes to answer a query is 9.99\,s).

The first four rows provide the results for benchmarks without code hints, whereas the last four (marcked with ``(CH)'') show the results for those
benchmarks where code hints were present. The rows denoted by ``Best Virtual Engine'' count all benchmarks (with and without code hints, respectively)
solved by at least one engine.

The results support our hypothesis that code hints are very valuable for automating the refactoring process.
For all engines, benchmarks with code hints have a considerably higher success rate than those without code hints, with the best virtual
engine solving 83\% of the benchmarks with code hints. In the absence of code hints, all the engines are struggling, solving only a few benchmarks.

To understand the discrepancy between the number of missed/unsound refactorings with the without code hints,
we next investigate the main reasons for such refactorings.


%In order to make sure that the benchmarks with code hints are just not 


%The results in Table~\ref{tab:configuration-results} support our assumption that this is a difficult refactoring as both engines only
%manage to solve a little under half the benchmarks. There is substantial overlap between the set of benchmarks solved by the different engines, such that,
%when considering the best virtual refactoring engine (denoted by the ``Best Virtual Engine'' row), which counts all benchmarks solved by at least one engine,
%we obtain an improvement of only 5\% over \tool.
%
%
%None of the engines manages to solve even half the problems.
%Some of the refactorings were valid, but rejected by our verification engine due to ...



\paragraph{Missed refactorings}
%% Out of the refactorings missed both with and without code hints, 
%% 2 were missed because the method return
%
%Another reason for missed refactorings is JVM crashes. We use reflection to produce counterexamples,
%and some may violate internal invariants.
%If executed on methods which eventually call native code, this can lead to the entire JVM
%crashing rather than e.g. throwing an exception. %% Currently, this terminates our synthesis
%% loop immediately, but in future versions we plan on treating our test VM crashing for a given input
%% as a regular type of outcome in the heap comparison.
%
%Additionally, since our neural engine shares the same verification approach with our symbolic engine, many missed/unsound refactorings
%are due to the same reasons (i.e. JVM crashes).
%
For the symbolic engine, the majority of the missed refactorings were due to fuzzing timeouts, which are likely 
caused by the component libraries missing some
instructions needed in the synthesis phase.
As explained in Section~\ref{sec:components-seeding}, one option 
for increasing the size of our libraries 
is allowing unrealisable generators and transformers to
be added.
%and iterating phases 2 and 3 of library seeding
%a given number of times (potentially until fixed point).

For the neural approach, we could only observe that the LLMs were unable to generate the refactoring from the provided prompt.

For both engines, when refactoring methods that eventually run native code, we encountered crashes of the verifier, which, in order to be conservative, we counted as overall failures.
% which we manually inspected.
The reason for this is the fact that we use reflection to produce counterexamples,
and some may violate internal invariants.
% we may create some that violate internal invariants
If they execute methods that eventually call native code, this can lead to the entire JVM
crashing rather than e.g. throwing an exception. %% Currently, this terminates our synthesis
%% loop immediately, but in future versions we plan on treating our test VM crashing for a given input
%% as a regular type of outcome in the heap comparison.


\paragraph{Unsound refactorings}
%
% Q: We do not consider I/O for synthesis, but consider it for verification?
%% Firstly, unsoundness can be caused by I/O,
%% which is not part of the JVM state. We currently don't check
%% that the original and refactored code perform the same I/O. While we exclude methods that only perform I/O, we attempt to refactor those that perform I/O alongside other core logic.
%
In several cases, refactorings that are not equivalent to the original code managed to pass our verifier.
While we expected to run into this problem because of the nature of fuzzing (the fuzzer may miss counterexamples that distinguish the behaviour of the original from the refactored code),
we also encountered other problems:
%One of the observations of our work is that it is actually very difficult to come up with a useful equivalence verifier 
%While we already gave up soundness by relying on fuzzing, we were confrunted with the following additional
%sources of unsoundness (which led to unsound refactorings passing the verification stage):

%As particularly obvious for the symbolic engine's results, 
%there are multiple scenarios where the equivalence check
%is insufficient to identify
%that a synthesised candidate is not equivalent to the original program. We
%identified the following sources of unsoundness in our benchmark set:

Unobservable state (discussed in Section~\ref{sec:observable}): I/O operations, static state in the boot class loader and
native methods are not observable by our equivalence predicate implementation,
since we rely on reflection to examine objects inside the Java runtime. As
a consequence, we cannot distinguish programs that only differ in these aspects.
%This is not a fundamental limitation, but instead just an implementation detail.

Abstract methods: 
There are classes in the JCL without any
existing implementation (e.g. \lstinline{javax.swing.InputVerifier}), and thus no concrete
method against which to verify the equivalence between the original and the refactored code. For those, the symbolic engine will
generate a no-op. We've been very conservative here, and counted this scenario as ``unsound'' because
it doesn't match human intent for the deprecated method.

Insufficient counterexamples: Our fuzzing-based equivalence check is inherently
incomplete, and for some benchmarks we do not explore sufficient counterexamples
%in sufficient number or quality
to identify unsound candidates. An example 
is \lstinline[breaklines=true]{javax.swing.JViewport#isBackingStoreEnabled}, where only a single input
out of the $2^{32}-1$ possible input values will trigger an alternate code path.
As a second exemplar, method \lstinline[breaklines=true]{java.rmi.server.RMIClassLoader#loadClass}
%is an example where our counterexample does not produce counterexamples of sufficient
%quality. The method
accepts a string as an input, but will throw when given any string
that is not a valid class name. %Since our fuzzer does not consider such context,
It is very unlikely that our fuzzer will randomly produce a string that matches a valid class
name.

\paragraph{Discussion}

The symbolic engine benefits significantly from code hints, as hints seed
the component library, making it less likely that needed instructions are
missing.  For the neural engine, we hypothesise that, by providing
additional context to the LLM, code hints are aiding the code generation
task.

Benchmarks with code hints also have fewer unsound results.  Our hypothesis
is that both engines are much more likely to generate the expected
refactoring before generating other candidate refactorings that may trick
our equivalence checker.

%% \todo{mention fuzzing}
%% Firstly, our algorithm does currently not explore the entire classpath for
%% potential overrides of the method to refactor. This may lead to unsound results
%% when different subclasses of the original class are used polymorphically in the
%% refactored context.
%
%% Secondly, as described in Sec.~\ref{sec:equiv}, we rely in some cases on
%% \texttt{equals} implementations for equivalence comparison.  While we
%% strictly filter which \texttt{equals} implementations we consider, if any of
%% them do not accurately identify an equivalent program state, our refactoring
%% may be unsound.
%
%% Furthermore, we also describe in Sec.~\ref{sec:equiv} the inherent unsoundness
%% using fuzzing to verify our solution. We observe four examples in our
%% benchmarks where we are very unlikely to explore enough counterexamples to
%% detect incorrect solutions. Examples include string input sequences, where only
%% valid class names exhibit desired behaviour, or where only one integer constant
%% out of all possible integer values triggers a critical code path.
%
%% We further observe three native methods where our heap abstraction does not
%% cover the native state, and thus our algorithm cannot observe that the produced
%% candidates are unsound.
%
%% Lastly, some solutions were unsound
%% because the method to refactor was a default method with an empty body, meant
%% to be overridden by subclasses. To find a sound refactoring in these cases, our
%% algorithm should expand its equivalence constraint to types that override the
%% method to refactor. 


%% In order to catch the unsound refactorings (for both the CodeHints-library and the
%% Types-library), we can either  
%% increase the fuzzing timeout for the verification phase (resulting in
%% more counterexamples), or attempt to produce higher quality
%% counterexamples. For the latter, one solution is to stop relying on libraries
%% like Objenesis for object construction and instead use public constructors.
%% For the former, the disadvantage of increasing the timeout is that it will
%% slow down the overall refactoring procedure.


%\subsection{Results when code hints are present}
%Many of the refactorings missed in Table~\ref{tab:configuration-results}
%happen for those benchmarks where no code hints were provided by the Javadoc.
%As we are also interested in the effect of code hints for the engines, 
%Table~\ref{tab:configuration-results-ch} shows the same evaluation as  
%Table~\ref{tab:configuration-results}, but excluding methods where
%no hint is provided in the Javadoc (or, in a few cases, we were unable to parse it as it was too far from well-formed Java code). 


%\begin{table}[h]
%\begin{tabular} {|l|r|r|r|r|r|}
%\hline
%Config         & \checkmark & \xmark & \lightning & \% & $\diameter$ runtime (s) \\ \hline
%%\tool w/ types      &         72 &     35 &  26         & 54 & 221.3 \\
%%\tool w/ code hints &        103 &     29 &   1         & 77 & 218.9 \\
%\tool       &        104 &     19 &   8         & 79 & 217.7 \\
%\llma   &             92 &     39  &  0         & 70 & 208.09 \\
%\llmb   &             95 &     35  &  1         & 73 & 205.27 \\
%Best Virtual Engine &           109 &     14  &  8         & 83 & 211.89 \\
%\hline\hline
%\end{tabular} 
%\caption{Results for benchmarks where code hints were present in the Javadoc.}
%\label{tab:configuration-results-ch}   
%\end{table}

%Results are better for these benchmarks, with less missed refactorings. The reasons for missed and unsound refactorings are the same as for the overall set of benchmarks in Section~\ref{sec:experimental-results}.

\subsection{Research questions}

{\bf (RQ1) Can the refactoring of deprecated Java APIs be automated?}
%
Yes, it can be easily automated if code hints are added to the JDK,
evidenced by the 83\% success rate for those benchmarks.  Without code
hints, Java's complexity makes these refactorings very hard for both for the
symbolic and neural engines.

\noindent
{\bf (RQ2) Do code hints help the generation of refactorings?}
%
The performance of \tool, \llma and \llmb improves considerably when code
hints are present.  All of them barely manage to solve any benchmark without
code hints.  When code hints are present, all engines solve at least 70\% of
benchmarks, with the best virtual engine solving 83\%.  For the symbolic
engine, they enable the effective seeding of the instruction library,
whereas for the neural engine, they provide additional context to the LLM.

\noindent
{\bf (RQ3) How do the symbolic and the neural approach compare against each other?}
%
There is only a small difference between the performance of the symbolic
engine and that of the neural engine.

%they both do well when code hints
%are present, and poorly when they are not.

%The symbolic engine does slightly better than the neural one when code hints are present.

When code hints are present, as shown in
Table~\ref{tab:configuration-results}, the symbolic approach does slightly
better than both \llma and \llmb.  This supports the intuition that, if
there is enough information about the solution to effectively prune the
solution space (in our case, when code hints are present),
%
%if we have a good idea about the instructions required such that we can build a component library that contains them all (without many spurious ones),
then the symbolic approach works well. % and should be chosen over the neural one.

One example of a benchmark that was solved by \tool but where both \llma and
\llmb failed to find a solution is the running example in
Figure~\ref{ex:deprecated-method-other}.  In all our runs, the LLMs failed
to generate \lstinline[breaklines=true]{calendar.setTime(date)}.

When code hints are not present, the neural engine does marginally better
than the symbolic approach.
%
%
%We observed that the neural engines are able to solve problems that access native code...?
%
%\autoref{tab:gpt-results} show that our neural approach performs better than the symbolic approach.
%
Compared to the symbolic engine, the neural one is able to provide correct
refactorings for deprecated concurrency primitives (e.g.,
\lstinline{weakCompareAndSet} in
\lstinline[breaklines=true]{java.util.concurrent.atomic.AtomicReference}). 
These methods call native methods, for which we cannot observe side-effects
(Section~\ref{sec:observable}).  Consequently, these are not captured by
the counterexamples returned by the fuzzer, and are thus not taken into
consideration by the symbolic engine during the synthesis phase.  For the
neural approach, the counterexamples are less critical, and the LLM can
obviously generate the correct code even though they are incomplete.

%
The neural engine is also able to handle benchmarks that require special
constants, such as
\lstinline[breaklines=true]{java.net.URLDecoder.decode(s)} described in the
introduction.

%{\bf (RQ2) What strategy should one use to achieve the best performance with the lowest budget?}
%If we were to design a general strategy that, given the symbolic and the neural approach, is able to achieve the best possible results  with the lowest budget, then such a technique should
%call the symbolic engine whenever code hints are present and the neural approach otherwise. Interestingly, this gives us exactly the best virtual engine
%in Table~\ref{tab:configuration-results}. (While, to have several comparison points, we used both \llma and \llmb, their results are very similar, so one
%should only run one of them.)


%We have also found that \llm is more aggressive in refactoring methods
%of which return value is dependent on external system states. For example, the deprecated method \lstinline{java.net.URLEncoder#encode}
%translates a string using a platform-specific encoding scheme. A correct refactoring requires explicitly providing
%this scheme, and \llm chose "UTF-8" which works on our platform.

%\todo{use example here}


%both engines fail to solve most of the benchmarks where code hints are not present. In particular, they only solve:
%7/105 for \tool, 12/105 for \llma, and 10/105 for \llmb  (i.e. the difference between the first column of Table~\ref{tab:configuration-results} and
%Table~\ref{tab:configuration-results-ch}). Thus, it seems that the neural approach is doing marginally better in solving
%benchmarks where there are no code hints.


%Using the CodeHints-library enables our technique to find more refactorings
%than with the Type-library.
%For the former, when code hints are present and used, the success rate goes from 52\% to 81\% (Table~\ref{tab:configuration-results-ch}).
%The higher quality of the instructions in the CodeHints-library compared to the
%Types-library leads to  a reduction in 
%the number of both unsound and missed refactorings.
%This also allows us to conclude that,
%while useful, types are not sufficient when synthesising code with side effects.

%For GPT-4 Turbo, the success rate improves from 33\% success rate to 44\% (last two rows of Table~\ref{tab:gpt-results}) when only considering benchmarks where code hints are present.

%{\bf (RQ2) How often does Chat GPT generate refactorings that preserve the semantics of the original code?}



%% \paragraph{Zest vs Rand guidance}
%% Our experiments did not yield significant performance
%% differences between using JQF's coverage-guided Zest guidance and its random
%% guidance. 
%% In general, Zest guidance is known to perform better at 
%% generating inputs fitting precise bug-revealing conditions than random guidance.
%% However, this better guidance comes at the price of lower performance than the
%% in-memory PRNG guidance and thus may result in fewer counterexamples and more
%% timeouts.

\section{Threats to validity}

\paragraph{Selection of benchmarks}
All our benchmarks are Java methods deprecated in the Oracle JDK 15.
The JDK is extremely well known to Java developers, and a lot
of Java application code evolves similarly.
%
 %% Also,
%% as explained at the beginning of Section~\ref{sec:experimental-results}, we excluded 
%% certain methods from our experimental evaluation. 
However, our claims may not extend to other programming languages.

%\paragraph{Not handled}

\paragraph{Quality of refactorings} Refactorings need to result in code that
remains understandable and maintainable.  It is difficult to assess
objectively how well our technique does with respect to this subjective
goal.
%
%% In our experimental evaluation, we define acceptable
%% solutions not just using the heap equivalence criterion, but also regular
%% expressions modelling what we consider sensible transformations after revewing
%% the deprecated method's JavaDoc.  These patterns are subjective in nature and
%% may also lead us to reject correct refactorings which do not conform to our
%% expectations.
We manually inspected the refactorings obtained with
our tool and found them to represent sensible transformations.

\paragraph{Efficiency and scalability of the program synthesiser}
%
We apply program synthesis and fuzzing.  This implies that our broader claim
is threatened by scalability limits of these techniques.  While for the
majority of our experiments the synthesiser was able to find a solution,
there were a few cases where it timed out, either because it could not
generate a candidate, or it could not verify it.  Component libraries with a
diverse range of component sizes may help mitigate this effect.

\paragraph{Prompt engineering for Claude}
%
In our current experiments, we engineered prompts for the Claude LLMs. 
While we made best efforts to follow the official prompt engineering
guidelines\footnote{https://docs.anthropic.com/en/docs/prompt-engineering},
which presents prompt design techniques to improve model performance, there
might be better ways of composing it.  This might invalidate our claim that
the symbolic and neural techniques deliver roughly the same performance.

%
%as future work, we plan on alos fine-tuning Claude 2.1 and Claude 3.
% While we made efforts to provide a comprehensive prompt (e.g. we took inspiration from \cite{white2023chatgpt}, which
% presents prompt design techniques for software engineering in the form of patterns), as future work, we plan on also fine-tuning GPT-4 Turbo. 

%% \section{Semantically different refactorings}\label{sec:semantically-different-refactorings}


%% Sometimes, the deprecated and the refactored features are not semantically
%% equivalent on the entire domain. Reasons for this might be that a bug
%% was fixed, a certain functionality was improved, or simply the fact
%% that the deprecated feature is only expected to be used on restricted
%% parts of the input domain. In such a case, the deprecated and the new
%% feature may not be semantically equivalent on the entire input domain.

%% For illustration, let's consider the example in Figure~\ref{ex:three-dates}.
%% In the example, we make use of the constructor \texttt{new Date(year, month, day)}, 
%% which is deprecated with the 
%% recommendation to use the \texttt{Calendar} class instead.
%% The refactoring is not trivial given that the 
%% \texttt{Date} constructor expects its year parameter to be an offset
%% from 1900.  This needs to be
%% transformed by adding 1900 when using \texttt{Calendar} API, as shown in the
%% example. Moreover, as already seen in the motivational example in  Figure~\ref{ex:deprecated-method-other},
%% the \texttt{Calendar} constructor is protected,
%% meaning that we must use \texttt{getInstance} to obtain a
%% \texttt{Calendar} object.
%% Alternatively, the \texttt{GregorianCalendar} class may be used with
%% similar difficulties.
%% In the example, we use \texttt{date1}, \texttt{date2} and \texttt{date3} to denote the three distinct ways of constructing a date.

%% \begin{figure}
%%   \begin{lstlisting}[mathescape=true,showstringspaces=false]
%%   // Deprecated:
%%   Date date1 = new Date(year, month, day);

%%   // Should have been:
%%   Calendar calendar = Calendar.getInstance();
%%   calendar.set(1900 + year, month, day, 0, 0, 0);
%%   calendar.set(Calendar.MILLISECOND, 0);
%%   Date date2 = calendar.getTime();
    
%%   // Or:
%%   Date date3=new GregorianCalendar(1900+year,month,day).
%%              getTime();
%%   \end{lstlisting}
%% \caption{Deprecated `Date' example.}
%% \label{ex:three-dates}
%% \end{figure}

%% Notably, the ways in which \texttt{date2} and \texttt{date3} are computed
%% represent valid refactorings for the original code computing
%% \texttt{date1}. However, when we provide this example to our
%% verification phase (as described in Section~\ref{sec:encoding}), we obtain the following
%% counterexample:

%%   \begin{lstlisting}[mathescape=true,showstringspaces=false]
%%     int year = -89412298;
%%     int month = 1439435067;
%%     int day = 378993182;
%%   \end{lstlisting}

%%   For these values of \texttt{year}, \texttt{month} and \texttt{day}, the three
%%   dates evaluate to:

%%   \begin{lstlisting}[mathescape=true,showstringspaces=false]
%%     date1: "Sat May 02 00:00:00 CEST 31580172"
%%     date2: "Fri Jul 02 00:00:00 CEST 31580799"
%%     date3: "Fri Jul 02 00:00:00 CEST 31580799"
%%   \end{lstlisting}

%%   Where \texttt{date1} evaluates to a different date than \texttt{date2} and \texttt{date3}. In principle, this should not affect any use case as the counterexample
%%   \texttt{year}, \texttt{month} and \texttt{day} are clearly artificial and outside the expected domain of a calendaristic date.
%%   In particular, for this example, we deduced that the negative year is the cause for the different values of \texttt{date1}, \texttt{date2} and \texttt{date3}.

%%   This counterexample is enough to stop our synthesiser from finding any of the correct refactorings in Figure~\ref{ex:three-dates}.
%%   To overcome this problem, we plan on computing preconditions for the verification phase. Namely, for this example, we would collect all the concrete values that
%%   \texttt{year}, \texttt{month} and \texttt{day} get instantiated to in the unit tests associated with the given project, and use them to
%%   infer a precondition over the domain of these three variables. For \texttt{year}, this should tell us that the expected year must be positive.
%%   Consequently, the verifier won't attempt to find counterexamples involving negative years, thus eliminating the counterexample above.

\section{Conclusions and Related Works}

In this paper, we presented and contrasted a symbolic and a neural automatic program refactoring
techniques that eliminate uses of deprecated methods. After implementing and testing them, we drew
conclusions about their respective strenghts, and ways in which they complement each other.
Next, we discuss some of the related works.
%Our
%techniques are guided by types and Javadoc code hints, and
%performs semantic reasoning and search in the space of possible
%refactorings using automated program synthesis. 

\paragraph{Program refactoring}

With respect to works targeting the refactoring of deprecated instances, the work of Perkins directly replaces calls to deprecated methods by their bodies~\cite{DBLP:conf/paste/Perkins05}.
As mentioned earlier in the paper, we believe that this solution does not follow the intention of the language designers. Moreover, it can
introduce concurrency bugs as inlining calls to deprecated
methods can cause undesirable effects if the original function was
synchronised.  While it is not possible for two invocations on the
same object of the synchronised original method to interleave, this is
not guaranteed after inlining the method's body. Notably, the authors of \cite{10.1007/978-3-642-14107-2_11}
show that it is very easy for a refactoring that works on concurrent
programs to introduce concurrency bugs by enabling new interactions between
parallel threads. %They also provide techniques to make such refactorings behavior-preserving.

A related class of techniques aim to adapt APIs after library updates. Such techniques
automatically identify change rules linking different library releases~\cite{DBLP:conf/icse/WuGAK10,DBLP:conf/kbse/Huang0PW021}. Conversely to our work, a change rule describes a match between methods existing in the old release, but which have been removed or deprecated in the new
one, and replacement methods in the new release. However, they do not provide the actual refactored code.
In \cite{DBLP:journals/tse/LeeWCK21}, Lee et al. address the problem of outdated APIs in documentation references. Their insight is that API updates in documentation can be derived from API implementation changes between code revisions. Conversely, we are looking at code changes, rather than documentation.
Other works focus on automatically updating API usages for Android apps based on
examples of how other developers evolved their apps for the same changes \cite{DBLP:conf/issta/FazziniXO19,DBLP:conf/iwpc/HaryonoTKSML0J20}. 
Furthermore, \cite{DBLP:journals/ese/HaryonoTLJLKSM22} improves on \cite{DBLP:conf/iwpc/HaryonoTKSML0J20} by using a data-flow analysis 
to resolve the values used as API arguments and variable name denormalization to improve the
readability of the updated code.
As opposed to these works which rely on examples of similar fixes, our approach uses code hints from the Javadoc.

%% In~\cite{DBLP:conf/kbse/Huang0PW021}, Huang et al. automatically find replacements for missing APIs in library update (an API is considered as a public method in a public class). As new library versions are released, some APIs may get deprecated, removed or refactored.
%% The focus in this work is on locating a replacement API through heuristics based on method signatures, rather than on finding an equivalent block of code.
%% The tool presented in this paper, {\sc RepFinder} searches deprecation messages, the current library and external libraries in order to find replacement APIs. Replacement APIs must have either identical signatures to the missing API, or similar. For the latter category, {\sc RepFinder} computes the weighted sum of the distances of return types, method names and parameter types. Finally, the API with the smallest distance is selected. As opposed to our technique, this work offers no guarantees as it doesn't
%% investigate the body of the deprecated method or of the replacement one. Moreover, API calls
%% are simply replaced by some other API call, rather than more complex code as in our situation.


%name change, signature change, move to a different package

%% The signature of mo can be changed in
%% ln, including its return type, method name and parameter type.
%% mo can also be move into a different class in ln.

%% At the class level, mo can be substituted
%% by a method from another class, either with the same method
%% name or a different method name. At the method level, mo
%% can be substituted by a different method from its own class.



There are several rule-based source-to-source transformation systems that provide languages in which
transformation rules based on the program's syntax can be expressed~\cite{stratego,txl}. 
%% Syntax-driven refactoring base program transformation decisions
%% on observations on the program's syntax tree.  Visser
%% presents a purely syntax-driven framework~\cite{stratego}.  The
%% presented method is intended to be configurable for specific
%% refactoring tasks, but cannot provide guarantees about semantics
%% preservation.
%% In~\cite{txl}, Cordy et al. introduce TXL, a programming language and rapid prototyping
%% system specifically designed to support structural source transformation.
As opposed to our work, such systems require the user to provide the actual transformation rules and are syntax-guided.


Search-based approaches to automating the task of software refactoring, based
on the concept of treating object-oriented design as a combinatorial optimisation
problem, have been also proposed~\cite{search1,search2}. They usually make use of techniques such as
simulated annealing, genetic algorithms and multiple ascent hill-climbing.
%O'Keffe and Cinn{\'{e}}ide present a search-based refactoring
%refactoring~\cite{search1, search2}, which is similar to syntax-driven
%refactoring.  They rephrase refactoring
%which is rephrased as an optimisation problem,
%using code metrics as fitness measures~\cite{search1, search2}.  %% As such, the method optimises
%% syntactical constraints and does not take program semantics into
%% account.
While our technique is search based, the search is guided by types, code hints, as well as counterexamples
provided by property-based testing.

Closer to our work, several refactoring techniques make explicit use of some form of semantic information.
%With respect to automated program refactoring,
Khatchadourian et al. use a type inference algorithm to automatically transform legacy Java code (pre Java 1.5) to use the \texttt{enum} construct~\cite{sawin}.
%\cite{sawin} by Sawin et al.,
Other automated refactoring techniques aim to transform programs into a particular design pattern~\cite{chris,bae}.
%In \cite{bae}, Bae et al. propose an automated approach to
%identifying the candidate spots, thene  on which the defined refactoring strategy would be applied for the
%~and \cite{chris} by Christopoulou et al.  In contrast to these approaches,
%our procedure constructs an equivalence proof before transforming the
%program.
%
%% In \cite{conf/sigsoft/GyoriFDL13}, Gyori et al. present a
%% similar refactoring to ours but performed in a syntax-driven manner.
%
%
%
Steimann et al.~present Constraint-Based Refactoring in \cite{Steimann2011},
\cite{Steimann2012Pilgrim} and \cite{Steimann2011KollePilgrim},
where given well-formedness logical rules about the program are translated into constraints that are then solved to assist the refactoring.
%% , their approach
%% generates explicit constraints
%% over the program's abstract syntax tree to
%% prevent compilation errors or behaviour changes by automated refactorings.
% This gives rise to a flexible framework of customisable refactorings,
% implementable through a refactoring constraint specification language
% (cf. \cite{Steimann2011KollePilgrim}).
%% The approach is limited by the information
%% a program's AST provides and thus favours conservative implementations of
%% syntax-focused refactorings such as \emph{Pull Up Field}.
%
Fuhrer et al.~implement a type constraint system to introduce missing type
parameters in uses of generic classes (cf. \cite{DBLP:conf/ecoop/FuhrerTKDK05})
and to introduce generic type parameters into classes that do not provide
a generic interfaces despite being used in multiple type contexts
(cf. \cite{DBLP:conf/icse/KiezunETF07}).
%
% Raychev et al.~present a semi-automatic approach where users perform
% incomplete refactorings manually and then employ a constraint solver
% to find a sequence of default refactorings such as move or rename
% which include the users' changes. The engine is limited to syntactic
% matching with the users' partial changes and does not consider program
% semantics~\cite{DBLP:conf/oopsla/RaychevSSV13}.
%
% Weissgerber and Diehl rely on meta information to classify changes
% between software versions as refactorings~\cite{weiss}.  The technique
% aims to identify past refactorings performed by programmers, but is
% not a decision procedure for automated refactorings.
%
%
% Bavota et al. implement refactoring decisions in \cite{Bavota:2011:IEC}
% using semantic information limited to identifiers and comments,
% which may differ from the actual semantics (e.g. due to bugs).
%
%Some of the closest to our refactoring from the point of considering program semantics are~\cite{Kataoka:2001:ASP:846228.848644}, 
Kataoka et al. use
program invariants (found by the dynamic tool Daikon) to infer
%such that when a particular pattern of invariant
%relationships appears at a program point, a
whether specific refactoring are applicable~\cite{Kataoka:2001:ASP:846228.848644}. %The invariant detection tool called Daikon is used to
%dynamically infer the required invariants.
Notably, %the refacoring process relies on finding the required invariants, which
finding invariants is notoriously difficult especially
for real-world programs. Moreover, the technique is limited to a small number of refactorings and does not include
the elimination of deprecated instances.
In \cite{conf/sigsoft/GyoriFDL13}, Gyori et al. present the tool {\sc LambdaFicator} which
automates two pattern-based refactorings. The first refactoring converts
anonymous inner classes to lambda expressions. The second
refactoring converts for loops that iterate over Collections
to functional operations that use lambda expressions.
The {\sc LambdaFicator} tool~\cite{DBLP:conf/icse/FranklinGLD04} is available as a
NetBeans branch.



%% . However, because search-based refactoring is a novel
%% approach it has yet to be established which search techniques are most suitable
%% for the task. In this article we report the results of an empirical comparison of
%% simulated annealing, genetic algorithms and multiple ascent hill-climbing in search-based
%% refactoring. 


%% Kataoka et al. interpret program semantics to apply refactorings
%% \cite{Kataoka:2001:ASP:846228.848644}, but use dynamic test execution
%% rather than formal verification, and hence their transformation lacks
%% soundness guarantees.
%
%% Franklin et al. implement a pattern-based refactoring approach
%% transforming statements to stream queries~\cite{Gyori:2013:CGI:2491411.2491461}.
%% Their tool LambdaFicator~\cite{DBLP:conf/icse/FranklinGLD04} is available as a
%% NetBeans branch.
%We compared \tool against it in our experimental evaluation
%in Sec.~\ref{experiments-results}.
%
%% \paragraph{Heap Logics}
%
%% While many decidable heap logics have been developed recently, none are
%% expressive enough to capture operations allowed by the Java Collection
%% interface, operations allowed by the Java Stream interface as well as
%% equality between collections (for lists this implies that we must be able to
%% reason about both content of lists and the order of
%% elements)~\cite{DBLP:conf/cav/ItzhakyBINS13, DBLP:conf/cav/PiskacWZ13,
%% DBLP:conf/esop/BrainDKS14, DBLP:conf/popl/MadhusudanPQ11,
%% DBLP:conf/atva/BouajjaniDES12, DBLP:conf/lpar/DavidKL15}.  On the other
%% hand, very expressive transitive closure
%% logics~\cite{DBLP:conf/csl/ImmermanRRSY04} are not concise and easily
%% translatable to stream code.

Although it is guided by semantics, our techniques does not expect any
precomputed information such as logical well-formedness properties or
invariants. Instead, we make use of information that is already
available in the original program and Javadoc, namely code hints and
type information. Moreover, we explore the space of all potential
candidate programs by combining techniques from type-directed,
component-based and counterexample-guided inductive synthesis.

\paragraph{Symbolic program synthesis}


CEGIS based approaches to program synthesis have been previously used
for program transformations such as superoptimisation and
deobfuscation~\cite{DBLP:conf/icse/JhaGST10}.  In
\cite{DBLP:journals/corr/abs-1712-07388}, David et al.~present an
automatic refactoring tool that transforms Java with external
iteration over collections into code that uses Streams. Their approach
is based on CEGIS and makes use of formal verification to check the
correctness of a refactoring.  Cheung et al.~describe a system that
automatically transforms fragments of application logic into SQL
queries~\cite{DBLP:conf/pldi/CheungSM13} by using a CEGIS-based
synthesiser to generate invariants and postconditions that validate
their transformations (a~similar approach is presented
in~\cite{DBLP:conf/cc/IuCZ10}).  While our approach is also based
on CEGIS, we are guided by code hints and types in
order to efficiently prune the search space. Without this guidance,
the current refactoring would not be feasible.  

% type-directed synthesis
Type information has been extensively used in program synthesis to
guide the search for a solution~\cite{DBLP:conf/sfp/Katayama05,DBLP:conf/pldi/FeserCD15,DBLP:conf/pldi/OseraZ15,DBLP:journals/pacmpl/LubinCOC20,DBLP:journals/pacmpl/YamaguchiMDW21}.
In our work, we combine type information with code hints.
%
% component-based synthesis
Another direction that inspired us is that of component-based synthesis~\cite{DBLP:conf/icse/JhaGST10,DBLP:conf/pldi/GulwaniJTV11,DBLP:conf/popl/FengM0DR17}, where the target program is generated by composing components from a library. Similarly
to these approaches, we use a library of components for our program
generation approach. However, our technique uses information about types
and code hints to build the component library, which is specific to each refactoring.

%% For the intro:
% not correct-by-contruction, preserves the behaviour, refer to the paper
% on refactoring concurrent programs.
%% for equivalence checking:
%% we use an explicit return variable ret, which is live. Also, we make "this explicit".
%% Essentially, we must capture the whole program state, consisting of return values and heap. 
%% Todo: related work, alising check, intro, equiv check.


%% An approach to program synthesis very similar to ours is Syntax Guided
%% Synthesis (SyGuS)~\cite{sygus}.  SyGuS synthesisers supplement the logical
%% specification with a syntactic template that constrains the space of allowed
%% implementations.  Thus, each semantic specification is accompanied by a
%% syntactic specification in the form of a grammar.  Other second-order
%% solvers are introduced in~\cite{DBLP:conf/pldi/GrebenshchikovLPR12,
%% DBLP:conf/cav/BeyenePR13}.  As opposed to ours, these focus on
%% Horn clauses.


  %% The main difference (besides the actual
%% goal of the work, which is different from ours) to our work is that the
%% lists they operate on are immutable and do not support operations such as
%% remove.  Capturing the potential side effects caused by such operations is
%% one of our work's main challenges.

Program synthesis is generally guided by the principle of correct-by-construction, meaning that it provides strong correctness guarantees about the generated code.
As opposed to that, our work is guided by program testing. This is similar in nature with genetic programming~\cite{Koza92} and genetic improvement~\cite{DBLP:journals/dagstuhl-reports/PetkeGFL18,7911210}. In particular, the latter has been used for program refactoring. Genetic programming and genetic improvement obtain candidate programs by applying operations analogous to genetic processes to an existing population of programs. Conversely, we pose the code generation problem as safety verification.
%
%% Often, genetic improvement
%% \cite{DBLP:journals/dagstuhl-reports/PetkeGFL18} is used for the
%% purpose of code refactoring. Due to the manner in which such works measure
%% the fitness of potential refactorings, genetic improvement cannot vary
%% the refactoring context.  Conversely, our technique allows varying this
%% context until a refactoring is being found.
%


\paragraph{Large Language Models}
LLMs have been used successfully in recent years for code generation tasks, with applications ranging from code completions~\cite{llmsforcodecompletion,ni2023lever,codegenclasslevel,Ding2024cocomic}, translations~\cite{tang-etal-2023-explain,lostintranslation} to repository-level generation~\cite{zhang2023repocoder} and general software engineering tasks~\cite{yang2024sweagent}.
Those models are typically pre-trained on vast amount of data, fine-tuned for specific tasks, used by applying advanced prompt engineering techniques~\cite{jiang2024survey}.
Among them, GPT-4~\cite{openai2024gpt4}, Claude2.1~\cite{claude}, Claude3~\cite{claude}, 
LLaMa~\cite{touvron2023llama} are general-purpose models covering a diverse set of language-related applications; there are also models pre-trained/fine-tuned specifically for code generation and programming-related task like CodeLLaMa2~\cite{roziere2024code}, 
StarCoder2~\cite{lozhkov2024starcoder}, DeepSeek-Coder~\cite{guo2024deepseekcoder}, GrammarT5~\cite{grammart5}, etc.

% Among them, GPT-4 Turbo~\cite{chatgpt} is a LLM trained using Reinforcement Learning from Human Feedback.
% There are recent studies investigating GPT-4 Turbo's abilities to solve software engineering problems such as bug fixing~\cite{sobania2023analysis,chatgptbugs} and 
% prompt pattern design for refactoring, requirements elicitation, and software design ~\cite{white2023chatgpt}.
In this work, we are interested in using the Claude family as a baseline, with the goal of investigating how our specialised symbolic refactoring technique compares
against general-purpose LLMs.

%% \section{Conclusion}

%% In this paper, we presented an automated program refactoring technique
%% that eliminates uses of deprecated methods. Our technique is guided by
%% types and Javadoc code hints, and performs semantic reasoning and
%% search in the space of possible refactorings using automated program
%% synthesis.  In particular, it makes use of component-based and
%% counterexample-guided inductive synthesis. We encoded both the
%% synthesis of refactoring candidates and their verification as program
%% safety problems that we then solved with fuzz testing.

%\acks

% We recommend abbrvnat bibliography style.
\bibliographystyle{ieeetr}
%\bibliographystyle{abbrv}
\bibliography{document}
%\appendix
%\input{appendix}

\if 0
We found a situation where the code hint provided by the Javadoc deprecation comments is
not semantically equivalent to the original method. For instance, the method \lstinline{javax.swing.JMenuBar#getComponentAtIndex(int)}
that retrieves the \lstinline{Component} at a specific index performs bound-checking: if the index is not within
range then a \lstinline{null} value is returned. However, the suggested replacement \lstinline{javax.swing.JMenuBar#getComponent(int)}
does not perform bound-checking. We noticed that \llm is able to generate the correct replacement, but
it fails equivalence checking and hence misses this refactoring.

====
Some of the missed refactorings were caused by the fact that the return
value was dependent on an external system state which changed frequently between
invocations (e.g. \texttt{getFreePhysicalMemorySize} in \texttt{OperatingSystemMXBean}),
and thus appeared nondeterministic to our engine.

=====
for our equivalence checking predicate implementation, we pick a single
concrete subclass for deprecated abstract methods. This proved sufficient for
all benchmarks in our set. However,
there are classes in the JCL without any
existing implementation (e.g. \lstinline{javax.swing.InputVerifier}),  and thus no concrete
method against we can verify equivalence of candidates.

====

We e.g. hard-coded that we should not generate an ArrayList using the constructor that takes a single int, since that is the capacity. But ArrayList has other constructors that we use.

\fi

\end{document}

%                       Revision History
%                       -------- -------
%  Date         Person  Ver.    Change
%  ----         ------  ----    ------

%  2013.06.29   TU      0.1--4  comments on permission/copyright notices

