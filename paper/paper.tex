%-----------------------------------------------------------------------------
%               Template for OOPSLA
%               based on:
%               Template for sigplanconf LaTeX Class
%
% Name:         sigplanconf-template.tex
%
% Purpose:      A template for sigplanconf.cls, which is a LaTeX 2e class
%               file for SIGPLAN conference proceedings.
%
% Guide:        Refer to "Author's Guide to the ACM SIGPLAN Class,"
%               sigplanconf-guide.pdf
%
% Author:       Paul C. Anagnostopoulos
%               Windfall Software
%               978 371-2316
%               paul@windfall.com
%
% Created:      15 February 2005
%
%-----------------------------------------------------------------------------
\documentclass[sigconf,review,anonymous]{acmart}
\acmConference[ESEC/FSE 2022]{The 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering}{14 - 18 November, 2022}{Singapore}
%\documentclass[runningheads,a4paper]{llncs}
%\documentclass[sigconf,authordraft]{acmart}
%\acmConference[ESEC/FSE 2017]{11th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering}{4--8 September, 2017}{Paderborn, Germany}
%\documentclass[10pt,numbers]{sigplanconf}
%\usepackage[1stsubmission]{oopsla2016}
%\usepackage[2ndsubmission]{oopsla2016}

%\usepackage[scaled]{helvet} % see www.ctan.org/get/macros/latex/required/psnfss/psnfss2e.pdf

%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}


\makeatletter
\newcommand{\removelatexerror}{\let\@latex@error\@gobble}
\makeatother


\usepackage{microtype}
\usepackage{url}                  % format URLs
\usepackage{listings}          % format code
\lstset{
  mathescape, 
  language={Java},
  basicstyle=\footnotesize
}
\usepackage{enumitem}      % adjust spacing in enums
%\usepackage[colorlinks=true,allcolors=blue,breaklinks,draft=false]{hyperref}   % hyperlinks, including DOIs and URLs in bibliography
% known bug: http://tex.stackexchange.com/questions/1522/pdfendlink-ended-up-in-different-nesting-level-than-pdfstartlink

\usepackage{graphicx}
\usepackage{float,subfig}
\usepackage{xspace,framed}
\usepackage{colortbl}
\usepackage{calc}
\usepackage[ruled,vlined]{algorithm2e}

%\usepackage{algorithm}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{tikz}
\usepackage[justification=centering]{caption}
\usepackage{stmaryrd}
\usetikzlibrary{positioning, arrows, automata, shapes}
\usepackage{hhline}
\usepackage{pifont}
%\usepackage{cite}
\usepackage{pdflscape} % Experiments table is landscape
\usepackage{longtable}
\usepackage{afterpage}
\usepackage{wasysym}

\usepackage[justification=centering]{caption}
\input{macros}

%\setlength{\belowcaptionskip}{-10pt}
\setlength{\textfloatsep}{1em}
\setlength{\dbltextfloatsep}{1em}
\setlength{\abovecaptionskip}{0.1em}
% \floatsep: space left between floats (12.0pt plus 2.0pt minus 2.0pt).
% \textfloatsep: space between last top float or first bottom float and the text (20.0pt plus 2.0pt minus 4.0pt).
% \intextsep : space left on top and bottom of an in-text float (12.0pt plus 2.0pt minus 2.0pt).
% \dbltextfloatsep is \textfloatsep for 2 column output (20.0pt plus 2.0pt minus 4.0pt).
% \dblfloatsep is \floatsep for 2 column output (12.0pt plus 2.0pt minus 2.0pt).
% \abovecaptionskip: space above caption (10.0pt).
% \belowcaptionskip: space below caption (0.0pt).

\allowdisplaybreaks

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% uncomment for extended version 
\newcommand*{\extended}{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% The following oopsla2016 options are available:
%
% 1stsubmission   For the initial submission
% 2ndsubmission   For the 2nd submission
% final           For camera-ready

\begin{document}

%\copyrightdata{978-1-nnnn-nnnn-n/yy/mm} 
%\doi{nnnnnnn.nnnnnnn}

% Uncomment one of the following two, if you are not going for the 
% traditional copyright transfer agreement.

%\exclusivelicense                % ACM gets exclusive license to publish, 
                                  % you retain copyright

%\permissiontopublish             % ACM gets nonexclusive license to publish
                                  % (paid open-access papers, 
                                  % short abstracts)

\title{Refactoring of Java deprecated guided by types and code hints}
%\subtitle{Subtitle Text, if any}
% double-blind submission
% single-blind only for Technical Research: http://icse2017.gatech.edu/technical-research-cfp
 % \authorinfo{Cristina David}
 %            {University of Oxford}
 %            {cristina.david@cs.ox.ac.uk}
 % \authorinfo{Pascal Kesseli}
 %            {University of Oxford}
 %            {pascal.kesseli@cs.ox.ac.uk}
 % \authorinfo{Daniel Kroening}
 %            {University of Oxford}
 %            {kroening@cs.ox.ac.uk}

\author{Cristina David}
\email{cristina.david@bristol.ac.uk}
\orcid{0000-0002-9106-934X}
\affiliation{
  \institution{University of Bristol}  
  \city{Bristol}  
  \country{UK}
  \postcode{BS8 1TH}
}
\author{Pascal Kesseli}
\affiliation{
  \institution{Diffblue}  
  \country{UK}
}
\author{Daniel Kroening}
\affiliation{
  \institution{Amazon}  
  \country{UK}
}


%% \author{
%% \mbox{Cristina David}\inst{1} \and
%% Pascal Kesseli\inst{2} \and
%% Daniel Kroening\inst{2}}

%% \institute{University of Bristol, UK 
%% \and
%%   University of Oxford, UK}

%% \author{Cristina David}
%% \affiliation{University of Oxford}
%% \email{cristina.david@cs.ox.ac.uk}

%% \author{Pascal Kesseli}
%% \affiliation{University of Oxford}
%% \email{pascal.kesseli@cs.ox.ac.uk}

%% \author{Daniel Kroening}
%% \affiliation{University of Oxford}
%% \email{kroening@cs.ox.ac.uk}

\begin{abstract}
%
  Refactorings are structured changes to existing software that leave its
  externally observable behaviour unchanged.  Their intent is to improve
  readability, performance or other non-behavioural properties. 
  State-of-the-art automatic refactoring tools are {\em syntax}-driven and,
  therefore, overly conservative.  In this paper we explore {\em
  semantics}-driven refactoring, which enables much more sophisticated
  refactoring schemata.  As~an exemplar of this broader idea, we present
  an automatic refactoring tool that replaces uses of deprecated legacy APIs/libraries
  with the latest ones.
  Our refactoring procedure performs
  semantic reasoning and search in the space of possible refactorings using
  automated program synthesis.  Our experimental results support the
  conjecture that semantics-driven refactorings are more precise and are
  able to rewrite more complex code scenarios when compared to syntax-driven
  refactorings.
%
\end{abstract}

\maketitle

%\keywords{program refactoring, program synthesis, program verification}

%\category{CR-number}{subcategory}{third-level}

% general terms are not compulsory anymore, 
% you may leave them out
%\terms
%term1, term2

%% \keywords
%% Maintenance and reuse



\section{Introduction}\label{sec:intro}

As a project evolves, there are certain fields, methods or classes
that the developers are discouraged from using in the future as
they've been superseded and may cease to exist in the future.
However, removing them directly would break the backward compatibility
of the project's API.  Instead, such elements can be tagged with the
\code{@Deprecated} annotation.  %% In general, when deprecating a
%% field/method/class, the \code{@deprecated} Javadoc tag is used in the
%% comment section to inform the developer the reason of deprecation and
%% what can be used in place.


%Even in the presence of such recommendations,
The transformation of existing code such that it doesn't use deprecated APIs
%to the recommended format
is not always straightforward,
as illustrated in Fig.~\ref{ex:deprecated-method-other}. In the example, we
make use of the \code{getHours} method of the \code{Date} class,
which is deprecated.
%% with the 
%% recommendation to use \code{Calendar.get(Calendar.HOUR\_OF\_DAY)} instead.
In this situation, in order to replace the use of the deprecated method,
we must first obtain a \code{Calendar} object.
However, we can't use the \code{Calendar} constructor as it is protected,
and we must instead call \code{getInstance}.
Furthermore, in order to be able to use this \code{Calendar} object for our purpose,
we must first set its time using the existing \code{date}.
We do this by calling \code{setTime} with \code{date} as argument.
%% followed by setting the date using \code{set}
%% and returning a Date object by invoking \code{getTime}.
%% Alternatively, the \code{GregorianCalendar} class may be used with
%% similar difficulties.
Finally, we can retrieve the hour by calling
%Only after all this set up, we can finally make use of the recommendation
\code{calendar.get(Calendar.HOUR\_OF\_DAY)}.

\begin{figure}
\begin{lstlisting}[mathescape=true,showstringspaces=false]
void main(String[] args) {
  // Deprecated:
  int hour = date.getHours();
  
  // Should have been:
  final Calendar calendar = Calendar.getInstance();
  calendar.setTime(date);
  int hour = calendar.get(Calendar.HOUR_OF_DAY);
}
\end{lstlisting}
\caption{Deprecated method example.}
\label{ex:deprecated-method-other}
\end{figure}


While it is trivial for IDEs such as Eclipse or IntelliJ IDEA to
recognise the use of deprecated APIs and warn the programmer about it,
refactoring the code to a preferred API is significantly more
complex. Indeed neither Eclipse 4.15.0 nor IntelliJ IDEA 2020.2.2 make
any attempt to suggest a better alternative to the user. There is also
very limited support offered by the research community, mainly
focusing on replacing calls to deprecated methods by their
bodies~\cite{DBLP:conf/paste/Perkins05}. 

Consequently, the majority of
such refactorings are done manually, which is a costly,
time-intensive, and not least error-prone process.  This situation is
also a real issue for legacy code that makes use of deprecated APIs
that need to be replaced with latest APIs/libraries.

The main cause of difficulty when replacing deprecated instances lies
with the fact that refactorings should not change the functionality of
the code, i.e., they must preserve observational equivalence.
Ensuring observational equivalence is far from trivial.  For instance,
inlining calls to deprecated methods can break equivalence if the
original function was synchronised.  While it is not possible for two
invocations  on the same object of the synchronised original method to
interleave, this is not guaranteed after inlining the method's body.
Fig.~\ref{ex:unsound-refactoring-original} and
Fig.~\ref{ex:unsound-refactoring-refactored} illustrate an example where
the "Extract Local Variable..." refactoring integrated in Eclipse 4.21.0
inadvertently removes synchronisation for the evaluation of the expression
$x > 0$.

\begin{minipage}{.23\textwidth}
\begin{lstlisting}[mathescape=true,showstringspaces=false]

synchronized (this) {
  if (x > 0) {
    foo();
  }
}
// ...
synchronized (this) {
  if (x > 0) {
    bar();
  }
}
\end{lstlisting}
\captionof{figure}{Concurrent example.}
\label{ex:unsound-refactoring-original}
\end{minipage}
\begin{minipage}{.23\textwidth}
\begin{lstlisting}[mathescape=true,showstringspaces=false]
boolean tmp = x > 0;
synchronized (this) {
  if (tmp) {
    foo();
  }
}
// ...
synchronized (this) {
  if (tmp) {
    bar();
  }
}
\end{lstlisting}
\captionof{figure}{Unsound refactoring.}
\label{ex:unsound-refactoring-refactored}
\end{minipage}
\vspace{0.8em}

%% Another example of a refactoring that unintuitively breaks
%% observational equivalence is given in Figure~\ref{}.
%% \todo{add example here.}

Consequently, refactorings that rely solely on a program's syntax
and do not consider its semantics end up
being overly conservative, mostly addressing structural changes
to the program.  For illustration, when referring back to the example in
Figure~\ref{ex:deprecated-method-other}, one must understand the code's semantics in order to conclude that the
refactoring maintains the same functionality.
%
%% such refactorings are based on semantics rather
%% than syntax. For illustration, let's refer back to the example in
%% Figure~\ref{ex:deprecated-method}.  A {\em syntax-driven refactoring}
%% addresses structural changes to the program requiring only limited
%% information about a program's semantics, whereas a {\em
%%   semantics-driven refactoring} requires detailed understanding of the
%% program semantics in order to be applied soundly.  Clearly, the
%% refactoring of the call to the deprecated \code{Date} constructor
%% falls into the latter category. 
%
%\subsection{From the previous introduction:}
%% Refactorings are structured changes to existing software which leave its
%% externally observable behaviour unchanged.  They improve non-functional
%% properties of the program code, such as testability, maintainability and
%% extensibility while retaining the semantics of the program.  Ultimately,
%% refactorings can improve the design of code, help finding bugs as well
%% as increase development speed and are therefore seen as an integral part
%% of agile software engineering processes~\cite{DBLP:conf/xpu/Kerievsky04b,
%% Fowler1999}.
%
%% However, manual refactorings are a costly, time-intensive, and not least
%% error-prone process.  This has motivated work on automating specific
%% refactorings, which promises safe application to large code bases at low
%% cost.  We differentiate in this context between {\em syntax-driven} and {\em
%% semantics-driven} refactorings.  While the former address structural changes
%% to the program requiring only limited information about a program's
%% semantics, the latter require detailed understanding of the program
%% semantics in order to be applied soundly.  An example of a refactoring that
%% requires a semantics-driven approach is {\em Substitute Algorithm}, where an
%% algorithm is replaced by a clearer, but equivalent
%% version~\cite{Fowler1999}.
%
%AST-based constraints 
%
%% A syntax-driven approach is insufficient to perform such substantial
%% transformations.  Figure~\ref{ex:syntax-limits} illustrates this using an
%% example: Both loops in the code implement the same behaviour.  In order to
%% recognise this and apply {\em Substitute Algorithm}, pattern-based
%% approaches need explicit patterns for vastly different syntaxes implementing
%% the same semantics, which is infeasible for practical applications.
%
%% \todo{I've removed the reference to Move Field as it's a bit controversial given that
%% Steinmann uses it to explain that purely syntactic approaches are not enough.}
%
Notably, the limitations of syntax-driven refactorings have been observed
in several works, resulting in an emerging trend to incorporate more {\em semantic}
information into refactoring decisions, such as Abstract Syntax Tree (AST) 
type information,
further preventing compilation errors and behaviour changes
\cite{Steimann2011,Steimann2012Pilgrim,Steimann2011KollePilgrim}.
%% use constraints involving the abstract syntax tree type 
%% information in order to handle the {\em Move Field} (when a field
%% is used by another class more than the class on which it is defined)
%% of syntax-driven, e.g.  
%% An example 
%% of a refactoring that is well handled by the syntax-driven approach 
%% implemented using constraints over the program's abstract syntax tree
%% There is an emerging trend to incorporate more {\em semantic}
%% information into refactoring decisions, such as AST type information,
%% further preventing compilation errors and behaviour changes
%% \cite{Steimann2011}.

In this paper, we take a step further in this direction by proposing a
semantics-driven refactoring technique that makes use of program synthesis.
While we focus on the refactoring of deprecated methods, the same
technique can be applied to deprecated fields and classes.
%Our goal is to design a refactoring techniques for deprecated instances that computes refactorings fast, thus allowing integration with an IDE.

%% As explained in more detail in the overview in Section~\ref{sec:overview},
%% the program synthesis technique will be guided by {\em code hints}
%% and {\em types}, as well 

Although powerful, program synthesis is very hard due to the vast search space, i.e. space of possible programs that must be considered. Thus, its success is generally relient on how efficiently it is guided towards a solution by discarding uninteresting programs as early as possible. Most commonly, the information used to guide the search is represented by input/output examples~\cite{DBLP:conf/pldi/FeserCD15}, type signatures~\cite{DBLP:conf/pldi/OseraZ15}, or full functional specifications of the expected code~\cite{DBLP:conf/ijcai/MannaW79}.
%
The vast search space is particularly a challenge in the current setting given that, in the absence of any additional guidance, the entire \code{java.lang} needs to be considered during code generation.
In order to prune the search space, we start by investigating the information available to our synthesis procedure.
%The information that we are going to use to direct
%\begin{itemize}

{\bf (1)} {\em Type information} about the code to be generated. In particular, this consists on the types of the objects to be consumed (i.e. the inputs), as well as the types of the objects that need to be produced (i.e. the outputs). As mentioned above, types are commonly used to guide  program synthesis. However, as shown by our experimental evaluation in Section~\ref{sec:experimental-results}, while useful, they are not sufficient in the current scenario. %, where the number of possible programs where considering \code{java.lang} %\cite{DBLP:conf/popl/FengM0DR17} \todo{add more citations}.

{\bf (2)} {\em Code hints} present in the Javadoc comments. In general, when deprecating a field/method/class, the \code{@deprecated} Javadoc tag is used in the
  comment section to inform the developer of the reason for deprecation and what can be used in its place. As we will show in Section~\ref{sec:overview}, these hints
  don't provide the whole refactoring, but can be used to guide the search process.

  {\bf (3)} One last information we have about the code to be generated is that it needs to be {\em semantically equivalent to the original}. %While this does provide us with the {\em semantics of the
  %code to be generated}, it's not easy to use. 
%* This is not enough to decide semantic equivalence, which, as discussed above, is one of the main difficulties in this refactoring.
  However, formally proving semantic equivalence is expensive and undecidable in the general case. %very difficult, having been the subject of extended research over the years \cite{}.
%Even when theoretically possible, the process of proving semantic equivalence is very expensive as it generally requires full understanding of the code's semantics.
%Moreover, it generally requires full understanding of a program's internal structure, which is expensive.
%This contradicts our aforementioned goal to design a technique that is fast. In a bid to obtain good performance,
%we investigated using greybox testing for checking program equivalence, rather than formally proving it.
In this work, instead of aiming for a formal proof,  we use coverage-guided property-based testing to check equivalence. %, which only requires limited instrumentation of the programs under test.
While we can't provide full correctness guarantees,
our experiments show that, when considering a large number of test inputs, the results are sound.

%Then, we choose a compromise between performance and ease of use and soundness. We use ...

%{\em Following the same principle, we design a greybox synthesis technique that is guided by code hints and types.}


%% For this purpose, we make use of component-based program synthesis, which assembles programs
%% from a library of existing components, e.g. methods provided by an API.
%% One of the challenges in component-based synthesis is the large number of components
%% when considering real-world APIs. Specifically for our use case, when generating Java code, we have
%% access to the whole of Java standard library.
%% We address this challenge by using the code hints provided in the Javadoc comments
%% (under the Javadoc \code{@code} tag) and type information to prune the component
%% library before the actual synthesis process.

%% With respect to the synthesis process, we propose using greybox
%% synthesis. Traditionally, synthesis techniques are either based
%% on whitebox or blackbox approaches.  The former are fully aware of the
%% internal structure of the program being synthesised, whereas the
%% latter only examine its functionality. In this paper, we use a greybox
%% approach, which relies on coverage-guided property-based testing. 
%% For this purpose, we provide an encoding for the synthesis problem, which
%% makes it amenable to solving via coverage-guided property-based testing.


%% In this paper, we follow a two-step procedure to address this challenge:
%% \begin{itemize}
%% \item[{\bf Step 1}] We start from the code hints provided as Javadoc comments, and use
%%   type guidance to seed a components library to be used during Step 2. 

%% \item[{\bf Step 2}] Based on the components library built during the first step, we use program synthesis to find a refactoring that is semantically equivalent to the original code.
%% \end{itemize}

%% syntax and semantics based approach based on program
%% synthesis.  Essentially, we make use of syntactic information from the
%% original code as well as the Javadoc comments to seed a components
%% library, which is then used by a program synthesis engine.
%% Notably, the synthesiser automatically generates refactorings
%% that are guaranteed to be semantically equivalent to the original
%% code.  With respect to the actual program synthesis technique, we use
%% a coverage-based, type and example guided synthesis.


%% fully semantic refactoring approach for removing deprecated elements.
%% There is a very broad space of methods that are able to reason about
%% program semantics.  The desire to perform refactorings safely suggests
%% the use of techniques that overapproximate program behaviours.  As one
%% possible embodiment of semantics-driven refactoring, we leverage
%% software verification technologies with the goal of reliably
%% automating refactoring decisions based on program semantics.
%% Our research
%% hypothesis is that semantics-driven refactorings are more precise and
%% can handle more complex code scenarios in comparison with
%% syntax-driven refactorings.

%% \todo{revise the next two paragraphs}
%% \paragraph{Goal of the paper} 
%% Our goal in this paper is to design a fully automated refactoring
%% technique for eliminating deprecated fields, methods and classes. Our
%% technique will make use of Java library comments to find components
%% for seeding the refactoring. Then, it will use program synthesis to
%% generate code that is semantically equivalent to the original but does not use
%% the deprecated elements.

%% One interesting aspect when using a fully semantics-driven approach is
%% that it allows introducing the notion of {\em refactoring context}.
%% The refactoring context is the context (of the code to be refactored)
%% taken into consideration when looking for a refactoring. Varying the
%% context enables the user to control the generality of the refactoring.
%% \todo{Are we going to cover context?}

%
%% For illustration, for the refactoring in Figure~\ref{ex:deprecated-method-other}...
%% \todo{hmm, what context do we consider for the standard library?}
%
%\todo{Should we also discuss our choice of synthesis in the intro?}

%% In this paper, we are interested in refactoring Java code handing
%% collections through external iteration to use streams. Our refactoring
%% procedure is based on the program semantics and makes use of program
%% synthesis.

\paragraph{Contributions:}

\begin{itemize}

\item We propose a semantics based approach to eliminate uses of deprecated APIs, which employs program synthesis.

\item We propose a greybox program synthesis technique that is driven by type information and code hints, and relies on coverage-guided property-based testing.
  
%\item We provide a technique for building the component library that is guided by code hints and types.
  
%\item We propose greybox program synthesis by encoding the code generation problem as a verification problem that can be solved by coverage-guided property-based testing.
  %% introduce the notion of coverage based CEGIS, which makes use of coverage-guided property-based testing in order to generate counterexamples for a type and example guided synthesiser.
  
%% \item We provide an encoding of the synthesis problem as the problem of verifying the safety of a program, which is then solved by fuzzing.

%% \item We introduce the concept of refactoring context. %% The refactoring context is different from the
%%   %% actual code to be refactored.
%%   Our refactoring technique generally starts 
%%   with no context and gradually increase the amount of context we take into consideration
%%   until a desirable refactoring is found.

\item We Implemented our technique in the tool ?? and used it to refactor the methods deprecated in jdk15.
  %refactor deprecated instances in X open source projects.
  
\end{itemize}  

%
%% \begin{itemize}
%% %
%% \item We present a program synthesis based refactoring procedure for Java
%% code that handles collections through external loop iteration.
%% %
%% \item We have implemented our refactoring method in the tool \tool. Our
%% experimental results support our conjecture that semantics-driven
%% refactorings are more precise and can handle more complex code scenarios
%% than syntax-driven refactorings.
%% %
%% \end{itemize}



\section{Overview of our approach} \label{sec:overview}

As mentioned in Section~\ref{sec:intro}, our approach employs program synthesis
in order to generate a refactoring that is semantically equivalent to the original code and
does not use deprecated APIs.
Essentially, the new code is obtained by composing methods
that are accessible from that respective location. 
Consequently, we make use of component-based program synthesis,
which weaves together components from a library
(typically methods from an API) in order to generate the desired program \cite{DBLP:conf/icse/JhaGST10,DBLP:conf/pldi/GulwaniJTV11,DBLP:conf/popl/FengM0DR17}.
%Component-based program synthesis allows us to 

%% While powerful, program synthesis is very hard due to the vast
%% search space. In particular,
For component-based synthesis, real-world
APIs can be very large. %, making synthesis infeasible.
For illustration, in our setting, indiscriminately adding all members
found in \code{java.lang} would make synthesis infeasible (see experimental results in Section~\ref{sec:experimental-results}).
%% In general program synthesis is very difficult. One of the main
%% challenges is the very large search space.
%% The objective of recent
%% program synthesis research has been to reduce the search space.
%% One such direction, component-based program synthesis allows
%% synthesis techniques to weave together components from a library
%% (typically methods from an API) in order to generate the desired program.
%% While this does improve the efficiency of program synthesis by providing
%% higher order building blocks that can be used..., real-world APIs are still
%% too big.
%
Previous works have used type information to make components-based synthesis
feasible in the presence of large component
libraries~\cite{DBLP:conf/popl/FengM0DR17}.  However, as shown by our
experimental results discussed in
Section~\ref{sec:experimental-results}, the use of types in our setting is insufficient
for obtaining a fast refactoring technique that is able to
provide hints to the developer on-the-fly.
%
%% For related work:
%% However, that worked well when the generated code
%% only had to be type sound and obey at most three test cases. However, in our case
%% we need to generate code that is observationally equivalent to the original.
%% While we will use coverage-guided testing to check observational equivalence (as discussed later),
%% we still check at least 400 inputs. 
%
Our solution is to use the code hints provided in the Javadoc comments to guide the building of the component library. For our running example in Figure~\ref{ex:deprecated-method-other}, the source code for the \code{getHours} method in class \code{Date} is accompanied by the comment in Figure~\ref{ex:code-hints}, where the \code{@code} tag suggests replacing the deprecated \code{getHours}
with \code{Calendar.get(Calendar.HOUR_OF_DAY)}.


\begin{figure}
\begin{lstlisting}[mathescape=true,showstringspaces=false]
/*
 * @return  the hour represented by this date.
 * @see     java.util.Calendar
 * @deprecated As of JDK version 1.1,
 * replaced by {@code Calendar.get(Calendar.HOUR_OF_DAY)}.
 */  
\end{lstlisting}
\caption{Code hints for the running example.}
\label{ex:code-hints}
\end{figure}


%While it might seem that this hint ma solve the entire problem, 
Note that this hint is not immediately usable. Some of the challenges
for this example are:

{\bf Challenge 1.}  Although it may seem as if
method \code{get} is static allowing us to make an immediate call,
it is actually an instance method, requiring us to have an object of
class \code{Calendar}. However, no such object is available in the
original code meaning that it must be created by the refactored code.
Consequently, we must populate the component library with the necessary
components to create such an object by consuming existing objects.

{\bf Challenge 2.} Adding too many components to the library will
make the synthesis task unfeasible. In particular, we should
be able to differentiate between components that we can use
(we have or are able to generate
all the necessary objects for calling them), and those
that we can't because we can't obtain some of the arguments
and/or receiver. Adding the latter to the library will
significantly slow down the synthesis process by
adding infeasible programs to the search space.

{\bf Challenge 3.} Besides adding components that allow
us to generate the required objects, we might also need
components for initialising the new objects. For instance,
in Figure~\ref{ex:deprecated-method-other}, we must call
\code{calendar.setTime(date)} to set the calendar's date
based on the existing \code{date} object.

In Section~\ref{sec:components-seeding}, we provide in-depth details about the
seeding of the component library and tackling the above challenges. 
At a high level, the process is guided by types and code hints.

Once the component library is built, for the actual synthesis,
we use a counterexample-guided iterative process that enables us
to keep refining a candidate refactoring until we are happy with it.
The refinement is based on the results provided by 
coverage-guided property-based testing~\cite{DBLP:conf/issta/PadhyeLS19}.
%% Intuitively, we wanted to avoid the overheads due to program analysis and
%% constraint solving incurred by whitebox methods, while also making use of
%% knowledge about the code that can be obtained through lightweight instrumentation.
%% For this purpose, %% we phrased the synthesis problem in a manner that makes it amenable
%% %% to solving via coverage-guided testing. Essentially,
%% we employ an iterative synthesis process based on CounterExample Guided Synthesis (CEGIS)~\ref{},
%% where the problems posed in each iteration are phrased such that they can be solved
%% by coverage-guided property-based testing.
We provide more details about %
the synthesis process in Section~\ref{sec:encoding}.

%% Whitebox approaches can be inefficient as they require fully interpreting the
%% internal structures of the code through some form of static analysis.

%% In this section, we use the example in
%% Figure~\ref{ex:deprecated-method-other} to informally illustrate our
%% technique.  Firstly, our synthesis method is component based, meaning
%% that it uses components from a given library to generate new code.  We
%% use a type-directed method for generating a specifically designed
%% component library for each refactoring. We will provide more details
%% on how to generate the component library in
%% Section~\ref{sec:components-seeding}.  In the current section, we will
%% just assume the component library containts all the instructions
%% necessary to generate the refactoring in
%% Figure~\ref{ex:deprecated-method-other}.

%% Similar to CEGIS, our synthesis process is iterative such that in each
%% iteration an inductive synthesiser generates a candidate refactoring
%% that is subsequently checked by a verifier.  
%% In general, each CEGIS
%% verification phase produces {\em one} counterexample that is added to
%% the set of inputs used by the subsequent synthesis iteration to
%% generate a better candidate.  The implicit assumption is that these
%% inputs are somehow important such that generating a candidate that
%% works for them will help efficiently prune the search space and avoid
%% enumerating all possible programs.

%% In this paper, we propose using the notion of coverage in order to
%% generate counterexamples to be added to the inputs set. Basically, in
%% each iteration of the synthesis process, the verifier will generate
%% multiple counterexample inputs that cause the current candidate to fail the property of interest by
%% exercising different paths.
%% %
%% As we generate refactorings, the property that we are interested to check is observational equivalence
%% with respect to the original code, i.e. for each input, the refactoring must generate the
%% same result as the original code (in reality, as we must handle code with side effects,
%% the refactored and original codes must generate the same program state). 

%% For our running example, the synthesiser generates the following candidate
%% refactoring in the first synthesis iteration:
 
%% \begin{lstlisting}[mathescape=true,showstringspaces=false]
%% void main(String[] args) {
%%   int hour = java.util.Calendar.getInstance().
%%              get(java.util.Calendar.HOUR_OF_DAY);
%% }
%% \end{lstlisting}

%% Compared to the original program in Figure~\ref{ex:deprecated-method-other}, this
%% candidate refactoring is not semantically equivalent as it does not make
%% use of the initial \texttt{date} variable. When passed to the verifier,
%% we obtain X counterexamples, each exercising a different path through the
%% candidate refactoring (note that, while these paths are not visible in the
%% code above, they come from the methods being called). With all these counterexamples,
%% the correct refactoring is found in the second iteration.

%% Our conjecture is that coverage-guided counterexamples help prune
%% the search space during CEGIS. In the experimental evaluation, we will
%% empirically evaluate our conjecture.

%%==========

%% Our technique relies on both syntax and semantics and makes use of advances in the
%% field of program synthesis. In particular, we make use of the general
%% structure of Counterexample Guided Inductive Synthesis (CEGIS), but
%% instead of using symbolic execution to both generate and verify
%% candidate refactorings, we use fuzzing, as described later in this
%% section.  Our synthesis method is also component based, meaning that
%% it uses components from a given library to generate new code.  In the
%% rest of this section, we describe how we build the components library
%% followed by presenting the actual refactoring synthesis.
%%Our technique for refactoring deprecated instances (e.g. fields, methods, classes) has two big steps, as described next.

%% \subsection{{\bf Input to our method}}
%% We expect as input the original deprecated code \var{Orig} with inputs
%% $\vec{x}$, as well as access to the library where the
%% field/method/class has been deprecated. We need the latter to seed the
%% components library.  Depending on the user's intention we may also
%% take into consideration some amount of context (of the location where
%% the deprecated element is found) when searching for a refactoring.

\section{{\bf Code hints and type guided seeding of the component library}}\label{sec:components-seeding}
% Our synthesis engine is component based, meaning that, in order to generate new code, it makes use of a library of components.
As mentioned in Section~\ref{sec:overview}, a critical aspect of our technique is building the
component library. 
%For each required refactoring, we start by building the components library.
%This step is critical as
%% the size of the component library has a direct impact
%% on the size of the search space for the respective refactoring.
If the component library is too small, meaning that it doesn't contain all
the required components, then synthesis will fail to find a refactoring.
Similarly, if we add too many components, then synthesis quickly becomes infeasible
due to the large search space.

Consequently, in our approach, the component library is dynamically built for each refactoring,
such that it only contains components specific to that use case.
To further increase our chances of finding a refactoring fast, we start with a restricted code  {\em core library} and incrementally increase its size until we find a refactoring.


\paragraph{The core component library}
As mentioned before, we make use of type and code hints to build the component library.
In our running example in Figure~\ref{ex:deprecated-method-other}, we can view \code{date} of type \code{Date} as the input to the code to be refactored. The objective is to weave together components that consume this object and generate the integer \code{hour}.
%% While there are many methods in the Java standard library that return an integer without consuming an object of type $Date$, our intention is to only add those that allow using the input object.

For this purpose, the starting point are the code hints provided by the Javadoc.
Intuitively, the library seeding algorithm must add components that are able to consume the
inputs to the deprecated code and produce the objects required to make use
of the Javadoc hints.
%
For our running example, the hints are given in Figure~\ref{ex:code-hints}.
These hints instruct us to add method \code{``int get(int field)''} from class
\code{Calendar} and constant \code{``Calendar.HOUR_OF_DAY''} to our library.
We actually fix the \code{``Calendar.HOUR_OF_DAY''} constant
in place, thus adding \code{``int get(Calendar.HOUR_OF_DAY)''}.

The next step is to make sure that we can use the components in the library.
For the running example, in order to be able to call method \code{get}, we require an object of
type \code{Calendar} and our library doesn't contain any way of creating such an object.
Therefore, we must seed our component library with ways of generating such an object.

We first scan all the public constructors of class \code{Calendar}
and all the public methods from class \code{Calendar} that return an object of type \code{Calendar}. We find the following four options:

\begin{enumerate}
  \item \code{static Calendar	getInstance()} -- gets a calendar using the default time zone and locale.
  \item \code{static Calendar getInstance(Locale aLocale)} -- gets a calendar using the default time zone and specified locale.
  \item \code{static Calendar	getInstance(TimeZone zone)} -- gets a calendar using the specified time zone and default locale.
  \item \code{static Calendar	getInstance(TimeZone zone, Locale aLocale)} -- gets a calendar with the specified time zone and locale.
\end{enumerate}

%% For our illustrating example, the only object used by the deprecated code is \code{Source\_Obj}=\{\code{date}\} with given type \code{Source\_Type}=\{\code{Date}\}. Using these, we need to obtain an object of type \code{Target\_Type}=\{\code{Calendar}\}, so that we can make use of the JavaDoc hint.  
%
Out of the four options above, we search for those for which we have the means to generate all their arguments and their receiver. We formally refer to such components as {\em realizable}.
In Definition~\ref{def:realizable}, we make use of the $source\_types$, referring to types
for which we can generate objects and $required\_types(instr)$, meaning the required types
for $instr$ to be realizable. In our running example, at the beginning,
$source\_types=\{Date\}$ and $required\_types(get) = \{Integer, Calendar\}$, where the Integer
corresponds to the argument and Calendar corresponds to the receiver. As for the core library,
we fix the argument \code{``Calendar.HOUR_OF_DAY''}, we actually have $required\_types(get) = \{Calendar\}$.


%% We call such an instruction {\em realizable}, according to Definition~\ref{def:realizable}.
%% In the definition, we use
%% \var{required\_types(i)} to denote
%% the set of the types of the parameters and receiver required to call instruction \var{i}.

\begin{definition}[Realizable instruction]\label{def:realizable}
Instruction \var{i} is {\em realizable} iff $\forall t \in required\_types(i). t \in source\_types$.
  
\end{definition}

Regarding the four methods in our example, the first one is realizable. Conversely, in order to call the second method, we would need to generate an object of type \code{Locale},
for which purpose we don't have an instruction in the library.
The last two methods are in a similar situation as the second. For the core library, we only add the first method.

%% Even though we add them, they are not going to be used by the synthesis engine
%% as we use a type-directed approach to term generation...

The seeding algorithm for the core library is provided in Figure~\ref{alg:seeding-core}.
We start by adding all the constants contained in the original code to the library
(those from the code hints are fixed in place).
Then, we also add all the instructions contained
in the code hints. Next, we need to make sure that the Javadoc suggestions
are realizable. For this purpose, we add ways of generating
objects whose types match the types of the required fields and receivers.
%
Essentially, for each required type that we have no way of generating, we add all the realizable
constructors and realizable methods returning objects of that type to the
core library.
Notably, it may be the case that there is no realizable instruction for a required type.
In such a case, the synthesiser won't be able to generate any useful programs.
%More details on this in Section~\ref{sec:synthesis}.

Besides ways of generating objects of certain types, we also need to be able to
modify the newly generated objects. This is covered in Figure~\ref{alg:seeding-core}
by adding $modifiers$ to the library. Notably, all the components that
we add must be accessible from the current location.

Once we added the components required to generate an object of a given type,
we add that type to $source\_types$.
%% we overload \var{ensured\_types} to take \var{library} as an argument, in which case it returns the set of all the types of the objects that can be created using the instructions in the library.

%% In certain situations, the seeding of the component library may require additional work.
%% For instance, for the example in Figure~\ref{ex:deprecated-method-other}, we need to also provide a way of obtaining
%% a \code{Calendar} object. Thus, we add \code{Calendar.getInstance()}. 

\SetKwInOut{KwIn}{Input}
\SetKwInOut{KwOut}{Output}

\begin{figure}
\removelatexerror% Nullify \@latex@error
\begin{algorithm}[H]
\SetAlgoLined
\KwOut{Core component library}
 Add constants from the original code to the library\;
 Add instructions from the code hints to the library\;
 \For{instr $\in$ library}{
   \If{$\neg$realizable(instr)}{
     \For{$type {\in} required\_types(instr) ~\&\&~ type {\not\in} source\_types$}{
       $generators = realizable\_constructors(type) \cup realizable\_methods\_returning\_type(type)$\;
       $modifiers = realizable\_methods\_with\_receiver\_of\_type(type)$\;
       Add $generators$ to the library\;
       Add $modifiers$ to the library\;
       \If{$generators \neq \emptyset$}{$source\_types = source\_types \cup\{type\} $}
     }
   }
 }
\end{algorithm}
 \caption{Seeding algorithm for the core library}
\label{alg:seeding-core}
\end{figure}

\paragraph{Incremental expansion of the library}
If the synthesis engine fails to find a refactoring within a certain amount of time,
there might be a need to increase the components library.
For the core library, in the algorithm in Figure~\ref{alg:seeding-core}, we only ever add realizable components. However, this requirement can be relaxed by adding components that
are not yet realizable, but would become so if we were to add one
additional instruction (the generator for the missing type). For our current experiments,
there was no need to expand the core library. \todo{check this.}

%% we start to incrementally increase the size of the library by adding additional components to be used during synthesis.
%% The library expansion algorithm is provided in Figure~\ref{alg:seeding-extended}.
%% Essentially, we go through all the instructions in the current library and add
%% ways of generating objects that can be used as their arguments (i.e. whose types match that of their arguments).
%% Note that, as opposed to the core library, here we look at all instructions, not just the realizable ones.


%%\begin{figure}
%% \begin{algorithm}[H]
%% \SetAlgoLined
%% \KwIn{Current library}
%% \KwOut{Expanded component library}
%%  \For{instr $\in$ library}{
%%      \For{$type {\in} required\_types(instr)$}{
%%        $generators = realizable\_public\_constructors(type) \cup realizable\_public\_methods\_returning\_object\_of\_type(type)$\;
%%        Add $generators$ to the library\;
%%      }
%%  }
%% \end{algorithm}
%% \caption{Incremental expansion of the component library}
%%\label{alg:seeding-extended}
%%\end{figure}

%% As future work we aim to also collect components and sketches from the  unit tests for the updated library.
 
\section{Synthesising the refactoring}\label{sec:encoding}

We make use of a CounterExample Guided Inductive Synthesis (CEGIS)~\cite{DBLP:conf/pldi/Solar-LezamaJB08} architecture, where we
iteratively improve a candidate refactoring until it obeys a given specification.
In our case, the specification expresses the fact that the original and
the refactored blocks of code need to be observationally equivalent.
%
In the rest of the paper, we will use the predicate
$equivalent(P_1(\vec{i}), P_2(\vec{i}))$ to denote that code $P_1$ is
observationally equivalent to $P_2$ for inputs $\vec{i}$.  In general,
we refer to the original code as $P_1$ and the refactored code as
$P_2$. We will fully define what $equivalent$ actually means in Section~\ref{sec:equiv}.


In each iteration of the synthesis process, there are two phases, a synthesis and a verification one. We next show how we encode the problem corresponding to each of these phases so that it can be solved with coverage-guided property-based testing. As we want to start with a high level view


\paragraph{The synthesis phase} In this phase, we have 
a finite set of input examples $\{\vec{i_1} \cdots \vec{i_n}\}$ and we must find a candidate refactoring
that is observationally equivalent to the original code for the given inputs.
For this purpose, we construct the following \code{Synthesise} method, which
takes the refactored code $P_2$ as input.

\begin{lstlisting}[mathescape=true,showstringspaces=false]
Synthesise ($P_2$) {

  if (equivalent($P_1$($\vec{i_1}$), $P_2$($\vec{i_1}$)) && ...
      && equivalent($P_1$($\vec{i_n}$), $P_2$($\vec{i_n}$)))
    assert(false);
}
\end{lstlisting}

This method consists of only one conditional saying that if $P_1$ and $P_2$ are equivalent on all given inputs $\vec{i_1}, \cdots, \vec{i_n}$, then \code{assert(false)} is reached.
Given that this assertion always fails, it means that the method is unsafe when $P_1$ and $P_2$ are equivalent on the given inputs.
Thus, we can reduce the synthesis problem to the problem of checking the safety of \code{Synthesise}. If a safety checker manages to
find an input $P_2$ for which the assertion fails (meaning that \code{Synthesise} is unsafe), then this $P_2$ must be equivalent to $P_1$ on the given inputs. This $P_2$ is the code
that the synthesis phase is supposed to generate. As aforementioned, we use an existing coverage-guided property-based testing (i.e. fuzzing) technique
as the safety checker. We configure the fuzzer such that it constructs $P_2$ by weaving together components from the component library.

If the safety checker fails to find a $P_2$ for which the assertion fails, then the overall synthesis technique fails to generate a solution refactoring.
There could be two reasons for this situation, which we cannot differentiate between.
Firstly, there may indeed be no $P_2$ that can be constructed with the available components in the component library such that
it is equivalent to $P_1$ on the given inputs. Secondly, this could be caused by the fuzzer's unsoundness.
Given that fuzzers rely on testing, they may fail to find inputs that trigger unsafe behaviours even when such inputs exist.
Consequently, we cannot guarantee that we will always find a refactoring whenever one exists. We will provide more details about our decision to use fuzzing as the safety checker in 
Section~\ref{sec:fuzzing}.

%% In other words, answering the initial question posed by the
%% synthesis phase is now reduced to checking the safety of the \code{Synthesise} method.
%% If the method is unsafe meaning that the assertion is reached and failed (if reached, the assertion \code{assert(false)} always fails)

%% Again, we reduce the problem to be solved in the synthesis phase to
%% the problem of checking the safety of the \code{Synthesise} method,
%% which we do by fuzzing.
%% The fuzzer will find an input \var{Candidate} that fails the assertion,
%% meaning that it returns the same output as \texttt{Orig}
%% on the finite set of inputs \var{x_1, \cdots, x_n}.


%% iterative process that enables us to keep refining a candidate refactoring until we are happy with it. This process consists of two phases, described next:
%%   \begin{itemize}
%%   \item[{\bf Phase 1:}] Given a fixed set of inputs $In$, find a refactoring that works for $In$. For this purpose, we encode   
%%     the synthesis problem as a verification problem that can be solved by fuzzing. We discuss this problem encoding in
%%     Section~\ref{sec:encoding}. The found refactoring is called a candidate.
%%   \item[{\bf Phase 2:}] Given the candidate synthesised at the previous step, verify whether it is indeed a correct refactoring.
%%     Again, we answer this question by encoding it as a software verification problem to be solved by fuzzing (see Section~\ref{sec:encoding}).
%%     If an input for which the candidate isn't observationally equivalent to the original code is found, then
%%     we return to the previous step and add this input to the set $In$. Otherwise we have found a correct refactoring and we are done.
%%   \end{itemize}


\paragraph{The verification phase} For the verification phase, we are provided with a candidate
refactoring $P_2$ and we must check whether there exists any input
$\vec{i}$ for which the original code and the candidate
refactoring are not observationally equivalent.  To do this, we build
the following \code{Verify} method, which given some input
$\vec{i}$, asserts that the two programs are equivalent for
$\vec{i}$.

\begin{lstlisting}[mathescape=true,showstringspaces=false]
Verify($\vec{i}$) {
  assert(equivalent($P_1(\vec{i})$, $P_2(\vec{i})$));
}
\end{lstlisting}

In other words, answering the initial question posed by the verification
phase is reduced to checking the safety of this method: if
\code{Verify} is safe (i.e. the assertion is not violated) for any
input $\vec{i}$, then there is no input that can distinguish
between the original and the candidate refactoring (this
is indeed a sound refactoring). However, if \code{Verify} is not safe,
then we want to be able to obtain a counterexample input
$\vec{i_{cex}}$ for which the assertion fails.

This counterexample will be provided back to the synthesis phase and
used to refine the current candidate refactoring. Again, we check the
safety of \code{Verify} with coverage-guided property-based testing,
which means that we may fail to find a distinguishing input when one
does exist.


%% notion of program equivalence.  While this is undecidable in
%% general, our synthesis procedure in the verification phase this problem is restricted to
%% checking that \var{Orig} and \var{Candidate} are equivalent for a .


%% The fuzzer will find an input for which the original and refactored code
%% produce a different result.



%% - Performing the synthesis. Complex bit: synthesising method invocations for methods that contain loops.

%% \begin{enumerate}
%% \item We seed the templates/snippets for the synthesis library using the following two strategies:
%%   \begin{itemize}
%% \item Use deprecated block messages.
%% \item Use the unit tests for the new library. As opposed to the comments in libraries,
%%   the unit tests are syntactically valid.
%%   \end{itemize}

%% \item Use the seeded templates to generate the refactoring.
%%   \begin{itemize}
%% \item Look at checking equivalence in the presence of loops.
%%   \end{itemize}
%%   \end{enumerate}

\subsection{Checking program equivalence for a certain input}\label{sec:equiv}

A core part of the synthesis procedure is the $equivalent(P_1(\vec{i}), P_2(\vec{i}))$ predicate,
which checks that the original code $P_1$ and a candidate refactoring $P_2$
are equivalent for a given input $\vec{i}$.
%% Naively, this would be checked by comparing the results returned by the
%% \var{Orig} and \var{Candidate}. However, that is not enough as
%% we must also consider the heap representation. We will discuss this in
%% Section~\ref{sec:equiv}.
%% In Section~\ref{sec:encoding}, we use \var{equivalent(Orig(x),
%%   Candidate(x))} to denote equivalence checking for \var{Orig} and
%% \var{Candidate} on concrete input \var{x}.
In this section, we provide details on how we check this equivalence.


%% First, this function needs
%% to check whether for input \var{x}, the outputs of the original and
%% candidate refactored programs are the same.  Moreover, any realistic
%% Java program makes modifications to the heap. Consequently,
%% \var{equivalent} must check that heap is being handled in the same
%% manner in both programs.


We start by introducing some notation:
\begin{itemize}
\item \var{loadedClasses(P)} returns the set of classes loaded by the class loader
  in which \var{P} is executed.
\item \var{liveVars(P)} provides the set of variables that are live
at the end of $P$. %% Thus, \var{liveVars(P_1)} gives us the
%% variables that have been updated in the original code \var{P_1} and are
%% live at the end of \var{P_1}.
\item \var{staticFields(C)} returns
  the set of static fields of a class \var{C}.
\item $exc(P, \vec{i})$ returns the exception thrown by $P$'s execution on input $\vec{i}$.  
\end{itemize}  

\begin{example}\label{ex:defs}
  For our running example in Figure~\ref{ex:deprecated-method-other},
  \var{P_1} and \var{P_2} are represented by the following lines of code. Note that
  both $P_1$ and $P_2$ take variable $date$ as input.

\begin{lstlisting}[mathescape=true,showstringspaces=false]
  // $P_1$:
  int hour = date.getHours();

  // $P_2$:
  final Calendar calendar = Calendar.getInstance();
  calendar.setTime(date);
  int hour = calendar.get(Calendar.HOUR_OF_DAY);
\end{lstlisting}
%
Then, we have:
\[
\begin{aligned}[t]
  \var{liveVars(P_1)} &= \{date, hour\}\\
  \var{liveVars(P_2)} &= \{calendar, date, hour\}\\  
  \var{loadedClasses(P_1)} &= \{Date\} \\
  \var{loadedClasses(P_2)} &= \{Calendar, Date\} \\  
  \var{staticFields(Date)} &= \emptyset\\
  \var{staticFields(Calendar)} &= \{DATE, YEAR, \cdots\}\\
  \var{exc(P_1, \_)}=\var{exc(P_2, \_)} &=\emptyset
\end{aligned}
\]
\end{example}

%Note $exc$ returns $\emptyset$ regardless of the value of $date$.

In order to extract the (last) object assigned to a variable \var{v} by the execution of \var{P} on a specific input $\vec{i}$,
we will use the notation $E[P(\vec{i})](v)$. Essentially, if we consider the trace generated by executing $P(\vec{i})$,
then $E[P(\vec{i})]$ maps each variable defined in \var{P} to the latest object assigned to it by this trace.
Next, we define the notion of equivalence with respect to a concrete input $\vec{i}$ (this definition is incomplete and we will build up on it in the rest of the section).

\begin{definition}[Program equivalence with respect to a concrete input $\vec{i}$ {\bf[partial]}]\label{def:prog-equiv}
  Given two code blocks \var{P_1} and \var{P_2} and concrete input $\vec{i}$,
  we say that \var{P_1} and \var{P_2} are equivalent
  with respect to $\vec{i}$, written as $equivalent(P_1(\vec{i}), P_2(\vec{i}))$
  if and only if the following conditions hold:

\[
\begin{aligned}
  & (1) exc(P_1, \vec{i})=\{e_1\} \wedge exc(P_2, \vec{i})=\{e_2\} \wedge equals(e_1,e_2) ~\vee\\
  & \qquad exc(P_1, \vec{i})=exc(P_2, \vec{i}) =\emptyset \\ 
      & (2)~ \forall v\in liveVar(P_1). \\
      & \qquad v\in liveVar(P_2) \wedge equals(E[P_1(\vec{i})](v), E[P_2(\vec{i})](v))\\
%& (2) equivalent(aliasEquivClass(liveVar(P_1)), aliasEquivClass(liveVar(P_2)))      
& (3)~  \forall C \in loadedClasses(P_1). C \in loadedClasses(P_2)~ \wedge \\
  &  \qquad \forall f {\in} staticFields(C).equals(E[P_1(\vec{i})](f), E[P_2(\vec{i})](f))
    %%   throws an exception and   $equiv(exception(R_1), exception(R_1))$. 
  %% \item \var{R_1} doesn't throw an exception and $equiv(R_1,R_2)$ and
   %% $\forall f_i \in getStatic(class(R_1)) \wedge g_i \in getStatic(class(R_2)). equiv(f_i,g_i)$.
    \end{aligned}
    \]
   
  \end{definition}

The above definition says that in order for \var{P_1} and
\var{P_2} to be equivalent with respect to input $\vec{i}$,
(1) either they both throw an exception and the two exceptions have the same type, or none does,
(2) any variable
that is live at the end of \var{P_1}, must also be live at the end of \var{P_2}, and  must be
assigned equal objects by the executions of \var{P_1} and
\var{P_2} on $\vec{i}$, respectively, and
(3) the static fields of the loaded classes must be respectively equal.
We check equality by recursively following attribute chains until we reach primitive types. We
generally do not consider \code{equals} methods unless (1) the method was not written by the user
(i.e. JCL classes), (2) the type inerits from \code{java.lang.Object}, (3) the class implements
\code{java.lang.Comparable} and (4) the type declares an \code{equals} implementation. Examples
of classes which satisfy these strict requirements are \code{java.lang.Integer} or
\code{java.util.Date}. All other \code{equals} implementations are considered unreliable and
ignored.
%Alternatively, if there is an explicit \code{equals} implemented for a given type, then we use it.
%% By equal objects we mean objects where the values of the
%% corresponding attributes are equal.

Additionally, for any class that
is being loaded by the class loader of \var{P_1}, it must also be loaded by the class loader of \var{P_2}, and
all its static fields must again be assigned equal objects by the
two executions, respectively. 
In our implementation, if either the deprecated or the refactored code didn't load a class, we load it for it,
and check that the initial state of that class is the same for both blocks of code.

%% Note that, as mentioned in its name, Definition~\ref{def:prog-equiv} only partially
%% defines the equivalence of \var{P_1} and \var{P_2} on input \var{\vec{x}}. For now
%% we will ignore this and get back to it later in the section.


%In our experiments, given that we only refactor deprecated methods, the only live variable at the end of the original invocation is \var{this}.

\begin{example}\label{ex:equiv}
  When applying Definition~\ref{def:prog-equiv} to \var{P_1} and \var{P_2} given in Example~\ref{ex:defs},
  the following conditions hold (given that class \var{Date} has no static fields, the third condition is trivial):
\[
\begin{aligned}
      & (1)~ \var{exc(P_1)}=\var{exc(P_2)} =\emptyset\\
      & (2)~ equals(E[P_1(\vec{i})](hour), E[P_2(\vec{i})](hour)) ~ \wedge\\ %date\in \{date,calendar\} ~ \wedge \\
      & \qquad equals(E[P_1(\vec{i})](date), E[P_2(\vec{i})](date))\\
& (3)~  Date \in \{Date, Calendar\}
    %%   throws an exception and   $equiv(exception(R_1), exception(R_1))$. 
  %% \item \var{R_1} doesn't throw an exception and $equiv(R_1,R_2)$ and
   %% $\forall f_i \in getStatic(class(R_1)) \wedge g_i \in getStatic(class(R_2)). equiv(f_i,g_i)$.
    \end{aligned}
    \]

 
    %% Then, in order for the two portions of code to be
    %% equivalent with respect to an input \var{x}, the objects assigned to variable \var{date} by the execution of  \var{P_1(x)} and \var{P_2(x)}
    %% must be equivalent. We will discuss checking equivalence of two objects in two distinct executions next.
    
\end{example}

%%\todo{define $equivalent(E[P_1(x)](v), E[P_2(x)](v))$}.

\paragraph{Preserving aliasing}

When expressing the equivalence relation between \var{P_1} and \var{P_2}, we intentionally missed one important aspect,
namely aliasing. To understand the problem let's look a the following example, which makes use of java.awt.Container,
where a generic Abstract Window Toolkit (AWT) container object is a component that can contain other AWT components.
Method \code{preferredSize} used by the original code below returns the preferred size of the calling container \code{container}.
However, \code{preferredSize} is deprecated, and, instead, the refactored version uses \code{getPreferredSize}.

%, which denotes a slightly
%modified version of the running example.

\begin{example}\label{ex:aliasing}
  \begin{lstlisting}[mathescape=true,showstringspaces=false]
    
  // Original code:
  Dimension dim1 = container.preferredSize();
  Dimension dim2 = container.preferredSize();    

  // Refactored code:
  Dimension dim1 = container.getpreferredSize();  
  Dimension dim2 = dim1;
\end{lstlisting}
%\end{example}


%\todo{Get another example from Pascal, where the return is an object.}

%% \begin{example}\label{ex:aliasing}
%%   \begin{lstlisting}[mathescape=true,showstringspaces=false]
    
%%   // assume date refers to 2022-12-01 13:30:00    

%%   // $P_3$:
%%   int hour1 = date.getHours();
%%   int hour2 = date.getHours();    

%%   // $P_4$:
%%   final Calendar calendar = Calendar.getInstance();
%%   calendar.setTime(date);
%%   int hour1 = calendar.get(Calendar.HOUR_OF_DAY);
%%   int hour2 = hour1;
%% \end{lstlisting}

%% For the code above, we will refer to the first two lines denoting the
%% original deprecated code as \var{P_3}, and the rest, denoting the
%% refactored code, as \var{P_4}.

We note that, the original code above defines two variables \var{dim1} and
\var{dim2}, each assigned an object of type \texttt{Dimension} returned by calling
\var{container.preferredSize()}.
Conversely, in the refactored code, \var{dim1} and \var{dim2} are aliases, i.e. they point to the same
object of type \code{Dimension}.

The original and the refactored code are equivalent according to Definition~\ref{def:prog-equiv}.
Let's next assume that the following code
is used after both the original and the refactored code, respectively.

\begin{lstlisting}[mathescape=true,showstringspaces=false]
  dim1.setSize(1,2);
  dim2.setSize(2,3);  
\end{lstlisting}

Our intention is to execute both the original and the refactored code, respectively, plus the additional two lines.
If the original and the refactored code were indeed equivalent,
%with respect to input \var{d},
then we would expect \var{dim1} and \var{dim2} to have the same value at the end of both executions. 
%executions, respectively.
However, this is not the case.
In the original code, \var{dim1} will have width 1 and height 2, whereas in the refactored code,
\var{dim1} will have width 2 and height 3. This is due to the fact that,
in the refactored code, \var{dim1} and \var{dim2} are aliases. %% Then, when the object referenced by
%% \var{hour1} was changed to refer to the date denoting ``30/10/2020'', this also affected \var{date2}.
Thus, when \var{dim2} has its size set to (2,3), this also affects the object \code{dim1} is referencing.
%\var{P_3} and \var{P_4} are not equivalent for input \var{d}.
\end{example}  

Intuitively, any aliases between live variables at the end of the original code should also
be present at the end of the refactored code. For this purpose, we use the notation
\var{aliasEquivClass(V)} which returns the set of all equivalence classes induced over the
set of variables \var{V} by the aliasing relation.  
Notably, the aliasing equivalence relation must also hold over the
static fields of the classes loaded by both programs.


\begin{example}
For \var{P_1} and \var{P_2}, there are no aliases, therefore:
\[
\begin{aligned}[t]
  \var{aliasEquivClass(liveVar(P_1) \cup staticFields(loadedClasses(P_1))} &= \emptyset\\
  \var{aliasEquivClass(liveVar(P_2) \cup staticFields(loadedClasses(P_2))} &= \emptyset
\end{aligned}
\]
%
However, for \var{P_3} and \var{P_4} we have:
\[
\begin{aligned}[t]
  \var{aliasEquivClass(liveVar(P_3) \cup staticFields(loadedClasses(P_3))} &= \emptyset\\
  \var{aliasEquivClass(liveVar(P_4) \cup staticFields(loadedClasses(P_4))} &= \\
  \qquad \{\{date1, date2\}\} &
\end{aligned}
\]
In \var{P_4}, the alising relation induces one equivalence class, namely \var{\{date1, date2\}}.

\end{example}
  
Next, we complete Definition~\ref{def:prog-equiv} by capturing the aliasing aspect.

\begin{definition}[Addition to Definition~\ref{def:prog-equiv}]\label{def:prog-equiv-add}
   In addition to Definition~\ref{def:prog-equiv},
  two programs \var{P_1} and \var{P_2} are equivalent
  with respect to $\vec{i}$ iff:
\[
    \begin{aligned}
      & (4)~ \forall v_1,v_2 \in liveVar(P_1) \cup staticFields(loadedClasses(P_1)). \\
      & \qquad aliases(P_1, v_1, v_2) \Rightarrow aliases(P_2, v_1,v_2)      
    %% & (5)~\forall f_1,f_2 \in staticFields(loadedClasses(P_1)).\\
    %%   & \qquad aliases(P_1, f_1, f_2) \Rightarrow aliases(P_2, f_1,f_2)
    %%   throws an exception and   $equiv(exception(R_1), exception(R_1))$. 
  %% \item \var{R_1} doesn't throw an exception and $equiv(R_1,R_2)$ and
   %% $\forall f_i \in getStatic(class(R_1)) \wedge g_i \in getStatic(class(R_2)). equiv(f_i,g_i)$.
    \end{aligned}
    \]
   
  \end{definition}

where, given a program \var{P}, variables \var{v_1} and \var{v_2} are aliases, i.e. \var{aliases(P, v_1, v_2)} holds,
if and only if they are in the same equivalence class induced by the aliasing relation,\\
  i.e. $aliasEquivClass(\{v_1,v_2\}) = \{\{v_1,v_2\}\}$.

  We abuse the notation to use \var{staticFields} over a set of classes, rather than just one class.
  The objective is to return the set of all static fields defined in all the classes in the set of classes taken as argument.
  

%\todo{also add an example where static fields are changed.}


%% Aliasing check: checks whether the two given objects have the same aliasing ID, and thus
%%    alias with the same objects on their respective heaps.

%%    What do you assign id's to? I'm trying to figure out whether this is too strict?
%%    Basically, equivalence classes induced by the aliasing relation need to be preserved,
%%    but they may not have the same representative ID.

%% You compare execution results:
%% - do you just look at the returned object?

%% Equivalent execution results
%% - returned value
%% - static fields of the corresponding classes

%% Equivalent objects
%% - ObjectIdComparator
%% - IsAliasingEquivalent

%% State: static fields of all classes, stack vars, follow object pointers, thread locals?, equivalence classes induced by the aliasing relation must be preserved
%% - touch the same vars
%% - all touched vars correspond to equivalent objects
%% - 2 graphs are isomorphic

%% 2 obj are equal if all their fields are equal
%% 2 primitive types are equal if they are the same value

%% ============

%% a1 = new Integer(1);
%% b1 = new Integer(1);

%% vs

%% a2 = new Integer(1);
%% b2 = a2;



%% A class is initialized when a symbol in the class is first used. When
%% a class is loaded it is not initialized.


%% Discuss different types of fuzzing and the one that we use.
%% \todo{Double check with Pascal what he uses.}

%% \section{Algorithm}

%% \todo{Write description of the algorithm}

%% \subsection{Reduction of semantically equivalent programs}

%% \subsection{Guarantees} \label{sec:guarantees}

%% \paragraph{Discussion on soundness} Given that we rely on fuzzing, we can't guarantee that the
%% refactoring works for all possible inputs. However, it is well suited
%% to the particular scenarios for which we designed our technique,
%% IDE integration where the refactoring is presented as a suggestion to the programmer,
%% or automated pull requests to be approved by the developers.

%% In order to fully guarantee the soundness of the refactorings proposed
%% by our technique, we would have to use a sound verification technique
%% in the verification phase mentioned earlier. However, such a technique
%% would require symbolic execution, which is notoriously difficult for
%% real-world programs due to features such as loops, library functions,
%% complex data structures. The most successful such verification
%% techniques are either bounded (i.e. check that an erroneous state
%% cannot be reached within a bounded number of loop unrollings) or
%% require user input in the form of loop invariants. While the former
%% class of techniques provides limited soundness guarantees, the latter
%% requires intricate help from the user.  In fact, we are not aware of
%% any sound and automatic symbolic verification method that can handle
%% the kind of code we are interested in.

%% Discuss tradeoff: symbolic execution is expensive; programming
%% languages evolve faster than verification techniques and therefore
%% many new features are not supported by their front ends (and/or
%% backends depending on how complex the new feature is); even after
%% translating the code into a logical formula, calls to solvers are
%% expensive.

%% \paragraph{Discussion on completeness}
%% We cannot guarantee that we'll find a refactoring whenever one exists.
%% The fuzzer in the synthesis phase may fail to find a candidate.

%% \section{Some research questions}

%% What if the new method doesn't maintain observational equivalence?
%% TODO: Check existing projects to see how many do preserve equivalence.
%% For the rest, is there a notion of partial equivalence that ensures the equivalence
%% for the new projects minus the differences introduced by the new function.

%% How easy is for the fuzzer to handle such structured inputs as we need in the
%% synthesis phase?


%% \subsection{For the experimental evaluation}
%% \begin{itemize}
%% \item Pick libraries that have deprecated instances.
%% \item Pick users of these libraries (i.e. open source projects that are actively maintained).
%% \item Make pull requests and see how many get merged.
%% \end{itemize}  

%% \section{Context-sensitive vs. context-insensitive refactoring}

%% \paragraph{Context-insensitive refactoring}
%% We consider different refactoring options. The first and most simple
%% one is the so called ``context-insensitive'' refactoring, where we
%% find a refactoring for the deprecated code that works irrespective of
%% context. This means that, once such a refactoring is found, it can be
%% used to replace all the uses of the deprecated code, e.g. a deprecated
%% method invocation will be replaced with the same refactored code
%% irrespective of the actual parameters or rest of the state at the call
%% site.

%% For instance, if we want to refactor the call to method
%% \texttt{toRefactor} in the code below, then we search for a refactoring that is
%% observationally equivalent to the given method irrespective of the
%% context of the call.

%% \begin{lstlisting}[mathescape=true]
%% class NoContext {
%%   int x;

%%   public double toRefactor(int y, int z) {
%%     return (double) x + y + z;
%%   }
%% }
%% \end{lstlisting}
 
%% This means that the state that we can vary in order to check for
%% observational equivalence includes \texttt{x, y} and \texttt{z}.

%% \paragraph{Context-sensitive refactoring}
%% The refactoring context consists of the context where the code to be refactored
%% is placed. For instance, in the snippet below, if we are trying to refactor the call to the deprecated
%% function \texttt{f}, then the context gives us the valuation $[\texttt{x}\mapsto 1]$ for the function's argument. 

%% \begin{lstlisting}[mathescape=true]
%%   int x = 1;
%%   A a = new A();
  
%%   a.f(1)
%% \end{lstlisting}
    
%% We now look at the possibility of considering the refactoring context
%% when searching for a refactoring. This gives us the option of starting
%% with very little context and gradually increasing it until finding a
%% refactoring. \todo{Adjust this based on what the experiments do.}

%% The other option is to find a refactoring that takes into consideration
%% the calling context. For instance, in the code below we have two
%% calls to \texttt{toRefactor}, one in method \texttt{foo} and one in
%% method \texttt{bar}. When considering the calling context, we have to
%% find two different refactorings, one for each of the method calls.
%% For the call in \texttt{foo}, the state that we need to consider for observational
%% equivalence contains \texttt{ctx} and \texttt{a}, whereas for the call in method
%% \texttt{bar} it contains \texttt{ctx, a, b, c, d} and \texttt{e}.

%% \begin{lstlisting}[mathescape=true]
%%   class Context1 {
%%     NoContext ctx = new NoContext();

%%     public double foo(int a) {
%%       return ctx.toRefactor(a, a);
%%     }
%%   }

 

%%   class Context2 {
%%     NoContext ctx = new NoContext();

%%     int d;
%%     int e;

%%     public double bar(int a, int b, int c) {
%%       return ctx.toRefactor(a, b + c + d + e);
%%     }
%%   }
%% \end{lstlisting}


%% Fuzzing:

%% someNoContext.x = x_*;

%% someNoContext.toRefactor(y_*, z_*);

%% Possible follow-up:
%% - if we have a sizeable codebase, the user might have to do 1,2 deprecated
%% refactorings and then the synthesisers can use the PRs done by the user
%% to seed the synthesiser and do the other thousands of refactorings
%% automatically.

  \section{Implementation}

  While in Section~\ref{sec:overview} we presented our technique from a theoretical point of view,   in this section we discuss practical aspects of the actual implementation.  

\subsection{Using program fuzzing for synthesis} \label{sec:fuzzing}

In order to generate programs, we make use of JQF~\cite{DBLP:conf/issta/PadhyeLS19}, which performs 
coverage-guided property-based fuzz testing.
Coverage-guided means that JQF uses lightweight program
instrumentation to trace the code coverage reached by each generated
input that is fed to the program under test.  Basically, the program
is continuously executed with randomly generated inputs. However,
instead of generating inputs from scratch, coverage-guided fuzzers
evolve a set of saved inputs. If a new input leads to an increase
in code coverage, it is saved for subsequent evolution.

In our setting, we use JQF for two tasks. Firstly, in the synthesis phase, 
JQF is generating the refactored block of code.
Secondly, in the verification phase, JQF generates
inputs that can differentiate between the original and refactored codes.
The former task in particular is a difficult setting for fuzzing because programs are
highly structured, whereas fuzzers often work with unstructured inputs
that can be generated via byte-level mutations such as bit flips.

JQF is designed to handle structured inputs, where inputs of type \code{T}
are generated by a backing \code{Generator<T>}. JQF provides a library of
generators for basic types such as primitive values. We implement custom
JQF generators for the synthesis and verification phase of the CEGIS loop. In
the case of the former the generator produces candidate programs, in the latter
it produces counterexample inputs %heaps
for candidate verification.

\subsection{Equivalence reduction}
JQF never generates the same exact program twice. However it may generate many
syntactically different, but semantically equivalent ones.
Program synthesis techniques make use of equivalence reduction in order to
reduce the number of equivalent programs that get explored.
For example, \citet{DBLP:conf/cav/AlbarghouthiGK13} 
prune the search space using observational equivalence with respect to a set of input/output examples, i.e., two programs are considered to be in the same equivalence class if, for all given inputs in the set of input/output examples, they produce the same outputs. Alternatively, \citet{DBLP:conf/vmcai/SmithA19} generate only programs in a specific normal form, where term rewriting is used to transform a program into its normal form. In~\cite{DBLP:journals/corr/KoukoutosKK16}, Koukoutos et al. make use of attribute grammars to only produce certain types of expressions in their normal form, thus skipping other expressions
that are syntactically different, yet semantically equivalent.
While we plan on investigating some of these directions as part of future work,
given the restricted set of instructions in the core library,
our current technique is still effective (as shown by the experimental evaluation).

%% We plan on addressing this 
%% This may be addressed
%% in future work by explicitly restricting redundancy in our instruciton set.
%% Since the instruction sets in our experiments are already restricted by type
%% information and other hints, this effect did not appear to affect the
%% effectiveness of our algorithm.
%% Next, we discuss a few techniques that we use to reduce the number of
%% generated programs that are semantically equivalent.


\subsection{Checking aliasing}

In Section~\ref{sec:equiv}, we discussed about checking program equivalence with respect to a concrete input. As part of this, 
we must check that the refactored code preserves the aliases present in the original code (see Definition~\ref{def:prog-equiv-add}).

%% In our implementation we enforce a strict notion of aliasing. In
%% particular, every reference variable encountered in the original and
%% refactored code is assigned a strictly increasing symbolic id
%% $sid(v)$. We start from the same initial symbolic id in the original
%% and refactored code.  Then, every live variable at the end of the
%% original code (which according to Definition~\ref{} is also live at
%% the end of the refactored code) must be assigned the same symbolic id
%% by the original code and the refactored one.

%% Notably, this is stronger than needed as it means that variables are declared in the same order by the
%% original and the refactored program
%% Then, two variables $v_1$ and $v_2$ in $P$ (where $P$ is either the original or the refactored code)
%% are aliases if 

%\todo{Discuss this.}
Intuitively, when checking preservation of aliases, we want to preserve the equivalence classes induced by the aliasing relation.
For simplicity, in our implementation, we enforce a stricter check than necessary.
In particular, all the objects
%(memory locations)
that are being referenced during the code's execution
are assigned increasing symbolic identifiers.
Then, we enforce aliasing preservation (condition (4) in Definition~\ref{def:prog-equiv-add}),
by checking that, for all variables defined by both the original and the refactored code,
the objects they reference have the same symbolic id.

While this is a much stronger requirement than needed, it was sufficient for our experiments.
For future work, we plan on relaxing it and explicitly tracking the equivalence classes induced by the aliasing
relation.

\todo{Refer to the new example obtained from Pascal.}

%% heap equivalence, we enforce a strict notion of aliasing
%% equivalence.
%% While exploring the heaps and comparing
%% them for value equivalence, every pointer encountered is assigned
%% a strictly increasing symbolic id $sid(v)$.
%% Two heaps $H_A$, $H_B$ are only considered equivalent if
%% $\mathop \forall \limits_{i=1}^{n} sid(v^A_i)=sid(v^B_i)$. This is a strong
%% under-approximation and may be subject to improvement in future work, but was
%% sufficient for the scope of our experiments.

%% Then, we can check whether the set of aliases is preserved in the
%% refactored program. For instance, if \texttt{obj1} and \texttt{obj2}
%% are aliases in the original program, then they must be aliases also
%% in the second program.
%\todo{check exactly what this is used for.}

\subsection{Sources of unsoundness and incompleteness}
As explained in Section~\ref{sec:encoding}, due to the use of fuzzing in the synthesis
phase, our technique may fail to find a refactoring even when one
exists (i.e. it is {\em incomplete}), whereas using fuzzing in the verification phase may result in finding a refactoring that is not equivalent to the original on all
inputs (i.e. our technique is {\em unsond}).

Another potential source of unsoundness is I/O,
which is not part of the JVM state. We currently don't check
that the original and refactored code perform the same I/O.
%the possibility that the static initialiser performs some I/O,. 

Furthermore, our algorithm does currently not explore the entire classpath for
potential overrides of the method to refactor. This may lead to unsound results
when different subclasses of the original class are used polymorphically in the
refactored context.

Finally, as described in Sec.~\ref{sec:equiv} we rely in some cases on \code{equals}
implementations for equivalence comparison. While we strictly filter which \code{equals}
implementations we consider, if any of them do not accurately assess an equivalent program state, 
our refactoring may prove unsound at runtime.

%%However, we make no guarantee about correctness with respect to such operations
%% anyway, whether static or instance state.}

With respect to the use of fuzzing,
%In order to guarantee the soundness and completeness of the
%proposed refactoring technique,
we could attempt to swap it with some formal verification technique.
However, program verification is undecidable in general.
Even for restricted, decidable fragments, such
techniques tend to be expensive as they generally require some form of
symbolic execution and calls to off-the-shelves solvers.  Moreover,
programming languages evolve faster than verification techniques and
therefore new features might not be supported by the front ends of
such techniques.


\todo{Other sources of incompleteness and unsoundness?}

%% such a technique would require symbolic execution, which is no-
%% toriously difficult for real-world programs due to features such as
%% loops, library functions, complex data structures. The most success-
%% ful such verification techniques are either bounded (i.e. check that
%% an erroneous state cannot be reached within a bounded number of
%% loop unrollings) or require user input in the form of loop invariants.
%% While the former class of techniques provides limited soundness
%% guarantees, the latter requires intricate help from the user. In fact,
%% we are not aware of any sound and automatic symbolic verification
%% method that can handle the kind of code we are interested in.

%% Discuss tradeoff: symbolic execution is expensive; programming
%% languages evolve faster than verification techniques and therefore
%% many new features are not supported by their front ends (and/or
%% backends depending on how complex the new feature is); even
%% after translating the code into a logical formula, calls to solvers are
%% expensive.


\subsection{Instrumentation and isolation}

We use reflection to invoke the original method to refactor with fuzzed inputs,
as well as our synthesised refactoring candidates. This allows us to dynamically
invoke new candidates without the need for compilation. The state of a Java
program is modelled by the current program stack as well as its heap consisting
of instance variables and static field values. Static Field values are stored
with their respective classes, which in turn are loaded by class loaders. Java
allows to load the same class in different class loaders, which creates
independent copies of its static fields.

In order to fully isolate the program state during the execution of the original
and refactored code from each other, we load all invovled classes in separate
class loaders. These class loaders are disposed immediately after the curent set
of fuzzed inputs were executed, and new class loaders are created for the next
inputs. Classes which do not maintain a static state are loaded in a shared
parent class loader to improve performance.

The OpenJDK Java virtual machine implementation does not allow us to load
classes contained in the \code{java.lang} package in such an isolated class
loader. For such classes we cannot apply refactorings which depend on static
fields, as they cannot be reset and will retain the state of previous
executions. Instead of observing the effect of one isolated refactoring
candidate, we would observe their accumulated effect, leading to both
incompleteness and unsoundness depending on the context. For the scope of our
experiments this did not pose a problem, since most of the affected classes in
the \code{java.lang} package do not maintain a static state, and the ones who do
were irrelevant to our refactorings.

%Everything that is part of our comparison is in the isolated class loader.


%% For instance user defined types will usually be located in the isolated
%% class loader, enabling the comparison.
%% However, what do we do about wrapper object aliasing?
%% For anything outside the class loader, we use reference equality.
%% PAK: I think this should be in a separate section. Being loaded in the isolated
%% class loader or not doesn't prohibit comparison per se.

%% We also never use reference equality,
%% but rather rely on native `equals' implementations if they satisfy certain criteria.
%% This should probably also be explained in its own subsection.

%% One potential issue when checking equivalence of two blocks of code
%% is the treatment of loaded classes. In particular, one program might
%% load more classes than the other, but still have the exact same effect
%% as the other one. This can happen for instance if one program calls a
%% helper method from a class that needs to be loaded, whereas the other
%% program inlines the same method, thus avoiding loading the class.  Our
%% assumption is that loading and initialization don't perform any
%% changes outside of the observable state already checked by the
%% existing equivalence check.  While this assumption is generally safe
%% to make, there might be some cases when this is not the case, e.g. the
%% class initializer might print something on the screen. For now, we
%% don't handle these cases.
%% \todo{So we don't need to load the exact same classes? What if static fields are changed? -
%% PAK: We check that all static fields in classes that were loaded by either program are equivalent.
%% That means that if one program didn't load a class, we load it for it, and check that the
%% initial state of that class is the same as the other program's. 



%% The goal of our instrumentation is to execute a program candidate in
%% the same context as the original, deprecated method. The original
%% method can affect the program state by virtue of its return value,
%% field assignments in the case of instance methods, and static field
%% assignments. In order to account for these effects, our
%% instrumentation passes a reference to the original method's object
%% instance, if present, as an argument to the program candidate.
%% An example of this is presented in
%% Fig.~\ref{ex:side-effects-instrumentation}.

%% \begin{figure}
%% \begin{lstlisting}[mathescape=true,showstringspaces=false]
%% // Redundant snippet:
%% Date today;
%% Date tomorrow;
  
%% void init(int year, int month, int day) {
%%   // Original:
%%   this.setDates(year, month, day);

%%   // Instrumented:
%%   Program.of(0xabcd).execute(year, month, day, this);
%% }
  
%% @Deprecated
%% private void setDates(int year, int month, int day) {
%%   final Calendar calendar = Calendar.getInstance();
%%   calendar.set(year, month, day);
%%   this.today = calendar.getTime();
%%   calendar.add(Calendar.DAY_OF_MONTH, 1);
%%   this.tomorrow = calendar.getTime();
%% }
%% \end{lstlisting}
%% \caption{Deprecated side-effects instrumentation.}
%% \label{ex:side-effects-instrumentation}
%% \end{figure}

%% Fig.~\ref{ex:date-instrumentation} illustrates an example where no
%% such relevant instance object exists and thus only the original
%% arguments are passed as arguments into the program candidate.

%% \begin{figure}
%% \begin{lstlisting}[mathescape=true,showstringspaces=false]
%% // Redundant snippet:
%% static void main(String[] args) {
%%   // Original:
%%   Date date = new Date(120, 12, 30);

%%   // Instrumented:
%%   Date date = Program.of(0xabcd).execute(120, 12, 30);
%% }
%% \end{lstlisting}
%% \caption{Deprecated `Date' instrumentation.}
%% \label{ex:date-instrumentation}
%% \end{figure}

\subsection{Abstract classes and interfaces}

Abstract classes and interfaces in counterexamples are by default instantiated
using the mocking framework Mockito. We curate a list of explicit constructors
of subclasses to be used in favour of Mockito mocks for certain types, and users
have the option to extend this list with a custom configuration. This is
particularly useful when the default behaviour of mocked objects affect the
refactoring, and thus mocked objects as under-approximation could lead to
spurious solutions. A trivial example of this limitation is refactoring the
method \code{layout} in the abstract class \code{java.awt.Component}. The
method is not abstract, but has an empty body and is meant to be overriden by
subclasses. Using this empty body as a specification will only guranatee a sound
refactoring for subclasses which do not override it. Our approach is still
guaranteed to find the preferred refactoring of using \code{doLayout} instead,
since we take the Javadoc hint into account. However, a more complete approach
in general would be to fuzz against all available subclasses of
\code{Component} on the classpath.

We explicitly avoid constructors which are not amenable to fuzzing, such as
collection constructors which take an integer capacity argument, where an
unlucky fuzzed input might lead to an out of memory error.

\subsection{Parsing the @code hints}

As explained in Section~\ref{sec:components-seeding}, when seeding the component library, our algorithm must
interpret Javadoc hints. % provided by the developer.
in particular, \code{@code} blocks inside \code{@deprecated} sections are interpreted as code
hints, and we attempt to parse them as Java expressions. Since code hints are
not always expressed as well-formed Java, we customised the
GitHub Java parser
%%framework
to accept undeclared identifiers and type names
as arguments. For instance, the Javadoc hint in Figure~\ref{ex:javadoc-hint} suggests using
\code{contains(int, int)}, which would normally cause a parsing error as the \code{int} type
appears in the place of argument names. In our setting, we accept this hint as valid.

%% provides an example of
%% a valid Javadoc hint.

\begin{figure}
\begin{lstlisting}[mathescape=true,showstringspaces=false]
/**
* Checks whether or not this {@code Rectangle} contains the
* point at the specified location {@code (X,Y)}.
*
* @param  X the specified X coordinate
* @param  Y the specified Y coordinate
* @return    {@code true} if the point
*            {@code (X,Y)} is inside this
*            {@code Rectangle};
*            {@code false} otherwise.
* @deprecated As of JDK version 1.1,
* replaced by {@code contains(int, int)}.
*/
@Deprecated
public boolean inside(int X, int Y) {
  // ...
}
\end{lstlisting}
\caption{Javadoc hint example.}
\label{ex:javadoc-hint}
\end{figure}

Additionally, the code example is parsed as if it were invoked in the context of
the method to refactor, such that imports or the implicit \code{this} argument
are considered during parsing.

\section{Experimental results}\label{sec:experimental-results}

\section{Semantically different refactorings}


\begin{figure}
\begin{lstlisting}[mathescape=true,showstringspaces=false]
void main(String[] args) {
  // Deprecated:
  Date date = new Date(120, 12, 30);
 
  // Should have been:
  final Calendar calendar = Calendar.getInstance();
  calendar.set(2020, 12, 30);
  final Date date = calendar.getTime();

  // Or:
  final Date date = new GregorianCalendar(2020, 12, 30)
                      .getTime();
}
\end{lstlisting}
\caption{Deprecated method example.}
\label{ex:deprecated-method}
\end{figure}


Sometimes, the deprecated and the new features are not semantically
equivalent on the entire domain. Reasons for this might be that a bug
was fixed, a certain functionality was improved, or simply the fact
that the deprecated feature is only expected to be used on restricted
parts of the input domain. In such a case, the deprecated and the new
feature may not be semantically equivalent on the entire input domain.

For illustration, let's consider the example in Figure~\ref{ex:three-dates}.
In the example, we make use of the constructor \code{new Date(year, month, day)}, 
which is deprecated with the 
recommendation to use the \code{Calendar} class instead.
The refactoring is not trivial given that the 
\code{Date} constructor expects its year parameter to be an offset
from 1900.  This needs to be
transformed by adding 1900 when using \code{Calendar} API, as shown in the
example. Moreover, as already seen in the motivational example in  Figure~\ref{ex:deprecated-method-other},
the \code{Calendar} constructor is protected,
meaning that we must use \code{getInstance} to obtain a
\code{Calendar} object.
Alternatively, the \code{GregorianCalendar} class may be used with
similar difficulties.
In the example, we use \var{date1}, \var{date2} and \var{date3} to denote the three distinct ways of constructing a date.

%% For illustration, let's consider the three distinct ways of
%% constructing a date captured in Figure~\ref{ex:three-dates} and
%% denoted by \var{date1}, \var{date2} and \var{date3}.

\begin{figure}
  \begin{lstlisting}[mathescape=true,showstringspaces=false]
  // Deprecated:
  Date date1 = new Date(year, month, day);

  // Should have been:
  Calendar calendar = Calendar.getInstance();
  calendar.set(1900 + year, month, day, 0, 0, 0);
  calendar.set(Calendar.MILLISECOND, 0);
  Date date2 = calendar.getTime();
    
  // Or:
  Date date3=new GregorianCalendar(1900+year,month,day).
             getTime();
  \end{lstlisting}
\caption{Deprecated `Date' example.}
\label{ex:three-dates}
\end{figure}

%% In the example, \var{date1} uses the deprecated
%% \var{Date(year, month, day)} constructor, whereas \var{date2} and
%% \var{date3} make use of the recommended \var{Calendar} and
%% \var{GregorianCalendar} classes.
Notably, the ways in which \var{date2} and \var{date3} are computed
represent a valid refactoring for the piece of code computing
\var{date1}. However, when we provide this example to our
verification phase (as described in Section~\ref{sec:encoding}), we obtain the following
counterexample:

  \begin{lstlisting}[mathescape=true,showstringspaces=false]
    int year = -89412298;
    int month = 1439435067;
    int day = 378993182;
  \end{lstlisting}

  For these values of \var{year}, \var{month} and \var{day}, the three
  dates evaluate to:

  \begin{lstlisting}[mathescape=true,showstringspaces=false]
    date1: "Sat May 02 00:00:00 CEST 31580172"
    date2: "Fri Jul 02 00:00:00 CEST 31580799"
    date3: "Fri Jul 02 00:00:00 CEST 31580799"
  \end{lstlisting}

  Where \var{date1} evaluates to a different date than \var{date2} and \var{date3}. In principle, this should not affect any user as the given
  \var{year}, \var{month} and \var{day} are clearly artificial and outside the expected domain of a calendaristic date.
  In particular, for this example, we deduced that the negative year is the cause for the different values of \var{date1}, \var{date2} and \var{date3}.

  This counterexample is enough to stop our synthesiser from finding any of the correct refactorings in Figure~\ref{ex:three-dates}.
  To overcome this problem, we plan on computing preconditions for the verification process. Namely, for this example, we would collect all the concrete values that
  \var{year}, \var{month} and \var{day} get instantiated to in the unit tests associated with the given project, and use them to
  infer a precondition over the domain of these three variables. For \var{year}, this should tell us that the expected year must be positive.
  Consequently, the verifier won't attempt to find counterexamples involving negative years, thus eliminating the counterexample above.


  
\section{Related Work}

\paragraph{Program synthesis}


CEGIS based approaches to program synthesis have been previously used
for program transformations such as superoptimisation and
deobfuscation~\cite{DBLP:conf/icse/JhaGST10}.  In
\cite{DBLP:journals/corr/abs-1712-07388}, David et al present an
automatic refactoring tool that transforms Java with external
iteration over collections into code that uses Streams. Their approach
is based on CEGIS and makes use of formal verification to check the
correctness of a refactoring.  Cheung et al.~describe a system that
automatically transforms fragments of application logic into SQL
queries~\cite{DBLP:conf/pldi/CheungSM13} by using a CEGIS-based
synthesiser to generate invariants and postconditions that validate
their transformations (a~similar approach is presented
in~\cite{DBLP:conf/cc/IuCZ10}).  While our approach is also based
on CEGIS, we are guided by code hints and types in
order to efficiently prune the search space. Without this guidance,
the current refactoring would not be feasible.  

% type-directed synthesis
Type information has been extensively used in program synthesis to
guide the search for a solution~\cite{DBLP:conf/sfp/Katayama05,DBLP:conf/pldi/FeserCD15,DBLP:conf/pldi/OseraZ15,DBLP:journals/pacmpl/LubinCOC20}.
In our work, we combine type information with code hints.
%
% component-based synthesis
Another direction that inspired us is that of component-based synthesis~\cite{DBLP:conf/icse/JhaGST10,DBLP:conf/pldi/GulwaniJTV11,DBLP:conf/popl/FengM0DR17}, where the target program is generated by composing components from a library. Similarly
to these approaches, we use a library of components for our program
generation approach. However, our technique uses information about types
and code hints to build the component library, which is specific to each refactoring.

%% For the intro:
% not correct-by-contruction, preserves the behaviour, refer to the paper
% on refactoring concurrent programs.
%% for equivalence checking:
%% we use an explicit return variable ret, which is live. Also, we make "this explicit".
%% Essentially, we must capture the whole program state, consisting of return values and heap. 
%% Todo: related work, alising check, intro, equiv check.


%% An approach to program synthesis very similar to ours is Syntax Guided
%% Synthesis (SyGuS)~\cite{sygus}.  SyGuS synthesisers supplement the logical
%% specification with a syntactic template that constrains the space of allowed
%% implementations.  Thus, each semantic specification is accompanied by a
%% syntactic specification in the form of a grammar.  Other second-order
%% solvers are introduced in~\cite{DBLP:conf/pldi/GrebenshchikovLPR12,
%% DBLP:conf/cav/BeyenePR13}.  As opposed to ours, these focus on
%% Horn clauses.


  %% The main difference (besides the actual
%% goal of the work, which is different from ours) to our work is that the
%% lists they operate on are immutable and do not support operations such as
%% remove.  Capturing the potential side effects caused by such operations is
%% one of our work's main challenges.

Program synthesis is generally guided by the principle of correct-by-construction, meaning that it provides strong correctness guarantees about the generated code.
As opposed to that, our work is guided by program testing. This is similar in nature with genetic programming~\cite{Koza92} and genetic improvement~\cite{DBLP:journals/dagstuhl-reports/PetkeGFL18,7911210}. In particular, the latter has been used for program refactoring. Genetic programming and genetic improvement obtain candidate programs by applying operations analogous to genetic processes to an existing population of programs. Conversely, we pose the code generation problem as safety verification.



%% Often, genetic improvement
%% \cite{DBLP:journals/dagstuhl-reports/PetkeGFL18} is used for the
%% purpose of code refactoring. Due to the manner in which such works measure
%% the fitness of potential refactorings, genetic improvement cannot vary
%% the refactoring context.  Conversely, our technique allows varying this
%% context until a refactoring is being found.


\paragraph{Program refactoring}

With respect to the refactoring of deprecated instances, 
the authors of \cite{DBLP:conf/paste/Perkins05}
replace calls to deprecated methods by their bodies.

The authors of \cite{10.1007/978-3-642-14107-2_11}
show that it is all too easy for a refactoring that works on concurrent
programs to introduce concurrency bugs by enabling new interactions between
parallel threads. They also provide techniques to make such refactorings
behavior-preserving.

Rule-based source-to-source transformations: they provide languages in which
transformation rules can be expressed ...
Syntax-driven refactoring base program transformation decisions
on observations on the program's syntax tree.  Visser
presents a purely syntax-driven framework~\cite{stratego}.  The
presented method is intended to be configurable for specific
refactoring tasks, but cannot provide guarantees about semantics
preservation.

In~\cite{txl}, Cordy et al. introduce TXL, a programming language and rapid prototyping
system specifically designed to support structural source transformation.
As opposed to our work, here the user needs to provide the transformation rules.

Automated program refactoring:
\cite{sawin} by Sawin et al., \cite{bae} by Bae et al.~and
\cite{chris} by Christopoulou et al.  In contrast to these approaches,
our procedure constructs an equivalence proof before transforming the
program. In \cite{conf/sigsoft/GyoriFDL13}, Gyori et al. present a
similar refactoring to ours but performed in a syntax-driven manner.
%

Steimann et al.~present Constraint-Based Refactoring in \cite{Steimann2011},
\cite{Steimann2012Pilgrim} and \cite{Steimann2011KollePilgrim}. Their approach
generates explicit constraints over the program's abstract syntax tree to
prevent compilation errors or behaviour changes by automated refactorings.
% This gives rise to a flexible framework of customisable refactorings,
% implementable through a refactoring constraint specification language
% (cf. \cite{Steimann2011KollePilgrim}).
The approach is limited by the information
a program's AST provides and thus favours conservative implementations of
syntax-focused refactorings such as \emph{Pull Up Field}.
%
Fuhrer et al.~implement a type constraint system to introduce missing type
parameters in uses of generic classes (cf. \cite{DBLP:conf/ecoop/FuhrerTKDK05})
and to introduce generic type parameters into classes which do not provide
a generic interfaces despite being used in multiple type contexts
(cf. \cite{DBLP:conf/icse/KiezunETF07}).
%
% Raychev et al.~present a semi-automatic approach where users perform
% incomplete refactorings manually and then employ a constraint solver
% to find a sequence of default refactorings such as move or rename
% which include the users' changes. The engine is limited to syntactic
% matching with the users' partial changes and does not consider program
% semantics~\cite{DBLP:conf/oopsla/RaychevSSV13}.
%
% Weissgerber and Diehl rely on meta information to classify changes
% between software versions as refactorings~\cite{weiss}.  The technique
% aims to identify past refactorings performed by programmers, but is
% not a decision procedure for automated refactorings.
%
O'Keffe and Cinn{\'{e}}ide present search-based
refactoring~\cite{search1, search2}, which is similar to syntax-driven
refactoring.  They rephrase refactoring as an optimisation problem,
using code metrics as fitness measure.  As such, the method optimises
syntactical constraints and does not take program semantics into
account.
%
% Bavota et al. implement refactoring decisions in \cite{Bavota:2011:IEC}
% using semantic information limited to identifiers and comments,
% which may differ from the actual semantics (e.g. due to bugs).
%
Kataoka et al. interpret program semantics to apply refactorings
\cite{Kataoka:2001:ASP:846228.848644}, but use dynamic test execution
rather than formal verification, and hence their transformation lacks
soundness guarantees.
%
Franklin et al. implement a pattern-based refactoring approach
transforming statements to stream queries~\cite{Gyori:2013:CGI:2491411.2491461}.
Their tool LambdaFicator~\cite{DBLP:conf/icse/FranklinGLD04} is available as a
NetBeans branch.
%We compared \tool against it in our experimental evaluation
%in Sec.~\ref{experiments-results}.
%
%% \paragraph{Heap Logics}
%
%% While many decidable heap logics have been developed recently, none are
%% expressive enough to capture operations allowed by the Java Collection
%% interface, operations allowed by the Java Stream interface as well as
%% equality between collections (for lists this implies that we must be able to
%% reason about both content of lists and the order of
%% elements)~\cite{DBLP:conf/cav/ItzhakyBINS13, DBLP:conf/cav/PiskacWZ13,
%% DBLP:conf/esop/BrainDKS14, DBLP:conf/popl/MadhusudanPQ11,
%% DBLP:conf/atva/BouajjaniDES12, DBLP:conf/lpar/DavidKL15}.  On the other
%% hand, very expressive transitive closure
%% logics~\cite{DBLP:conf/csl/ImmermanRRSY04} are not concise and easily
%% translatable to stream code.




\section{Conclusion}

We conjecture that refactorings driven by the semantics of programs have
broader applicability and are able to address more complex refactoring
schemata in comparison to conventional syntax-driven refactorings, thereby
increasing the benefits of automated refactoring.  The space of possible
semantic refactoring methods is enormous; as an instance, we have presented
a method for refactoring iteration over Java collection classes based on
program synthesis methods.  Our experiments indicate that refactoring using
this specific instance is feasible, sound and sufficiently performant. 
Future research must broaden the evidence for our general hypothesis by
considering other programming languages, further, ideally more complex
refactoring schemata, and other semantics-based analysis techniques.

%\acks

% We recommend abbrvnat bibliography style.
\bibliographystyle{ACM-Reference-Format}
%\bibliographystyle{abbrv}
\bibliography{document}



\end{document}

%                       Revision History
%                       -------- -------
%  Date         Person  Ver.    Change
%  ----         ------  ----    ------

%  2013.06.29   TU      0.1--4  comments on permission/copyright notices

